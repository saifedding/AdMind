# Daily Scraping Control - Complete Setup Guide

## ğŸ‰ What's New

You now have a **complete frontend interface** to control daily scraping of Facebook ads! This replaces manual CLI commands with an intuitive web interface.

## ğŸš€ Quick Start (3 Steps)

### 1. Start the Application
```bash
cd "C:\Users\ASUS\Documents\coding area\ads"
docker-compose up --build
```

### 2. Access the Interface
Open your browser to: **http://localhost:3000**

### 3. Configure Daily Scraping
1. Click **"Daily Scraping"** in the sidebar (Clock icon â°)
2. Toggle **"Enable Daily Scraping"** ON
3. Set your schedule time (e.g., 02:00 for 2 AM)
4. Configure parameters:
   - Max Pages: **3**
   - Delay: **2 seconds**
   - Hours Lookback: **24**
   - Countries: Select **AE, US, UK** (or your preferences)
5. Click **"Save Configuration"**
6. Test with **"Start Manual Scraping"** button

## ğŸ“Š What You Can Do

### In the Frontend:
âœ… **Schedule Configuration**: Set when scraping runs daily  
âœ… **Scraping Parameters**: Control pages, delays, and filters  
âœ… **Country Selection**: Choose target countries with checkboxes + Select All/Clear  
âœ… **Competitor Selection**: Choose specific competitors or scrape all  
âœ… **Manual Trigger**: Start scraping immediately  
âœ… **Real-time Monitoring**: Watch tasks progress live  
âœ… **Task Management**: Cancel running tasks  
âœ… **Results Display**: See new ads found, competitors processed

### Interface Features:
- ğŸ¨ Beautiful, dark-themed UI
- ğŸ“± Fully responsive (works on mobile)
- âš¡ Real-time updates
- ğŸ”” Visual status indicators
- ğŸ’¾ Save/Reset configurations
- ğŸ“Š Competitor overview with latest ad dates

## ğŸ—‚ï¸ File Structure

New files added:

```
frontend/
â””â”€â”€ src/
    â””â”€â”€ app/
        â””â”€â”€ daily-scraping/
            â””â”€â”€ page.tsx              # Main UI page

backend/
â””â”€â”€ app/
    â”œâ”€â”€ routers/
    â”‚   â””â”€â”€ daily_scraping.py        # API endpoints
    â””â”€â”€ tasks/
        â””â”€â”€ daily_ads_scraper.py      # Background tasks

# Documentation
â”œâ”€â”€ DAILY_SCRAPING_GUIDE.md           # Backend guide
â”œâ”€â”€ DAILY_SCRAPING_SETUP.md           # This file
â”œâ”€â”€ daily_scraping_cli.py             # CLI tool (optional)
â””â”€â”€ frontend/
    â””â”€â”€ DAILY_SCRAPING_FRONTEND.md    # Frontend guide
```

## ğŸ¯ Usage Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Opens Frontend                   â”‚
â”‚              http://localhost:3000                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Navigate to Daily Scraping                  â”‚
â”‚            (Sidebar: Clock Icon)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Configure Settings in UI                       â”‚
â”‚   â€¢ Schedule Time: 02:00                                 â”‚
â”‚   â€¢ Countries: AE, US, UK                                â”‚
â”‚   â€¢ Max Pages: 3                                         â”‚
â”‚   â€¢ Delay: 2 seconds                                     â”‚
â”‚   â€¢ Enable: ON                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Click "Save Configuration"                  â”‚
â”‚         (Stores settings in backend)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Optional: Click "Start Manual Scraping"          â”‚
â”‚           (Tests immediately)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Watch Real-time Progress                      â”‚
â”‚   â€¢ Task Status: PENDING â†’ STARTED â†’ SUCCESS             â”‚
â”‚   â€¢ See competitors processed                            â”‚
â”‚   â€¢ View new ads found                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Automatic Daily Execution                      â”‚
â”‚    (Celery Beat runs at scheduled time)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ API Endpoints Available

All accessible through the frontend, but also available via API:

```bash
# Start daily scraping
POST http://localhost:8000/api/v1/scraping/daily/start

# Start specific competitors
POST http://localhost:8000/api/v1/scraping/competitors/scrape

# Check task status
GET http://localhost:8000/api/v1/scraping/tasks/{task_id}/status

# Get active competitors
GET http://localhost:8000/api/v1/scraping/active-competitors

# View active tasks
GET http://localhost:8000/api/v1/scraping/tasks/active

# Cancel task
DELETE http://localhost:8000/api/v1/scraping/tasks/{task_id}/cancel
```

## ğŸ“ Example Configuration

**Recommended for Daily Use:**
```
Enable Daily Scraping: âœ… ON
Schedule Time: 02:00 (2 AM)
Max Pages per Competitor: 3
Delay Between Requests: 2 seconds
Hours Lookback: 24
Active Status: Active Only
Countries: âœ… UAE, âœ… US, âœ… UK
```

**For Testing:**
```
Enable Daily Scraping: âŒ OFF
Max Pages per Competitor: 1
Delay Between Requests: 1 second
Hours Lookback: 48
Active Status: All
Countries: âœ… UAE
```

## ğŸ› Troubleshooting

### Frontend Not Loading
```bash
# Check if containers are running
docker-compose ps

# Check frontend logs
docker-compose logs -f web

# Restart frontend
docker-compose restart web
```

### API Calls Failing
```bash
# Check backend logs
docker-compose logs -f api

# Verify API is accessible
curl http://localhost:8000/api/v1/health

# Check CORS settings
docker-compose logs api | grep CORS
```

### No Competitors Showing
1. Go to Competitors page first
2. Add at least one competitor
3. Make sure competitor is **Active**
4. Click **Refresh** button in Daily Scraping page

### Tasks Not Running
```bash
# Check Celery worker
docker-compose logs -f worker

# Check Celery beat
docker-compose logs -f beat

# Restart Celery services
docker-compose restart worker beat
```

## ğŸ“– Documentation

- **Frontend Guide**: `frontend/DAILY_SCRAPING_FRONTEND.md`
- **Backend Guide**: `DAILY_SCRAPING_GUIDE.md`
- **CLI Tool**: Use `python daily_scraping_cli.py --help`
- **API Docs**: Visit `http://localhost:8000/docs`

## ğŸ¨ UI Preview

The interface includes:

1. **Header Section**
   - Title and description
   - Refresh and Start Manual Scraping buttons

2. **Task Status Card** (when running)
   - Live status updates
   - Progress indicators
   - Results display
   - Cancel button

3. **Configuration Cards**
   - Schedule Configuration (left)
   - Scraping Parameters (right)

4. **Countries Selection**
   - Grid of country checkboxes with flags
   - Multi-select capability

5. **Active Competitors**
   - List of saved pages
   - Latest ad dates
   - Count summary

6. **Competitor Selection Card** (NEW)
   - Checkbox list of all competitors
   - Select All / Clear Selection buttons
   - Visual highlighting for selected competitors
   - Info indicator showing selection status
   - Leave empty to scrape all competitors

7. **Action Buttons**
   - Reset Changes
   - Save Configuration

8. **Info Card**
   - How daily scraping works
   - Key features explanation

## ğŸ” Security Notes

- The frontend proxies API requests through Next.js
- No credentials are stored in frontend
- All sensitive operations happen in backend
- Task cancellation requires valid task ID

## ğŸš¦ Status Indicators

- â±ï¸ **Yellow Timer**: Task is pending
- ğŸ”„ **Blue Spinner**: Task is running
- âœ… **Green Check**: Task completed successfully
- âŒ **Red X**: Task failed
- âšª **Gray Circle**: Unknown state

## ğŸ“ Getting Help

1. **Check Frontend Console**: Press F12 in browser
2. **Check Backend Logs**: `docker-compose logs -f api worker`
3. **Test API Directly**: Visit `http://localhost:8000/docs`
4. **Review Documentation**: All MD files in project root

## ğŸ‰ Success Checklist

After setup, you should be able to:

- [ ] Access Daily Scraping page at http://localhost:3000/daily-scraping
- [ ] See your active competitors listed
- [ ] Toggle Enable Daily Scraping switch
- [ ] Set schedule time
- [ ] Select target countries (try "Select All" button)
- [ ] Select specific competitors or leave empty for all
- [ ] Configure scraping parameters
- [ ] Click "Start Manual Scraping" button
- [ ] See task status update in real-time
- [ ] View results when task completes
- [ ] Save configuration successfully

If all items are checked âœ…, you're ready to go! ğŸš€

---

**Enjoy your automated daily ads scraping! ğŸ¯ğŸ“Š**