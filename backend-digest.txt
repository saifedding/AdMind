Directory structure:
└── backend/
    ├── Dockerfile
    ├── alembic.ini
    ├── celerybeat-schedule
    ├── requirements.txt
    ├── alembic/
    │   ├── README
    │   ├── env.py
    │   ├── script.py.mako
    │   └── versions/
    │       ├── 014e83d13d0a_create_campaign_model_and_link_to_ad.py
    │       ├── 0a968eddd8d7_enhanced_extraction_fields_and_cleanup.py
    │       ├── 31ab1628a920_add_page_url_to_competitor.py
    │       ├── 4b419cac5311_initial_migration_create_competitors_.py
    │       ├── 568f9fde44b0_add_enhanced_extraction_fields_and_.py
    │       ├── 5a527db67d43_enhanced_extraction_fields_and_cleanup.py
    │       ├── 6168e4625f40_add_enhanced_extraction_fields_and_.py
    │       ├── 779734a0c91c_add_comprehensive_facebook_ads_data_.py
    │       ├── 8a69c9ce3517_add_duration_days_to_ads.py
    │       ├── a220f98e38c2_add_form_details_running_countries_.py
    │       ├── a2feb1e20f39_add_meta_targeting_lead_form_and_.py
    │       ├── a7031f8c7467_add_enhanced_extraction_fields_and_keep_.py
    │       └── d26d6ee785d3_add_enhanced_extraction_fields_hybrid_.py
    └── app/
        ├── __init__.py
        ├── celery.py
        ├── celery_worker.py
        ├── database.py
        ├── main.py
        ├── __pycache__/
        ├── api/
        │   ├── __init__.py
        │   └── internal_api.py
        ├── core/
        │   ├── config.py
        │   └── __pycache__/
        ├── models/
        │   ├── __init__.py
        │   ├── ad.py
        │   ├── ad_analysis.py
        │   ├── competitor.py
        │   ├── task_status.py
        │   ├── __pycache__/
        │   └── dto/
        │       ├── __init__.py
        │       ├── ad_dto.py
        │       ├── competitor_dto.py
        │       └── __pycache__/
        ├── routers/
        │   ├── __init__.py
        │   ├── ads.py
        │   ├── competitors.py
        │   └── health.py
        ├── services/
        │   ├── __init__.py
        │   ├── ad_service.py
        │   ├── ai_service.py
        │   ├── competitor_service.py
        │   ├── enhanced_ad_extraction.py
        │   ├── facebook_ads_scraper.py
        │   ├── ingestion_service.py
        │   └── __pycache__/
        └── tasks/
            ├── __init__.py
            ├── ai_analysis_tasks.py
            ├── basic_tasks.py
            ├── facebook_ads_scraper_task.py
            └── __pycache__/

================================================
File: Dockerfile
================================================
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV PYTHONPATH /app

# Set work directory
WORKDIR /app

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        postgresql-client \
        build-essential \
        libpq-dev \
        curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir -r requirements.txt

# Copy project
COPY . .

# Create uploads directory
RUN mkdir -p uploads

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"] 


================================================
File: alembic.ini
================================================
# A generic, single database configuration.

[alembic]
# path to migration scripts
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the
# "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# sqlalchemy.url = driver://user:pass@localhost/dbname
# The URL will be set programmatically in env.py using environment variables


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S



================================================
File: celerybeat-schedule
================================================
[Non-text file]


================================================
File: requirements.txt
================================================
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
alembic==1.13.1
psycopg2-binary==2.9.9
pydantic==2.5.0
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6
celery==5.3.4
redis==5.0.1
httpx==0.25.2
python-dotenv==1.0.0
pytest==7.4.3
pytest-asyncio==0.23.2
requests==2.31.0
pandas==2.1.4
numpy==1.26.2
Pillow==10.1.0
python-facebook-api==0.18.0
aiofiles==23.2.1
jinja2==3.1.2
email-validator==2.1.0
google-generativeai==0.3.2 


================================================
File: alembic/README
================================================
Generic single-database configuration.


================================================
File: alembic/env.py
================================================
from logging.config import fileConfig
import os
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Import our models and database configuration
from app.database import Base
from app.models import Competitor, Ad, AdAnalysis

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata

# Set the database URL from environment variables
database_url = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@db:5432/ads_db")
config.set_main_option("sqlalchemy.url", database_url)

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()



================================================
File: alembic/script.py.mako
================================================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}



================================================
File: alembic/versions/014e83d13d0a_create_campaign_model_and_link_to_ad.py
================================================
"""create_campaign_model_and_link_to_ad

Revision ID: 014e83d13d0a
Revises: 31ab1628a920
Create Date: 2025-07-06 21:55:01.128553

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '014e83d13d0a'
down_revision: Union[str, None] = '31ab1628a920'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('campaign_id', sa.Integer(), nullable=False))
    op.create_index(op.f('ix_ads_campaign_id'), 'ads', ['campaign_id'], unique=False)
    op.create_foreign_key(None, 'ads', 'campaigns', ['campaign_id'], ['id'])
    op.drop_column('ads', 'creatives')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('creatives', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'ads', type_='foreignkey')
    op.drop_index(op.f('ix_ads_campaign_id'), table_name='ads')
    op.drop_column('ads', 'campaign_id')
    # ### end Alembic commands ###



================================================
File: alembic/versions/0a968eddd8d7_enhanced_extraction_fields_and_cleanup.py
================================================
"""enhanced_extraction_fields_and_cleanup

Revision ID: 0a968eddd8d7
Revises: d26d6ee785d3
Create Date: 2025-07-06 15:07:39.350024

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0a968eddd8d7'
down_revision: Union[str, None] = 'd26d6ee785d3'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('creatives_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.Boolean(), nullable=False))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('landing_page_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.String(), nullable=True))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'extraction_version')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'form_details')
    op.drop_column('ads', 'extraction_date')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'main_title')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'creatives')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'targeted_countries')
    op.drop_column('ads', 'main_image_urls')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('main_image_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('targeted_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('creatives', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('running_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_categories', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('spend', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_titles', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_title', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('currency', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extraction_date', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('form_details', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_links', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_bodies', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extraction_version', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'landing_page_url')
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'is_enhanced_processed')
    op.drop_column('ads', 'creatives_data')
    # ### end Alembic commands ###



================================================
File: alembic/versions/31ab1628a920_add_page_url_to_competitor.py
================================================
"""add page_url to competitor

Revision ID: 31ab1628a920
Revises: a2feb1e20f39
Create Date: 2025-07-06 20:54:51.622764

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '31ab1628a920'
down_revision: Union[str, None] = 'a2feb1e20f39'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('competitors', sa.Column('page_url', sa.String(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('competitors', 'page_url')
    # ### end Alembic commands ###



================================================
File: alembic/versions/4b419cac5311_initial_migration_create_competitors_.py
================================================
"""Initial migration: Create competitors, ads, and ad_analyses tables

Revision ID: 4b419cac5311
Revises: 
Create Date: 2025-07-03 14:33:19.713938

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '4b419cac5311'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('competitors',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('page_id', sa.String(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_competitors_id'), 'competitors', ['id'], unique=False)
    op.create_index(op.f('ix_competitors_name'), 'competitors', ['name'], unique=False)
    op.create_index(op.f('ix_competitors_page_id'), 'competitors', ['page_id'], unique=True)
    op.create_table('ads',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('competitor_id', sa.Integer(), nullable=False),
    sa.Column('ad_archive_id', sa.String(), nullable=False),
    sa.Column('ad_copy', sa.Text(), nullable=True),
    sa.Column('media_type', sa.String(), nullable=True),
    sa.Column('media_url', sa.String(), nullable=True),
    sa.Column('landing_page_url', sa.String(), nullable=True),
    sa.Column('date_found', sa.DateTime(timezone=True), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('raw_data', sa.JSON(), nullable=True),
    sa.Column('page_name', sa.String(), nullable=True),
    sa.Column('publisher_platform', sa.JSON(), nullable=True),
    sa.Column('impressions_text', sa.String(), nullable=True),
    sa.Column('end_date', sa.DateTime(timezone=True), nullable=True),
    sa.Column('cta_text', sa.String(), nullable=True),
    sa.Column('cta_type', sa.String(), nullable=True),
    sa.ForeignKeyConstraint(['competitor_id'], ['competitors.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_ads_ad_archive_id'), 'ads', ['ad_archive_id'], unique=True)
    op.create_index(op.f('ix_ads_date_found'), 'ads', ['date_found'], unique=False)
    op.create_index(op.f('ix_ads_id'), 'ads', ['id'], unique=False)
    op.create_table('ad_analyses',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('ad_id', sa.Integer(), nullable=False),
    sa.Column('summary', sa.Text(), nullable=True),
    sa.Column('hook_score', sa.Float(), nullable=True),
    sa.Column('overall_score', sa.Float(), nullable=True),
    sa.Column('ai_prompts', sa.JSON(), nullable=True),
    sa.Column('raw_ai_response', sa.JSON(), nullable=True),
    sa.Column('target_audience', sa.String(), nullable=True),
    sa.Column('ad_format_analysis', sa.JSON(), nullable=True),
    sa.Column('competitor_insights', sa.JSON(), nullable=True),
    sa.Column('content_themes', sa.JSON(), nullable=True),
    sa.Column('performance_predictions', sa.JSON(), nullable=True),
    sa.Column('analysis_version', sa.String(), nullable=True),
    sa.Column('confidence_score', sa.Float(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['ad_id'], ['ads.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('ad_id')
    )
    op.create_index(op.f('ix_ad_analyses_id'), 'ad_analyses', ['id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_ad_analyses_id'), table_name='ad_analyses')
    op.drop_table('ad_analyses')
    op.drop_index(op.f('ix_ads_id'), table_name='ads')
    op.drop_index(op.f('ix_ads_date_found'), table_name='ads')
    op.drop_index(op.f('ix_ads_ad_archive_id'), table_name='ads')
    op.drop_table('ads')
    op.drop_index(op.f('ix_competitors_page_id'), table_name='competitors')
    op.drop_index(op.f('ix_competitors_name'), table_name='competitors')
    op.drop_index(op.f('ix_competitors_id'), table_name='competitors')
    op.drop_table('competitors')
    # ### end Alembic commands ###



================================================
File: alembic/versions/568f9fde44b0_add_enhanced_extraction_fields_and_.py
================================================
"""add_enhanced_extraction_fields_and_cleanup_legacy

Revision ID: 568f9fde44b0
Revises: a220f98e38c2
Create Date: 2025-07-06 14:33:53.996725

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '568f9fde44b0'
down_revision: Union[str, None] = 'a220f98e38c2'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('campaign_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_is_active', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('meta_cta_type', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_display_format', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_start_date', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_end_date', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('targeting_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('lead_form_questions', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('lead_form_standalone_fields', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('creatives_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_name', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_likes', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_profile_picture', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('platforms', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.Boolean(), nullable=False))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.String(), nullable=True))
    op.drop_index('ix_ads_page_id', table_name='ads')
    op.create_index(op.f('ix_ads_advertiser_page_id'), 'ads', ['advertiser_page_id'], unique=False)
    op.create_index(op.f('ix_ads_campaign_id'), 'ads', ['campaign_id'], unique=False)
    op.drop_column('ads', 'start_date_readable')
    op.drop_column('ads', 'main_title')
    op.drop_column('ads', 'form_details')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'end_date_readable')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'page_number')
    op.drop_column('ads', 'main_image_urls')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'collation_count')
    op.drop_column('ads', 'card_media_types')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'structured_ad_copy')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'targeted_countries')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('targeted_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('structured_ad_copy', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('spend', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_media_types', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('collation_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_categories', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_image_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_number', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_titles', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_links', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('currency', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_bodies', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('end_date_readable', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('running_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('form_details', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_title', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('start_date_readable', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.drop_index(op.f('ix_ads_campaign_id'), table_name='ads')
    op.drop_index(op.f('ix_ads_advertiser_page_id'), table_name='ads')
    op.create_index('ix_ads_page_id', 'ads', ['page_id'], unique=False)
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'is_enhanced_processed')
    op.drop_column('ads', 'platforms')
    op.drop_column('ads', 'advertiser_page_profile_picture')
    op.drop_column('ads', 'advertiser_page_likes')
    op.drop_column('ads', 'advertiser_page_url')
    op.drop_column('ads', 'advertiser_page_name')
    op.drop_column('ads', 'advertiser_page_id')
    op.drop_column('ads', 'creatives_data')
    op.drop_column('ads', 'lead_form_standalone_fields')
    op.drop_column('ads', 'lead_form_questions')
    op.drop_column('ads', 'targeting_data')
    op.drop_column('ads', 'meta_end_date')
    op.drop_column('ads', 'meta_start_date')
    op.drop_column('ads', 'meta_display_format')
    op.drop_column('ads', 'meta_cta_type')
    op.drop_column('ads', 'meta_is_active')
    op.drop_column('ads', 'campaign_id')
    # ### end Alembic commands ###



================================================
File: alembic/versions/5a527db67d43_enhanced_extraction_fields_and_cleanup.py
================================================
"""enhanced_extraction_fields_and_cleanup

Revision ID: 5a527db67d43
Revises: 0a968eddd8d7
Create Date: 2025-07-06 17:19:37.862077

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '5a527db67d43'
down_revision: Union[str, None] = '0a968eddd8d7'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/6168e4625f40_add_enhanced_extraction_fields_and_.py
================================================
"""add_enhanced_extraction_fields_and_cleanup_legacy

Revision ID: 6168e4625f40
Revises: 568f9fde44b0
Create Date: 2025-07-06 14:39:39.986058

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '6168e4625f40'
down_revision: Union[str, None] = '568f9fde44b0'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/779734a0c91c_add_comprehensive_facebook_ads_data_.py
================================================
"""Add comprehensive Facebook ads data fields

Revision ID: 779734a0c91c
Revises: 4b419cac5311
Create Date: 2025-07-03 16:52:23.691781

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '779734a0c91c'
down_revision: Union[str, None] = '4b419cac5311'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('page_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('collation_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('is_active', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('start_date', sa.DateTime(timezone=True), nullable=True))
    op.add_column('ads', sa.Column('currency', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('spend', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('display_format', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('page_categories', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('targeted_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('main_title', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_image_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('card_bodies', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_titles', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_media_types', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_links', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('start_date_readable', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('end_date_readable', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_number', sa.Integer(), nullable=True))
    op.create_index(op.f('ix_ads_page_id'), 'ads', ['page_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_ads_page_id'), table_name='ads')
    op.drop_column('ads', 'page_number')
    op.drop_column('ads', 'end_date_readable')
    op.drop_column('ads', 'start_date_readable')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'card_media_types')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'main_image_urls')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'main_title')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'targeted_countries')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'display_format')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'start_date')
    op.drop_column('ads', 'is_active')
    op.drop_column('ads', 'collation_count')
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'page_id')
    # ### end Alembic commands ###



================================================
File: alembic/versions/8a69c9ce3517_add_duration_days_to_ads.py
================================================
"""add_duration_days_to_ads

Revision ID: 8a69c9ce3517
Revises: 014e83d13d0a
Create Date: 2025-07-08 11:42:09.030863

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '8a69c9ce3517'
down_revision: Union[str, None] = '014e83d13d0a'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    # First drop foreign key constraint and campaign_id column
    op.drop_index('ix_ads_campaign_id', table_name='ads')
    op.drop_constraint('ads_campaign_id_fkey', 'ads', type_='foreignkey')
    op.drop_column('ads', 'campaign_id')
    
    # Then add the new duration_days column
    op.add_column('ads', sa.Column('duration_days', sa.Integer(), nullable=True))
    op.create_index(op.f('ix_ads_duration_days'), 'ads', ['duration_days'], unique=False)
    
    # Finally drop the campaigns table and its indexes
    op.drop_index('ix_campaigns_content_signature', table_name='campaigns')
    op.drop_index('ix_campaigns_id', table_name='campaigns')
    op.drop_table('campaigns')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    # First recreate the campaigns table and its indexes
    op.create_table('campaigns',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('content_signature', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('creatives', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name='campaigns_pkey')
    )
    op.create_index('ix_campaigns_id', 'campaigns', ['id'], unique=False)
    op.create_index('ix_campaigns_content_signature', 'campaigns', ['content_signature'], unique=True)
    
    # Then remove duration_days column and index
    op.drop_index(op.f('ix_ads_duration_days'), table_name='ads')
    op.drop_column('ads', 'duration_days')
    
    # Finally add back campaign_id column and foreign key
    op.add_column('ads', sa.Column('campaign_id', sa.INTEGER(), autoincrement=False, nullable=True))
    op.create_index('ix_ads_campaign_id', 'ads', ['campaign_id'], unique=False)
    op.create_foreign_key('ads_campaign_id_fkey', 'ads', 'campaigns', ['campaign_id'], ['id'])
    # ### end Alembic commands ###



================================================
File: alembic/versions/a220f98e38c2_add_form_details_running_countries_.py
================================================
"""add_form_details_running_countries_structured_ad_copy_to_ads

Revision ID: a220f98e38c2
Revises: 779734a0c91c
Create Date: 2025-07-04 13:44:11.317930

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'a220f98e38c2'
down_revision: Union[str, None] = '779734a0c91c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('form_details', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('running_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('structured_ad_copy', sa.Text(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('ads', 'structured_ad_copy')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'form_details')
    # ### end Alembic commands ###



================================================
File: alembic/versions/a2feb1e20f39_add_meta_targeting_lead_form_and_.py
================================================
"""Add meta, targeting, lead_form, and creatives columns and remove obsolete fields

Revision ID: a2feb1e20f39
Revises: 5a527db67d43
Create Date: 2025-07-06 19:37:34.588344

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'a2feb1e20f39'
down_revision: Union[str, None] = '5a527db67d43'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('meta', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('targeting', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('lead_form', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('creatives', sa.JSON(), nullable=True))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_index('ix_ads_advertiser_page_id', table_name='ads')
    op.drop_index('ix_ads_campaign_id', table_name='ads')
    op.create_index(op.f('ix_ads_competitor_id'), 'ads', ['competitor_id'], unique=False)
    op.drop_column('ads', 'targeting_data')
    op.drop_column('ads', 'lead_form_standalone_fields')
    op.drop_column('ads', 'landing_page_url')
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'page_id')
    op.drop_column('ads', 'advertiser_page_url')
    op.drop_column('ads', 'advertiser_page_id')
    op.drop_column('ads', 'advertiser_page_likes')
    op.drop_column('ads', 'ad_copy')
    op.drop_column('ads', 'lead_form_questions')
    op.drop_column('ads', 'impressions_text')
    op.drop_column('ads', 'advertiser_page_name')
    op.drop_column('ads', 'is_active')
    op.drop_column('ads', 'campaign_id')
    op.drop_column('ads', 'cta_text')
    op.drop_column('ads', 'meta_start_date')
    op.drop_column('ads', 'meta_display_format')
    op.drop_column('ads', 'advertiser_page_profile_picture')
    op.drop_column('ads', 'start_date')
    op.drop_column('ads', 'platforms')
    op.drop_column('ads', 'cta_type')
    op.drop_column('ads', 'meta_is_active')
    op.drop_column('ads', 'media_url')
    op.drop_column('ads', 'meta_end_date')
    op.drop_column('ads', 'is_enhanced_processed')
    op.drop_column('ads', 'page_name')
    op.drop_column('ads', 'media_type')
    op.drop_column('ads', 'end_date')
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'display_format')
    op.drop_column('ads', 'creatives_data')
    op.drop_column('ads', 'meta_cta_type')
    op.drop_column('ads', 'publisher_platform')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('publisher_platform', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_cta_type', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('creatives_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('display_format', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('end_date', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('media_type', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.BOOLEAN(), autoincrement=False, nullable=False))
    op.add_column('ads', sa.Column('meta_end_date', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('media_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_is_active', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('cta_type', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('platforms', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('start_date', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_profile_picture', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_display_format', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_start_date', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('cta_text', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('campaign_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('is_active', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('impressions_text', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('lead_form_questions', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('ad_copy', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_likes', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('landing_page_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('lead_form_standalone_fields', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('targeting_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_index(op.f('ix_ads_competitor_id'), table_name='ads')
    op.create_index('ix_ads_campaign_id', 'ads', ['campaign_id'], unique=False)
    op.create_index('ix_ads_advertiser_page_id', 'ads', ['advertiser_page_id'], unique=False)
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'creatives')
    op.drop_column('ads', 'lead_form')
    op.drop_column('ads', 'targeting')
    op.drop_column('ads', 'meta')
    # ### end Alembic commands ###



================================================
File: alembic/versions/a7031f8c7467_add_enhanced_extraction_fields_and_keep_.py
================================================
"""add_enhanced_extraction_fields_and_keep_legacy

Revision ID: a7031f8c7467
Revises: 6168e4625f40
Create Date: 2025-07-06 14:49:31.579960

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'a7031f8c7467'
down_revision: Union[str, None] = '6168e4625f40'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/d26d6ee785d3_add_enhanced_extraction_fields_hybrid_.py
================================================
"""add_enhanced_extraction_fields_hybrid_model

Revision ID: d26d6ee785d3
Revises: a7031f8c7467
Create Date: 2025-07-06 14:54:11.470819

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'd26d6ee785d3'
down_revision: Union[str, None] = 'a7031f8c7467'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('main_title', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_image_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('page_categories', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('targeted_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('spend', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('currency', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('card_titles', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_bodies', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_links', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('form_details', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('running_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('creatives', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extraction_version', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('extraction_date', sa.DateTime(timezone=True), nullable=True))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'creatives_data')
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'landing_page_url')
    op.drop_column('ads', 'is_enhanced_processed')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=False))
    op.add_column('ads', sa.Column('landing_page_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('creatives_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'extraction_date')
    op.drop_column('ads', 'extraction_version')
    op.drop_column('ads', 'creatives')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'form_details')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'targeted_countries')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'main_image_urls')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'main_title')
    # ### end Alembic commands ###



================================================
File: app/__init__.py
================================================
# Empty file to make this directory a Python package 


================================================
File: app/celery.py
================================================
from celery import Celery
import os
from dotenv import load_dotenv

load_dotenv()

# Create Celery instance
celery = Celery(
    "ads_app",
    broker=os.getenv("CELERY_BROKER_URL", "redis://localhost:6379"),
    backend=os.getenv("CELERY_RESULT_BACKEND", "redis://localhost:6379"),
    include=[
        "app.tasks.basic_tasks",
        "app.tasks.ai_analysis_tasks", 
        "app.tasks.facebook_ads_scraper_task"
    ]
)

# Celery configuration
celery.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutes
    task_soft_time_limit=25 * 60,  # 25 minutes
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Configure periodic tasks - using actual registered tasks
celery.conf.beat_schedule = {
    'scrape-facebook-ads': {
        'task': 'app.tasks.facebook_ads_scraper_task.scrape_facebook_ads_task',
        'schedule': 3600.0,  # Run every hour
    },
}

if __name__ == "__main__":
    celery.start() 


================================================
File: app/celery_worker.py
================================================
from celery import Celery
from app.core.config import settings
import logging

# Configure logging
logging.basicConfig(level=getattr(logging, settings.LOG_LEVEL))
logger = logging.getLogger(__name__)

# Create Celery instance
celery_app = Celery(
    "ads_worker",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND,
    include=[
        "app.tasks.basic_tasks", 
        "app.tasks.ai_analysis_tasks", 
        "app.tasks.facebook_ads_scraper_task"
    ]
)

# Export the celery app for the CLI
app = celery_app

# Configure Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=settings.CELERY_TASK_TIME_LIMIT,
    task_soft_time_limit=settings.CELERY_TASK_SOFT_TIME_LIMIT,
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
    worker_log_format="[%(asctime)s: %(levelname)s/%(processName)s] %(message)s",
    worker_task_log_format="[%(asctime)s: %(levelname)s/%(processName)s][%(task_name)s(%(task_id)s)] %(message)s",
    broker_connection_retry_on_startup=True,
)

# Configure periodic tasks (Celery Beat) - using actual registered tasks
celery_app.conf.beat_schedule = {
    'scrape-facebook-ads': {
        'task': 'facebook_ads_scraper.scrape_ads',
        'schedule': 3600.0,  # Run every hour
    },
    'batch-ai-analysis': {
        'task': 'app.tasks.ai_analysis_tasks.batch_ai_analysis_task',
        'schedule': 21600.0,  # Run every 6 hours
        'kwargs': {'ad_ids': []}  # Empty list as default
    },
}

# Add Redis broker configuration
celery_app.conf.broker_transport_options = {
    'visibility_timeout': 3600,
    'fanout_prefix': True,
    'fanout_patterns': True
}

if __name__ == "__main__":
    logger.info("Starting Celery worker...")
    celery_app.start() 


================================================
File: app/database.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

# Database configuration
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@localhost:5432/ads_db")

# Create engine
engine = create_engine(DATABASE_URL)

# Create SessionLocal class
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create Base class
Base = declarative_base()

# Dependency to get DB session
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close() 


================================================
File: app/main.py
================================================
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import uvicorn
from pathlib import Path
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import configuration
from app.core.config import settings

# Import routers
from app.routers import health, ads, competitors
from app.api import internal_router

# Database imports
from app.database import engine, Base

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    Base.metadata.create_all(bind=engine)
    yield
    # Shutdown
    pass

# Create FastAPI app
app = FastAPI(
    title=settings.APP_NAME,
    description="A comprehensive API for managing Facebook Ads data with AI-powered analysis",
    version=settings.APP_VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(health.router, prefix=settings.API_V1_PREFIX, tags=["health"])
app.include_router(ads.router, prefix=settings.API_V1_PREFIX, tags=["ads"])
app.include_router(competitors.router, prefix=f"{settings.API_V1_PREFIX}/competitors", tags=["competitors"])
app.include_router(internal_router, prefix=settings.API_V1_PREFIX, tags=["internal"])

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": f"Welcome to {settings.APP_NAME}",
        "version": settings.APP_VERSION,
        "docs": "/docs",
        "redoc": "/redoc",
        "health": f"{settings.API_V1_PREFIX}/health"
    }

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"detail": f"Internal server error: {str(exc)}"}
    )

if __name__ == "__main__":
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG
    ) 



================================================
File: app/api/__init__.py
================================================
from .internal_api import router as internal_router

__all__ = ["internal_router"] 


================================================
File: app/api/internal_api.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Header, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.orm import Session
from typing import List, Optional
import logging

from app.database import get_db
from app.core.config import settings
from app.services.ingestion_service import DataIngestionService
from app.models.dto import AdCreate, AdIngestionResponse

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/internal", tags=["internal"])

# Security
security = HTTPBearer(auto_error=False)

# API Key for internal services (can be set via environment variable)
INTERNAL_API_KEY = settings.INTERNAL_API_KEY


async def verify_api_key(
    authorization: Optional[HTTPAuthorizationCredentials] = Depends(security),
    x_api_key: Optional[str] = Header(None)
) -> bool:
    """
    Verify API key for internal endpoints.
    Supports both Bearer token and X-API-Key header methods.
    """
    provided_key = None
    
    # Check Bearer token first
    if authorization:
        provided_key = authorization.credentials
    
    # Check X-API-Key header as fallback
    elif x_api_key:
        provided_key = x_api_key
    
    if not provided_key:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="API key required. Provide via Authorization header (Bearer token) or X-API-Key header"
        )
    
    if provided_key != INTERNAL_API_KEY:
        logger.warning(f"Invalid API key attempt: {provided_key[:10]}...")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API key"
        )
    
    return True


@router.post("/ingest", response_model=AdIngestionResponse)
async def ingest_ad(
    ad_data: AdCreate,
    db: Session = Depends(get_db),
    _: bool = Depends(verify_api_key)
) -> AdIngestionResponse:
    """
    Ingest a single ad from external scraper.
    
    This is the main endpoint that external scraping scripts should use
    to submit raw ad data to the system.
    
    **Security**: Requires API key authentication.
    
    **Process**:
    1. Validates incoming ad data
    2. Creates/updates competitor record
    3. Creates/updates ad record
    4. Triggers AI analysis task
    
    **Example Usage**:
    ```bash
    curl -X POST "http://localhost:8000/api/v1/internal/ingest" \
         -H "X-API-Key: your-api-key" \
         -H "Content-Type: application/json" \
         -d '{
           "ad_archive_id": "1234567890",
           "competitor": {
             "name": "Example Company",
             "page_id": "example123"
           },
           "ad_copy": "Amazing product!",
           "media_type": "image"
         }'
    ```
    """
    logger.info(f"Received ingestion request for ad: {ad_data.ad_archive_id}")
    
    try:
        # Create ingestion service instance
        ingestion_service = DataIngestionService(db)
        
        # Process the ad ingestion
        result = await ingestion_service.ingest_ad(ad_data)
        
        if result.success:
            logger.info(f"Successfully ingested ad: {ad_data.ad_archive_id} (ID: {result.ad_id})")
        else:
            logger.warning(f"Failed to ingest ad: {ad_data.ad_archive_id} - {result.message}")
        
        return result
        
    except Exception as e:
        logger.error(f"Unexpected error in ingest endpoint: {e}")
        return AdIngestionResponse(
            success=False,
            message=f"Internal server error: {str(e)}"
        )


@router.post("/ingest/batch")
async def batch_ingest_ads(
    ads_data: List[AdCreate],
    db: Session = Depends(get_db),
    _: bool = Depends(verify_api_key)
) -> dict:
    """
    Batch ingest multiple ads from external scraper.
    
    This endpoint allows efficient ingestion of multiple ads at once,
    useful for bulk scraping operations.
    
    **Security**: Requires API key authentication.
    
    **Limits**: Maximum 100 ads per batch request.
    
    **Example Usage**:
    ```bash
    curl -X POST "http://localhost:8000/api/v1/internal/ingest/batch" \
         -H "X-API-Key: your-api-key" \
         -H "Content-Type: application/json" \
         -d '[
           {
             "ad_archive_id": "1234567890",
             "competitor": {"name": "Company A", "page_id": "compA123"},
             "ad_copy": "Product A"
           },
           {
             "ad_archive_id": "1234567891", 
             "competitor": {"name": "Company B", "page_id": "compB123"},
             "ad_copy": "Product B"
           }
         ]'
    ```
    """
    # Validate batch size
    if len(ads_data) > 100:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Batch size too large. Maximum 100 ads per request."
        )
    
    if len(ads_data) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Empty batch. At least one ad required."
        )
    
    logger.info(f"Received batch ingestion request for {len(ads_data)} ads")
    
    try:
        # Create ingestion service instance
        ingestion_service = DataIngestionService(db)
        
        # Process the batch ingestion
        result = await ingestion_service.batch_ingest_ads(ads_data)
        
        logger.info(f"Batch ingestion completed: {result['successful']}/{result['total_ads']} successful")
        
        return result
        
    except Exception as e:
        logger.error(f"Unexpected error in batch ingest endpoint: {e}")
        return {
            "success": False,
            "error": f"Internal server error: {str(e)}",
            "total_ads": len(ads_data),
            "successful": 0,
            "failed": len(ads_data)
        }


@router.get("/stats")
async def get_ingestion_stats(
    db: Session = Depends(get_db),
    _: bool = Depends(verify_api_key)
) -> dict:
    """
    Get ingestion statistics.
    
    **Security**: Requires API key authentication.
    
    Returns statistics about ingested ads, competitors, and recent activity.
    """
    logger.info("Fetching ingestion statistics")
    
    try:
        ingestion_service = DataIngestionService(db)
        stats = await ingestion_service.get_ingestion_stats()
        
        return stats
        
    except Exception as e:
        logger.error(f"Error fetching ingestion stats: {e}")
        return {
            "error": f"Failed to fetch statistics: {str(e)}"
        }


@router.get("/health")
async def internal_health_check(_: bool = Depends(verify_api_key)) -> dict:
    """
    Internal health check endpoint.
    
    **Security**: Requires API key authentication.
    
    Used by internal services to verify the ingestion API is available.
    """
    return {
        "status": "ok",
        "service": "internal-ingestion-api",
        "endpoints": [
            "POST /internal/ingest - Single ad ingestion",
            "POST /internal/ingest/batch - Batch ad ingestion", 
            "GET /internal/stats - Ingestion statistics",
            "GET /internal/health - This health check"
        ]
    }


@router.get("/api-key/test")
async def test_api_key(_: bool = Depends(verify_api_key)) -> dict:
    """
    Test API key authentication.
    
    **Security**: Requires API key authentication.
    
    Useful for testing if your API key is working correctly.
    """
    return {
        "status": "authenticated",
        "message": "API key is valid",
        "note": "You can now use the ingestion endpoints"
    }

# Debug endpoint removed for security 


================================================
File: app/core/config.py
================================================
import os
from typing import List


class Settings:
    """
    Application settings and configuration management.
    Uses simple environment variable access for now.
    """
    
    # Application Settings
    APP_NAME: str = "Ads Management API"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = os.getenv("DEBUG", "true").lower() == "true"
    
    # Security
    SECRET_KEY: str = os.getenv("SECRET_KEY", "your-secret-key-here-change-in-production")
    INTERNAL_API_KEY: str = os.getenv("INTERNAL_API_KEY", os.getenv("SECRET_KEY", "your-secret-key-here-change-in-production") + "-internal")
    
    # Database Configuration
    DATABASE_URL: str = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@db:5432/ads_db")
    
    # Redis Configuration
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://redis:6379")
    
    # Celery Configuration
    CELERY_BROKER_URL: str = os.getenv("CELERY_BROKER_URL", "redis://redis:6379")
    CELERY_RESULT_BACKEND: str = os.getenv("CELERY_RESULT_BACKEND", "redis://redis:6379")
    
    # CORS Configuration
    CORS_ORIGINS: List[str] = os.getenv("CORS_ORIGINS", "http://localhost:3000").split(",")
    
    # API Configuration
    API_V1_PREFIX: str = "/api/v1"
    
    # Task Configuration
    CELERY_TASK_TIME_LIMIT: int = 30 * 60  # 30 minutes
    CELERY_TASK_SOFT_TIME_LIMIT: int = 25 * 60  # 25 minutes
    
    # AI Service Configuration
    GOOGLE_AI_API_KEY: str = os.getenv("GOOGLE_AI_API_KEY", "")
    GOOGLE_AI_MODEL: str = os.getenv("GOOGLE_AI_MODEL", "gemini-pro")
    AI_ANALYSIS_ENABLED: bool = os.getenv("AI_ANALYSIS_ENABLED", "true").lower() == "true"
    
    # Logging Configuration
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")


# Create settings instance
settings = Settings()


def get_settings() -> Settings:
    """
    Dependency to get settings instance.
    Can be used in FastAPI dependency injection.
    """
    return settings 



================================================
File: app/models/__init__.py
================================================
from .competitor import Competitor
from .ad import Ad
from .ad_analysis import AdAnalysis
from .task_status import TaskStatus

__all__ = ["Competitor", "Ad", "AdAnalysis", "TaskStatus"] 


================================================
File: app/models/ad.py
================================================
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, JSON, Boolean, func, Float, ARRAY
from sqlalchemy.orm import relationship
from app.database import Base


class Ad(Base):
    __tablename__ = "ads"

    # Core identification fields
    id = Column(Integer, primary_key=True, index=True)
    competitor_id = Column(Integer, ForeignKey("competitors.id"), nullable=False, index=True)
    ad_archive_id = Column(String, unique=True, nullable=False, index=True)
    
    # Basic tracking fields
    date_found = Column(DateTime(timezone=True), nullable=False, index=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)
    
    # Duration field - calculated during scraping
    duration_days = Column(Integer, nullable=True, index=True)  # Number of days the ad has been running
    
    # Raw data from initial scrape
    raw_data = Column(JSON, nullable=True)
    
    # Relationships
    competitor = relationship("Competitor", back_populates="ads")
    analysis = relationship("AdAnalysis", uselist=False, back_populates="ad", cascade="all, delete-orphan")
    
    # New structured fields for enhanced extraction
    meta = Column(JSON, nullable=True)
    targeting = Column(JSON, nullable=True)
    lead_form = Column(JSON, nullable=True)
    creatives = Column(JSON, nullable=True)

    def __repr__(self):
        return f"<Ad(id={self.id}, ad_archive_id='{self.ad_archive_id}')>"
    
    def to_dict(self):
        """Convert Ad instance to dictionary for JSON serialization"""
        return {
            "id": self.id,
            "competitor_id": self.competitor_id,
            "ad_archive_id": self.ad_archive_id,
            "date_found": self.date_found.isoformat() if self.date_found else None,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
            "raw_data": self.raw_data,
            "meta": self.meta,
            "targeting": self.targeting,
            "lead_form": self.lead_form,
            "creatives": self.creatives
        }
    
    def to_enhanced_format(self):
        """Convert to the new enhanced frontend format"""
        return {
            "ad_archive_id": self.ad_archive_id,
            "meta": self.meta,
            "targeting": self.targeting,
            "lead_form": self.lead_form,
            "creatives": self.creatives
        } 


================================================
File: app/models/ad_analysis.py
================================================
from sqlalchemy import Column, Integer, String, Text, Float, DateTime, ForeignKey, JSON, func
from sqlalchemy.orm import relationship
from app.database import Base


class AdAnalysis(Base):
    __tablename__ = "ad_analyses"

    id = Column(Integer, primary_key=True, index=True)
    ad_id = Column(Integer, ForeignKey("ads.id"), unique=True, nullable=False)
    summary = Column(Text, nullable=True)
    hook_score = Column(Float, nullable=True)  # Score for how engaging the hook is
    overall_score = Column(Float, nullable=True)  # Overall ad effectiveness score
    
    # JSONB columns for flexible AI data storage
    ai_prompts = Column(JSON, nullable=True)  # Store the prompts sent to AI
    raw_ai_response = Column(JSON, nullable=True)  # Store complete AI response
    
    # Additional analysis fields
    target_audience = Column(String, nullable=True)
    ad_format_analysis = Column(JSON, nullable=True)  # Analysis of ad format effectiveness
    competitor_insights = Column(JSON, nullable=True)  # Insights about competitor strategy
    content_themes = Column(JSON, nullable=True)  # Identified themes in the ad content
    performance_predictions = Column(JSON, nullable=True)  # Predicted performance metrics
    
    # Metadata
    analysis_version = Column(String, nullable=True)  # Track AI model version used
    confidence_score = Column(Float, nullable=True)  # AI confidence in the analysis
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Relationships
    ad = relationship("Ad", back_populates="analysis")

    def __repr__(self):
        return f"<AdAnalysis(id={self.id}, ad_id={self.ad_id}, overall_score={self.overall_score})>" 


================================================
File: app/models/competitor.py
================================================
from sqlalchemy import Column, Integer, String, Boolean, DateTime, func
from sqlalchemy.orm import relationship
from app.database import Base


class Competitor(Base):
    __tablename__ = "competitors"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, nullable=False, index=True)
    page_id = Column(String, unique=True, nullable=False, index=True)
    page_url = Column(String, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Relationship to ads
    ads = relationship("Ad", back_populates="competitor")

    def __repr__(self):
        return f"<Competitor(id={self.id}, name='{self.name}', page_id='{self.page_id}')>" 


================================================
File: app/models/task_status.py
================================================
from sqlalchemy import Column, Integer, String, DateTime, JSON, func
from app.database import Base


class TaskStatus(Base):
    """Model for tracking task status and results"""
    __tablename__ = "task_status"

    id = Column(Integer, primary_key=True, index=True)
    task_id = Column(String, unique=True, index=True, nullable=False)
    status = Column(String, nullable=False)  # pending, running, completed, failed
    result = Column(JSON, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    def __repr__(self):
        return f"<TaskStatus(id={self.id}, task_id='{self.task_id}', status='{self.status}')>" 



================================================
File: app/models/dto/__init__.py
================================================
from .ad_dto import AdCreate, CompetitorCreateDTO, AdIngestionResponse
from .competitor_dto import (
    CompetitorCreateDTO,
    CompetitorUpdateDTO,
    CompetitorResponseDTO,
    CompetitorDetailResponseDTO,
    PaginatedCompetitorResponseDTO,
    CompetitorFilterParams,
    CompetitorStatsResponseDTO
)

__all__ = [
    "AdCreate", 
    "CompetitorCreateDTO", 
    "AdIngestionResponse",
    "CompetitorUpdateDTO",
    "CompetitorResponseDTO",
    "CompetitorDetailResponseDTO",
    "PaginatedCompetitorResponseDTO",
    "CompetitorFilterParams",
    "CompetitorStatsResponseDTO"
] 


================================================
File: app/models/dto/ad_dto.py
================================================
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime

from .competitor_dto import CompetitorResponseDTO


class ExtraTextItemDTO(BaseModel):
    """
    DTO for a single item in the extra_texts field.
    """
    text: str
    
    class Config:
        from_attributes = True


class CompetitorCreateDTO(BaseModel):
    """
    DTO for creating or finding competitor records.
    Used when ingesting ad data to ensure competitor exists.
    """
    name: str = Field(..., description="Competitor/page name")
    page_id: str = Field(..., description="Facebook page ID")
    is_active: bool = Field(default=True, description="Whether competitor is active")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class AdMeta(BaseModel):
    """
    DTO for ad meta information.
    """
    is_active: Optional[bool] = Field(None, description="Whether the ad is active")
    cta_type: Optional[str] = Field(None, description="Call to action type")
    display_format: Optional[str] = Field(None, description="Display format")
    start_date: Optional[str] = Field(None, description="Start date of the ad")
    end_date: Optional[str] = Field(None, description="End date of the ad")
    
    class Config:
        from_attributes = True


class AgeRange(BaseModel):
    min: int
    max: int


class Location(BaseModel):
    name: str
    num_obfuscated: int = 0
    type: str
    excluded: bool = False


class GenderAgeBreakdown(BaseModel):
    age_range: str
    male: Optional[int] = None
    female: Optional[int] = None
    unknown: Optional[int] = None


class CountryReachBreakdown(BaseModel):
    country: str
    age_gender_breakdowns: List[GenderAgeBreakdown]


class AdTargeting(BaseModel):
    """
    DTO for ad targeting information.
    """
    locations: Optional[List[str]] = Field(None, description="Targeted locations")
    age_range: Optional[str] = Field(None, description="Targeted age range")
    gender: Optional[str] = Field(None, description="Targeted gender")
    reach_breakdown: Optional[Dict[str, Any]] = Field(None, description="Reach breakdown")
    total_reach: Optional[int] = Field(None, description="Total reach")
    
    class Config:
        from_attributes = True


class LeadFormQuestion(BaseModel):
    """
    DTO for a lead form question.
    """
    question_id: str = Field(..., description="Question ID")
    question_text: str = Field(..., description="Question text")
    question_type: str = Field(..., description="Question type")
    options: Optional[List[str]] = Field(None, description="Question options")
    
    class Config:
        from_attributes = True


class LeadForm(BaseModel):
    """
    DTO for lead form information.
    """
    questions: Optional[Dict[str, Any]] = Field(None, description="Lead form questions")
    standalone_fields: Optional[List[str]] = Field(None, description="Standalone fields")
    
    class Config:
        from_attributes = True


class CreativeMedia(BaseModel):
    """
    DTO for creative media.
    """
    url: str = Field(..., description="Media URL")
    type: str = Field(..., description="Media type")
    
    class Config:
        from_attributes = True


class CreativeCta(BaseModel):
    text: Optional[str] = None
    type: Optional[str] = None


class CreativeLink(BaseModel):
    url: Optional[str] = None
    caption: Optional[str] = None


class Creative(BaseModel):
    """
    DTO for ad creative.
    """
    id: str = Field(..., description="Creative ID")
    title: Optional[str] = Field(None, description="Creative title")
    body: Optional[str] = Field(None, description="Creative body")
    caption: Optional[str] = Field(None, description="Creative caption")
    link_url: Optional[str] = Field(None, description="Creative link URL")
    link_description: Optional[str] = Field(None, description="Creative link description")
    media: Optional[List[CreativeMedia]] = Field(None, description="Creative media")
    
    class Config:
        from_attributes = True


class AdBase(BaseModel):
    ad_archive_id: str
    meta: Optional[AdMeta] = None
    targeting: Optional[AdTargeting] = None
    lead_form: Optional[LeadForm] = None
    creatives: List[Creative] = []


class AdCreate(AdBase):
    competitor_id: int


class AdUpdate(BaseModel):
    meta: Optional[AdMeta] = None
    targeting: Optional[AdTargeting] = None
    lead_form: Optional[LeadForm] = None
    creatives: Optional[List[Creative]] = None


class AdInDB(AdBase):
    id: int
    competitor_id: int
    date_found: datetime
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    class Config:
        orm_mode = True


class AdResponse(AdBase):
    id: int
    competitor_id: int
    date_found: datetime
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    class Config:
        orm_mode = True


class AdFilterParams(BaseModel):
    page: int = 1
    page_size: int = 20
    competitor_id: Optional[int] = None
    competitor_name: Optional[str] = None
    media_type: Optional[str] = None
    has_analysis: Optional[bool] = None
    min_hook_score: Optional[float] = None
    max_hook_score: Optional[float] = None
    min_overall_score: Optional[float] = None
    max_overall_score: Optional[float] = None
    min_duration_days: Optional[int] = None
    max_duration_days: Optional[int] = None
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None
    is_active: Optional[bool] = None
    search: Optional[str] = None
    sort_by: Optional[str] = "date_found"
    sort_order: Optional[str] = "desc"


class PaginatedAdResponse(BaseModel):
    items: List[AdResponse]
    total: int
    page: int
    limit: int
    total_pages: int


class AdvertiserInfo(BaseModel):
    page_id: str
    page_name: str
    page_url: Optional[str] = None
    page_likes: Optional[int] = None
    page_profile_picture: Optional[str] = None


class Campaign(BaseModel):
    campaign_id: str
    platforms: List[str] = []
    ads: List[AdResponse] = []


class AdsFullResponse(BaseModel):
    advertiser_info: AdvertiserInfo
    campaigns: List[Campaign] = []


class AdStats(BaseModel):
    total_ads: int = 0
    active_ads: int = 0
    with_lead_form: int = 0
    platforms: Dict[str, int] = {}
    media_types: Dict[str, int] = {}


class AdIngestionResponse(BaseModel):
    """
    Response model for ad ingestion operations.
    """
    success: bool = Field(..., description="Whether ingestion was successful")
    ad_id: Optional[int] = Field(None, description="ID of created/updated ad")
    competitor_id: Optional[int] = Field(None, description="ID of associated competitor")
    analysis_task_id: Optional[str] = Field(None, description="ID of triggered analysis task")
    message: str = Field(..., description="Human-readable result message")
    
    class Config:
        json_schema_extra = {
            "example": {
                "success": True,
                "ad_id": 123,
                "competitor_id": 456,
                "analysis_task_id": "some-task-id",
                "message": "Ad ingested successfully and analysis task started."
            }
        }


class AdAnalysisResponseDTO(BaseModel):
    """
    DTO for AI analysis data in ad responses.
    """
    id: int = Field(..., description="Analysis ID")
    summary: Optional[str] = Field(None, description="AI-generated summary")
    hook_score: Optional[float] = Field(None, description="Hook effectiveness score (1-10)")
    overall_score: Optional[float] = Field(None, description="Overall ad effectiveness score (1-10)")
    confidence_score: Optional[float] = Field(None, description="AI confidence level (0-1)")
    target_audience: Optional[str] = Field(None, description="Target audience description")
    content_themes: Optional[List[str]] = Field(None, description="Identified content themes")
    analysis_version: Optional[str] = Field(None, description="Analysis version used")
    created_at: datetime = Field(..., description="Analysis creation timestamp")
    updated_at: datetime = Field(..., description="Analysis update timestamp")
    
    class Config:
        from_attributes = True


class AdResponseDTO(BaseModel):
    """
    DTO for ad data in list responses.
    """
    id: int = Field(..., description="Ad ID")
    ad_archive_id: str = Field(..., description="Facebook Ad Library ID")
    competitor: CompetitorResponseDTO = Field(..., description="Competitor information")
    
    # Ad content
    ad_copy: Optional[str] = Field(None, description="Main ad text/copy")
    main_title: Optional[str] = Field(None, description="Main title")
    main_body_text: Optional[str] = Field(None, description="Main body text")
    main_caption: Optional[str] = Field(None, description="Main caption")
    
    # Media information
    media_type: Optional[str] = Field(None, description="Type of media")
    media_url: Optional[str] = Field(None, description="Primary media URL")
    main_image_urls: Optional[List[str]] = Field(None, description="Main image URLs")
    main_video_urls: Optional[List[str]] = Field(None, description="Main video URLs")
    
    # Page information
    page_name: Optional[str] = Field(None, description="Page name")
    page_id: Optional[str] = Field(None, description="Page ID")
    
    # Platform and targeting
    publisher_platform: Optional[List[str]] = Field(None, description="Publisher platforms")
    targeted_countries: Optional[List[str]] = Field(None, description="Targeted countries")
    
    # Performance indicators
    impressions_text: Optional[str] = Field(None, description="Impressions text")
    spend: Optional[str] = Field(None, description="Spend amount")
    
    # Call-to-action
    cta_text: Optional[str] = Field(None, description="CTA text")
    cta_type: Optional[str] = Field(None, description="CTA type")
    
    # Dates
    date_found: datetime = Field(..., description="When ad was discovered")
    start_date: Optional[str] = Field(None, description="Ad start date")
    end_date: Optional[str] = Field(None, description="Ad end date")
    is_active: Optional[bool] = Field(None, description="Whether ad is active")
    duration_days: Optional[int] = Field(None, description="Duration in days the ad has been running")
    
    # Timestamps
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Update timestamp")
    
    # AI Analysis (optional)
    analysis: Optional[AdAnalysisResponseDTO] = Field(None, description="AI analysis data")
    
    # New fields to match frontend_payload_final.json
    meta: Optional[AdMeta] = Field(None, description="Meta information about the ad")
    targeting: Optional[AdTargeting] = Field(None, description="Targeting information")
    lead_form: Optional[LeadForm] = Field(None, description="Lead form information")
    creatives: List[Creative] = Field(default_factory=list, description="Ad creatives")
    
    class Config:
        from_attributes = True


class AdDetailResponseDTO(BaseModel):
    """
    DTO for detailed ad data in single ad responses.
    """
    id: int = Field(..., description="Ad ID")
    ad_archive_id: str = Field(..., description="Facebook Ad Library ID")
    competitor: CompetitorResponseDTO = Field(..., description="Competitor information")
    
    # All ad content
    ad_copy: Optional[str] = Field(None, description="Main ad text/copy")
    main_title: Optional[str] = Field(None, description="Main title")
    main_body_text: Optional[str] = Field(None, description="Main body text")
    main_caption: Optional[str] = Field(None, description="Main caption")
    main_link_url: Optional[str] = Field(None, description="Main link URL")
    main_link_description: Optional[str] = Field(None, description="Main link description")
    
    # Media information
    media_type: Optional[str] = Field(None, description="Type of media")
    media_url: Optional[str] = Field(None, description="Primary media URL")
    main_image_urls: Optional[List[str]] = Field(None, description="Main image URLs")
    main_video_urls: Optional[List[str]] = Field(None, description="Main video URLs")
    
    # Page information
    page_name: Optional[str] = Field(None, description="Page name")
    page_id: Optional[str] = Field(None, description="Page ID")
    page_like_count: Optional[int] = Field(None, description="Page like count")
    page_categories: Optional[List[str]] = Field(None, description="Page categories")
    page_profile_uri: Optional[str] = Field(None, description="Page profile URI")
    page_profile_picture_url: Optional[str] = Field(None, description="Page profile picture URL")
    
    # Platform and targeting
    publisher_platform: Optional[List[str]] = Field(None, description="Publisher platforms")
    targeted_countries: Optional[List[str]] = Field(None, description="Targeted countries")
    display_format: Optional[str] = Field(None, description="Display format")
    
    # Performance indicators
    impressions_text: Optional[str] = Field(None, description="Impressions text")
    impressions_index: Optional[int] = Field(None, description="Impressions index")
    spend: Optional[str] = Field(None, description="Spend amount")
    currency: Optional[str] = Field(None, description="Currency")
    
    # Call-to-action
    cta_text: Optional[str] = Field(None, description="CTA text")
    cta_type: Optional[str] = Field(None, description="CTA type")
    
    # Additional content
    extra_texts: Optional[List[ExtraTextItemDTO]] = Field(None, description="Extra text content")
    extra_links: Optional[List[str]] = Field(None, description="Extra links")
    
    # Content flags
    contains_sensitive_content: Optional[bool] = Field(None, description="Contains sensitive content")
    contains_digital_created_media: Optional[bool] = Field(None, description="Contains digital created media")
    
    # Dates
    date_found: datetime = Field(..., description="When ad was discovered")
    start_date: Optional[str] = Field(None, description="Ad start date")
    end_date: Optional[str] = Field(None, description="Ad end date")
    is_active: Optional[bool] = Field(None, description="Whether ad is active")
    
    # Timestamps
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Update timestamp")
    
    # AI Analysis (optional)
    analysis: Optional[AdAnalysisResponseDTO] = Field(None, description="AI analysis data")
    
    # Raw data (optional)
    raw_data: Optional[Dict[str, Any]] = Field(None, description="Raw API response data")
    
    # New fields to match frontend_payload_final.json
    meta: Optional[AdMeta] = Field(None, description="Meta information about the ad")
    targeting: Optional[AdTargeting] = Field(None, description="Targeting information")
    lead_form: Optional[LeadForm] = Field(None, description="Lead form information")
    creatives: List[Creative] = Field(default_factory=list, description="Ad creatives")
    
    class Config:
        from_attributes = True


class PaginationMetadata(BaseModel):
    """
    DTO for pagination metadata.
    """
    page: int = Field(..., description="Current page number")
    page_size: int = Field(..., description="Number of items per page")
    total_items: int = Field(..., description="Total number of items")
    total_pages: int = Field(..., description="Total number of pages")
    has_next: bool = Field(..., description="Whether there's a next page")
    has_previous: bool = Field(..., description="Whether there's a previous page")


class PaginatedAdResponseDTO(BaseModel):
    """
    DTO for paginated ad responses.
    """
    data: List[AdResponseDTO] = Field(..., description="List of ads")
    pagination: PaginationMetadata = Field(..., description="Pagination metadata")
    
    class Config:
        json_schema_extra = {
            "example": {
                "data": [
                    {
                        "id": 1,
                        "ad_archive_id": "1557310628577134",
                        "competitor": {
                            "page_id": "1591077094491398",
                            "page_name": "Binghatti"
                        },
                        "ad_copy": "ندعوك لحضور فعالية حصرية",
                        "media_type": "video",
                        "page_name": "Binghatti",
                        "analysis": {
                            "hook_score": 8.5,
                            "overall_score": 7.8,
                            "summary": "Exclusive real estate event invitation"
                        }
                    }
                ],
                "pagination": {
                    "page": 1,
                    "page_size": 20,
                    "total_items": 150,
                    "total_pages": 8,
                    "has_next": True,
                    "has_previous": False
                }
            }
        }


class AdStatsResponseDTO(BaseModel):
    """
    DTO for ad statistics response.
    """
    total_ads: int = Field(..., description="Total number of ads")
    active_ads: int = Field(..., description="Number of active ads")
    total_competitors: int = Field(..., description="Total number of competitors")
    active_competitors: int = Field(..., description="Number of active competitors")
    analyzed_ads: int = Field(..., description="Number of ads with AI analysis")
    analysis_coverage: float = Field(..., description="Percentage of ads with analysis")
    
    # Media type breakdown
    media_type_breakdown: Dict[str, int] = Field(..., description="Breakdown by media type")
    
    # Platform breakdown
    platform_breakdown: Dict[str, int] = Field(..., description="Breakdown by platform")
    
    # Recent activity
    recent_ads_7_days: int = Field(..., description="Ads added in last 7 days")
    recent_ads_30_days: int = Field(..., description="Ads added in last 30 days")
    
    # Analysis scores
    avg_hook_score: Optional[float] = Field(None, description="Average hook score")
    avg_overall_score: Optional[float] = Field(None, description="Average overall score")
    
    # Last update
    last_updated: datetime = Field(..., description="When stats were last updated")
    
    class Config:
        json_schema_extra = {
            "example": {
                "total_ads": 1250,
                "active_ads": 890,
                "total_competitors": 45,
                "active_competitors": 38,
                "analyzed_ads": 1100,
                "analysis_coverage": 88.0,
                "media_type_breakdown": {
                    "image": 650,
                    "video": 400,
                    "carousel": 200
                },
                "platform_breakdown": {
                    "FACEBOOK": 800,
                    "INSTAGRAM": 450
                },
                "recent_ads_7_days": 85,
                "recent_ads_30_days": 320,
                "avg_hook_score": 7.2,
                "avg_overall_score": 6.8,
                "last_updated": "2024-01-15T10:30:00Z"
            }
        } 


================================================
File: app/models/dto/competitor_dto.py
================================================
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime


class CompetitorCreateDTO(BaseModel):
    """DTO for creating a new competitor"""
    name: str = Field(..., description="Competitor name", min_length=1, max_length=255)
    page_id: str = Field(..., description="Facebook page ID", min_length=1, max_length=100)
    is_active: bool = Field(True, description="Whether the competitor is active")


class CompetitorUpdateDTO(BaseModel):
    """DTO for updating an existing competitor"""
    name: Optional[str] = Field(None, description="Competitor name", min_length=1, max_length=255)
    page_id: Optional[str] = Field(None, description="Facebook page ID", min_length=1, max_length=100)
    is_active: Optional[bool] = Field(None, description="Whether the competitor is active")


class CompetitorResponseDTO(BaseModel):
    """DTO for competitor response data"""
    id: int
    name: str
    page_id: str
    page_url: Optional[str] = None
    is_active: bool
    ads_count: Optional[int] = Field(0, description="Number of ads for this competitor")
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True


class CompetitorDetailResponseDTO(BaseModel):
    """DTO for detailed competitor response with additional stats"""
    id: int
    name: str
    page_id: str
    is_active: bool
    ads_count: int = Field(0, description="Total number of ads")
    active_ads_count: int = Field(0, description="Number of active ads")
    analyzed_ads_count: int = Field(0, description="Number of analyzed ads")
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True


class PaginatedCompetitorResponseDTO(BaseModel):
    """DTO for paginated competitor response"""
    data: List[CompetitorResponseDTO]
    total: int
    page: int
    page_size: int
    total_pages: int
    has_next: bool
    has_previous: bool


class CompetitorFilterParams(BaseModel):
    """DTO for competitor filtering parameters"""
    page: int = Field(1, ge=1, description="Page number")
    page_size: int = Field(20, ge=1, le=100, description="Number of items per page")
    is_active: Optional[bool] = Field(None, description="Filter by active status")
    search: Optional[str] = Field(None, description="Search in competitor names")
    sort_by: Optional[str] = Field("created_at", description="Sort by field")
    sort_order: Optional[str] = Field("desc", description="Sort order (asc/desc)")


class CompetitorStatsResponseDTO(BaseModel):
    """DTO for competitor statistics"""
    total_competitors: int
    active_competitors: int
    inactive_competitors: int
    competitors_with_ads: int
    total_ads_across_competitors: int
    avg_ads_per_competitor: float 



================================================
File: app/routers/__init__.py
================================================
# Empty file to make this directory a Python package 


================================================
File: app/routers/ads.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List, Optional, TYPE_CHECKING, Dict
from datetime import datetime
import logging
from pydantic import BaseModel, Field
import uuid
import json

from app.database import get_db
from app.models.dto.ad_dto import (
    PaginatedAdResponseDTO,
    AdDetailResponseDTO,
    AdFilterParams,
    AdStatsResponseDTO,
    AdResponseDTO
)
from app.models import Ad, TaskStatus, Competitor
from app.services.facebook_ads_scraper import FacebookAdsScraperService, FacebookAdsScraperConfig
from app.services.ingestion_service import DataIngestionService
from app.services.enhanced_ad_extraction import EnhancedAdExtractionService

# Import Celery tasks
from app.tasks.basic_tasks import add_together, test_task, long_running_task
from app.tasks.facebook_ads_scraper_task import scrape_facebook_ads_task, scrape_competitor_ads_task
from app.tasks.ai_analysis_tasks import ai_analysis_task, batch_ai_analysis_task

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    from app.services.ad_service import AdService

router = APIRouter()
logger = logging.getLogger(__name__)

# ========================================
# Request/Response Models
# ========================================

class BulkDeleteRequest(BaseModel):
    ad_ids: List[int]

class TaskResponse(BaseModel):
    task_id: str
    status: str
    message: str

class FacebookAdsScraperRequest(BaseModel):
    """
    Request model for scraping Facebook ads.
    """
    view_all_page_id: Optional[str] = "1591077094491398"
    countries: Optional[List[str]] = ["AE"]
    max_pages: Optional[int] = 10
    delay_between_requests: Optional[int] = 1
    active_status: Optional[str] = "active"
    ad_type: Optional[str] = "ALL"
    media_type: Optional[str] = "all"
    search_type: Optional[str] = "page"
    query_string: Optional[str] = ""
    save_json: Optional[bool] = False

class CompetitorAdsScraperRequest(BaseModel):
    """
    Request model for scraping ads from a specific competitor page.
    """
    competitor_page_id: str
    countries: Optional[List[str]] = ["AE"]
    max_pages: Optional[int] = 5
    delay_between_requests: Optional[int] = 1
    active_status: Optional[str] = "active"
    ad_type: Optional[str] = "ALL"
    media_type: Optional[str] = "all"
    date_from: Optional[str] = None  # YYYY-MM-DD
    date_to: Optional[str] = None
    save_json: Optional[bool] = False

class AddNumbersRequest(BaseModel):
    x: int
    y: int

class TestTaskRequest(BaseModel):
    message: Optional[str] = "Hello from API!"

class ReprocessAdsRequest(BaseModel):
    """
    Request model for reprocessing ads
    
    ad_ids: Optional list of ad IDs to reprocess. If None, all ads with raw data will be reprocessed.
    """
    ad_ids: Optional[List[str]] = None

class DeleteAllAdsResponse(BaseModel):
    message: str
    deleted_count: int

# Dependency factory to avoid circular imports
def get_ad_service_dependency(db: Session = Depends(get_db)) -> "AdService":
    """Factory function to create AdService instance for dependency injection."""
    from app.services.ad_service import AdService
    return AdService(db)

# ========================================
# Main Dashboard API Endpoints
# ========================================

@router.get("/ads", response_model=PaginatedAdResponseDTO)
async def get_ads(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    competitor_id: Optional[int] = Query(None, description="Filter by competitor ID"),
    competitor_name: Optional[str] = Query(None, description="Filter by competitor name"),
    media_type: Optional[str] = Query(None, description="Filter by media type"),
    has_analysis: Optional[bool] = Query(None, description="Filter by analysis availability"),
    min_hook_score: Optional[float] = Query(None, ge=0, le=10, description="Minimum hook score"),
    max_hook_score: Optional[float] = Query(None, ge=0, le=10, description="Maximum hook score"),
    min_overall_score: Optional[float] = Query(None, ge=0, le=10, description="Minimum overall score"),
    max_overall_score: Optional[float] = Query(None, ge=0, le=10, description="Maximum overall score"),
    min_duration_days: Optional[int] = Query(None, ge=1, description="Minimum duration in days"),
    max_duration_days: Optional[int] = Query(None, ge=1, description="Maximum duration in days"),
    date_from: Optional[datetime] = Query(None, description="Filter ads from this date"),
    date_to: Optional[datetime] = Query(None, description="Filter ads to this date"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    search: Optional[str] = Query(None, description="Search in ad copy and titles"),
    sort_by: Optional[str] = Query("created_at", description="Sort by field"),
    sort_order: Optional[str] = Query("desc", description="Sort order (asc/desc)"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
) -> PaginatedAdResponseDTO:
    """
    Get paginated and filtered ads with AI analysis data.
    
    This is the main endpoint for the dashboard to fetch ads with comprehensive
    filtering, pagination, and sorting capabilities.
    """
    try:
        # Create filter parameters
        filters = AdFilterParams(
            page=page,
            page_size=page_size,
            competitor_id=competitor_id,
            competitor_name=competitor_name,
            media_type=media_type,
            has_analysis=has_analysis,
            min_hook_score=min_hook_score,
            max_hook_score=max_hook_score,
            min_overall_score=min_overall_score,
            max_overall_score=max_overall_score,
            min_duration_days=min_duration_days,
            max_duration_days=max_duration_days,
            date_from=date_from,
            date_to=date_to,
            is_active=is_active,
            search=search,
            sort_by=sort_by,
            sort_order=sort_order
        )
        
        # Get ads using service
        result = ad_service.get_ads(filters)
        
        logger.info(f"Retrieved {len(result.data)} ads for page {page}")
        return result
        
    except Exception as e:
        logger.error(f"Error fetching ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ads: {str(e)}")

@router.get("/ads/{ad_id}", response_model=AdDetailResponseDTO)
async def get_ad(
    ad_id: int, 
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get detailed information for a specific ad including AI analysis.
    """
    try:
        ad = ad_service.get_ad_by_id(ad_id)
        
        if not ad:
            raise HTTPException(status_code=404, detail="Ad not found")
        
        return ad
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching ad {ad_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ad: {str(e)}")

@router.get("/ads/stats/overview", response_model=AdStatsResponseDTO)
async def get_ads_stats(
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get comprehensive statistics about ads and analysis for the dashboard.
    """
    try:
        stats = ad_service.get_ad_stats()
        return stats
        
    except Exception as e:
        logger.error(f"Error fetching ad stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ad stats: {str(e)}")

@router.get("/ads/search", response_model=List[AdResponseDTO])
async def search_ads(
    q: str = Query(..., description="Search query"),
    limit: int = Query(50, ge=1, le=100, description="Maximum number of results"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Search ads by text content in ad copy, titles, and page names.
    """
    try:
        results = ad_service.search_ads(q, limit)
        return results
        
    except Exception as e:
        logger.error(f"Error searching ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error searching ads: {str(e)}")

@router.get("/ads/top-performing", response_model=List[AdResponseDTO])
async def get_top_performing_ads(
    limit: int = Query(10, ge=1, le=50, description="Number of top ads to return"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get top performing ads based on AI analysis scores.
    """
    try:
        top_ads = ad_service.get_top_performing_ads(limit)
        return top_ads
        
    except Exception as e:
        logger.error(f"Error fetching top performing ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching top performing ads: {str(e)}")

@router.get("/ads/competitor/{competitor_id}", response_model=List[AdResponseDTO])
async def get_competitor_ads(
    competitor_id: int,
    limit: int = Query(50, ge=1, le=100, description="Maximum number of ads to return"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get ads for a specific competitor.
    """
    try:
        ads = ad_service.get_competitor_ads(competitor_id, limit)
        return ads
        
    except Exception as e:
        logger.error(f"Error fetching competitor ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor ads: {str(e)}")

@router.delete("/ads/bulk")
async def bulk_delete_ads(
    request: BulkDeleteRequest,
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Delete multiple ads by IDs.
    """
    try:
        if not request.ad_ids:
            raise HTTPException(status_code=400, detail="No ad IDs provided")
        
        deleted_count = ad_service.bulk_delete_ads(request.ad_ids)
        
        return {
            "message": f"Successfully deleted {deleted_count} ads",
            "deleted_count": deleted_count,
            "requested_count": len(request.ad_ids)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error bulk deleting ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error bulk deleting ads: {str(e)}")

@router.delete("/ads/{ad_id}")
async def delete_ad(
    ad_id: int,
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Delete an ad by ID.
    """
    try:
        result = ad_service.delete_ad(ad_id)
        logger.info(f"Deleted ad ID {ad_id}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting ad {ad_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting ad: {str(e)}")

@router.delete("/ads/all", response_model=DeleteAllAdsResponse)
async def delete_all_ads(
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Delete all ads from the database.
    
    DANGER: This endpoint is for development purposes only!
    It will delete all ads and their analyses from the database.
    """
    try:
        count = ad_service.delete_all_ads()
        return DeleteAllAdsResponse(
            message=f"Successfully deleted all {count} ads",
            deleted_count=count
        )
    except Exception as e:
        logger.error(f"Error deleting all ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting all ads: {str(e)}")

# ========================================
# AI Analysis Endpoints
# ========================================

@router.post("/ads/{ad_id}/analyze")
async def trigger_ad_analysis(
    ad_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Manually trigger AI analysis for a specific ad.
    """
    try:
        # Check if ad exists
        ad = db.query(Ad).filter(Ad.id == ad_id).first()
        if not ad:
            raise HTTPException(status_code=404, detail="Ad not found")
        
        # Trigger analysis task
        task = ai_analysis_task.delay(ad_id)
        
        return {
            "message": f"AI analysis triggered for ad {ad_id}",
            "task_id": task.id,
            "ad_id": ad_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error triggering analysis for ad {ad_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error triggering analysis: {str(e)}")

@router.post("/ads/analyze/batch")
async def trigger_batch_analysis(
    ad_ids: List[int],
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Trigger AI analysis for multiple ads.
    """
    try:
        # Validate ad IDs exist
        existing_ads = db.query(Ad.id).filter(Ad.id.in_(ad_ids)).all()
        existing_ad_ids = [ad.id for ad in existing_ads]
        
        if not existing_ad_ids:
            raise HTTPException(status_code=404, detail="No valid ads found")
        
        # Trigger batch analysis
        task = batch_ai_analysis_task.delay(existing_ad_ids)
        
        return {
            "message": f"Batch AI analysis triggered for {len(existing_ad_ids)} ads",
            "task_id": task.id,
            "ad_ids": existing_ad_ids,
            "invalid_ids": list(set(ad_ids) - set(existing_ad_ids))
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error triggering batch analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error triggering batch analysis: {str(e)}")

@router.post("/ads/reprocess", response_model=Dict)
def reprocess_ads_data(
    request: ReprocessAdsRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Reprocess existing ads data with enhanced extraction
    
    This endpoint starts a background task to reprocess ads data that was previously scraped,
    applying the enhanced extraction to structure the data better.
    """
    logger.info(f"Received request to reprocess ads with enhanced extraction")
    
    try:
        # Start background task
        background_tasks.add_task(
            _run_enhanced_reprocessing,
            db=db,
            ad_ids=request.ad_ids
        )
        
        return {
            "success": True,
            "message": "Ads reprocessing task started",
            "ad_count": len(request.ad_ids) if request.ad_ids else "all"
        }
    except Exception as e:
        logger.error(f"Error starting ads reprocessing task: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting reprocessing task: {str(e)}")

async def _run_enhanced_reprocessing(db: Session, ad_ids: Optional[List[str]] = None):
    """
    Background task to reprocess ads with enhanced extraction
    
    Args:
        db: Database session
        ad_ids: Optional list of ad IDs to reprocess. If None, all ads with raw data will be reprocessed.
    """
    logger.info(f"Starting background task for ads reprocessing")
    
    try:
        # Create enhanced extraction service
        enhanced_extraction_service = EnhancedAdExtractionService(db)
        
        # Query ads to reprocess
        query = db.query(Ad)
        
        # Filter by ad_ids if provided
        if ad_ids:
            logger.info(f"Reprocessing specific ads: {ad_ids}")
            query = query.filter(Ad.ad_id.in_(ad_ids))
        else:
            logger.info("Reprocessing all ads with raw data")
            query = query.filter(Ad.raw_data.isnot(None))
        
        # Execute query
        ads = query.all()
        logger.info(f"Found {len(ads)} ads to reprocess")
        
        if not ads:
            logger.warning("No ads found to reprocess")
            return
        
        # Group ads by campaign_id for batch processing
        campaigns = {}
        for ad in ads:
            campaign_id = ad.campaign_id or "unknown"
            if campaign_id not in campaigns:
                campaigns[campaign_id] = []
            campaigns[campaign_id].append(ad)
        
        logger.info(f"Grouped ads into {len(campaigns)} campaigns")
        
        # Process each campaign
        processed_campaigns = 0
        total_processed = 0
        total_updated = 0
        total_errors = 0
        
        for campaign_id, campaign_ads in campaigns.items():
            try:
                # Prepare raw data for enhanced extraction
                raw_responses = []
                for ad in campaign_ads:
                    if ad.raw_data:
                        # Create a mock response structure that the enhanced extractor can process
                        mock_response = {
                            "data": {
                                "ad_library_main": {
                                    "search_results_connection": {
                                        "edges": [
                                            {
                                                "node": {
                                                    "collated_results": [ad.raw_data]
                                                }
                                            }
                                        ]
                                    }
                                }
                            }
                        }
                        raw_responses.append(mock_response)
                    else:
                        logger.warning(f"Ad {ad.ad_id} has no raw data, skipping")
                    
                if not raw_responses:
                    logger.warning(f"No raw data found for campaign {campaign_id}, skipping")
                    continue
                    
                # Process with enhanced extraction
                enhanced_data, stats = enhanced_extraction_service.process_raw_responses(raw_responses)
                
                # Update stats
                processed_campaigns += 1
                total_processed += len(campaign_ads)
                total_updated += stats.get("total_updated", 0)
                total_errors += stats.get("total_errors", 0)
                
                logger.info(f"Processed campaign {campaign_id}: {stats}")
                    
            except Exception as e:
                logger.error(f"Error processing campaign {campaign_id}: {str(e)}")
                total_errors += 1
        
        # Log final stats
        logger.info(f"Reprocessing complete!")
        logger.info(f"Campaigns processed: {processed_campaigns}")
        logger.info(f"Total ads processed: {total_processed}")
        logger.info(f"Total ads updated: {total_updated}")
        logger.info(f"Total errors: {total_errors}")
        
    except Exception as e:
        logger.error(f"Error in reprocessing: {str(e)}")
    finally:
        # Close database session
        db.close()

# ========================================
# Scraping Endpoints (Backward Compatibility)
# ========================================

@router.post("/ads/scrape", response_model=Dict)
def scrape_facebook_ads(
    request: FacebookAdsScraperRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Endpoint to scrape Facebook Ads Library data
    
    This endpoint will start a background task to scrape Facebook Ads Library data.
    """
    logger.info(f"Received request to scrape Facebook Ads for page: {request.view_all_page_id}")
    
    # Create task ID
    task_id = str(uuid.uuid4())
    
    # Create config from request
    config = {
        "view_all_page_id": request.view_all_page_id,
        "countries": request.countries,
        "max_pages": request.max_pages,
        "delay_between_requests": request.delay_between_requests,
        "active_status": request.active_status,
        "ad_type": request.ad_type,
        "media_type": request.media_type,
        "search_type": request.search_type,
        "query_string": request.query_string,
        "save_json": request.save_json
    }
    
    # Start background task
    background_tasks.add_task(
        _run_facebook_ads_scraper_task,
        task_id=task_id,
        config=config,
        db=db
    )
    
    return {
        "message": "Facebook Ads scraping task started",
        "task_id": task_id,
        "status": "pending"
    }

@router.post("/ads/scrape/competitor", response_model=Dict)
def scrape_competitor_ads(
    request: CompetitorAdsScraperRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Endpoint to scrape ads from a specific competitor page
    
    This endpoint will start a background task to scrape ads from a specific competitor page.
    """
    logger.info(f"Received request to scrape competitor ads for {request.competitor_page_id}")
    
    # Create task ID
    task_id = str(uuid.uuid4())
    
    # Start background task
    background_tasks.add_task(
        _run_competitor_ads_scraper_task,
        task_id=task_id,
            competitor_page_id=request.competitor_page_id,
            countries=request.countries,
            max_pages=request.max_pages,
        delay_between_requests=request.delay_between_requests,
        active_status=request.active_status,
        ad_type=request.ad_type,
        media_type=request.media_type,
        date_from=request.date_from,
        date_to=request.date_to,
        save_json=request.save_json,
        db=db
    )
    
    return {
        "message": f"Competitor ads scraping task started for {request.competitor_page_id}",
        "task_id": task_id,
        "status": "pending"
    }

@router.get("/ads/scrape/status/{task_id}", response_model=Dict)
def get_scraping_task_status(task_id: str):
    """
    Get the status of a scraping task
    
    This endpoint returns the current status of a Facebook Ads scraping task,
    including progress information and results if the task is complete.
    """
    try:
        # Get task result from Celery
        from celery.result import AsyncResult
        from app.celery_worker import celery_app
        
        task = AsyncResult(task_id, app=celery_app)
        
        # Check if task exists
        if not task:
            logger.error(f"Task {task_id} not found")
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
        
        # Get task state
        state = task.state
        
        # Prepare response
        response = {
            "task_id": task_id,
            "state": state
        }
        
        # Add info based on state
        if state == "PENDING":
            response["status"] = "Task is pending execution"
            
        elif state == "STARTED":
            response["status"] = "Task has started"
            
        elif state == "PROGRESS":
            # Task is in progress, get meta info
            meta = task.info
            response["status"] = meta.get("status", "Task is in progress")
            response["progress"] = {
                "current": meta.get("current", 0),
                "total": meta.get("total", 100)
            }
            
        elif state == "SUCCESS":
            # Task completed successfully
            result = task.result
            
            # Add task result to response
            response["status"] = "Task completed successfully"
            response["result"] = {
                "success": result.get("success", True),
                "total_ads_scraped": result.get("total_ads_scraped", 0),
                "database_stats": result.get("database_stats", {}),
                "completion_time": result.get("completion_time")
            }
            
            # Add scraper config if available
            if "scraper_config" in result:
                response["result"]["scraper_config"] = result["scraper_config"]
            
            # Add competitor info if available
            if "competitor_page_id" in result:
                response["result"]["competitor_page_id"] = result["competitor_page_id"]
            
        elif state == "FAILURE":
            # Task failed
            response["status"] = "Task failed"
            
            # Add error info if available
            if task.info:
                if isinstance(task.info, dict):
                    response["error"] = task.info.get("status", str(task.info))
                else:
                    response["error"] = str(task.info)
        
        return response
        
    except Exception as e:
        logger.error(f"Error getting task status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting task status: {str(e)}")

# ========================================
# Competitors endpoints have been moved to dedicated competitors router
# ========================================

# ========================================
# Testing Endpoints (Backward Compatibility)
# ========================================

@router.post("/test/add", response_model=TaskResponse)
async def test_add_numbers(request: AddNumbersRequest):
    """Test endpoint for adding numbers via Celery task"""
    try:
        task = add_together.delay(request.x, request.y)
        return TaskResponse(
            task_id=task.id,
            status="started",
            message=f"Addition task started: {request.x} + {request.y}"
        )
    except Exception as e:
        logger.error(f"Error starting addition task: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting addition task: {str(e)}")

@router.post("/test/task", response_model=TaskResponse)
async def test_celery_task(request: TestTaskRequest):
    """Test endpoint for basic Celery task"""
    try:
        task = test_task.delay(request.message)
        return TaskResponse(
            task_id=task.id,
            status="started",
            message="Test task started successfully"
        )
    except Exception as e:
        logger.error(f"Error starting test task: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting test task: {str(e)}")

@router.get("/test/task/{task_id}", response_model=None)
async def get_task_status(task_id: str):
    """Get the status of any Celery task"""
    try:
        from celery.result import AsyncResult
        from app.celery_worker import celery_app
        
        task_result = AsyncResult(task_id, app=celery_app)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'status': 'Task is pending...'
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'result': task_result.result,
                'status': 'Task completed successfully'
            }
        elif task_result.state == 'FAILURE':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'error': str(task_result.info),
                'status': 'Task failed'
            }
        else:
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'info': task_result.info,
                'status': f'Task is {task_result.state.lower()}'
            }
        
        return response
        
    except Exception as e:
        logger.error(f"Error fetching task status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching task status: {str(e)}") 

def _run_facebook_ads_scraper_task(
    task_id: str,
    config: Dict,
    db: Session
):
    """Run Facebook Ads scraper task in the background"""
    try:
        # Create scraper service
        scraper = FacebookAdsScraperService(db)
        
        # Create configuration
        scraper_config = FacebookAdsScraperConfig(**config)
        
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper.scrape_ads(scraper_config)
        
        # Save to database if requested
        if scraper_config.save_json and all_json_responses:
            # Save raw JSON responses to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_ads_{scraper_config.view_all_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")
        
        # Update task status
        task_status = TaskStatus(
            task_id=task_id,
            status="completed",
            result={
                "stats": stats,
                "enhanced_data_summary": {
                    "advertiser_info": enhanced_data.get("advertiser_info", {}),
                    "campaigns_count": len(enhanced_data.get("campaigns", [])),
                    "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
                }
            }
        )
        db.add(task_status)
        db.commit()
        
    except Exception as e:
        logger.error(f"Error in Facebook Ads scraping task: {str(e)}")
        # Update task status with error
        task_status = TaskStatus(
            task_id=task_id,
            status="failed",
            result={"error": str(e)}
        )
        db.add(task_status)
        db.commit()

def _run_competitor_ads_scraper_task(
    task_id: str,
    competitor_page_id: str,
    countries: List[str],
    max_pages: int,
    delay_between_requests: int,
    active_status: str,
    ad_type: str,
    media_type: str,
    date_from: Optional[str],
    date_to: Optional[str],
    save_json: bool,
    db: Session
):
    """Run competitor ads scraper task in the background"""
    try:
        # Create scraper service
        scraper = FacebookAdsScraperService(db)
        
        # Get competitor
        competitor = db.query(Competitor).filter_by(page_id=competitor_page_id).first()
        if not competitor:
            # Don't create new competitor, exit early
            logger.warning(f"Competitor with page_id {competitor_page_id} not found, skipping scraping")
            # Update task status with warning
            task_status = TaskStatus(
                task_id=task_id,
                status="completed",
                result={"warning": f"Competitor with page_id {competitor_page_id} not found, skipping scraping"}
            )
            db.add(task_status)
            db.commit()
            return
        
        # Create configuration
        scraper_config = FacebookAdsScraperConfig(
            view_all_page_id=competitor_page_id,
            countries=countries,
            max_pages=max_pages,
            delay_between_requests=delay_between_requests,
            active_status=active_status,
            ad_type=ad_type,
            media_type=media_type,
            date_from=date_from,
            date_to=date_to,
            save_json=save_json
        )
        
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper.scrape_ads(scraper_config)
        
        # Save to database if requested
        if scraper_config.save_json and all_json_responses:
            # Save raw JSON responses to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"competitor_ads_{competitor_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")
        
        # Update task status
        task_status = TaskStatus(
            task_id=task_id,
            status="completed",
            result={
                "competitor_id": competitor.id,
                "competitor_page_id": competitor_page_id,
                "stats": stats,
                "enhanced_data_summary": {
                    "advertiser_info": enhanced_data.get("advertiser_info", {}),
                    "campaigns_count": len(enhanced_data.get("campaigns", [])),
                    "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
                }
            }
        )
        db.add(task_status)
        db.commit()
        
    except Exception as e:
        logger.error(f"Error in competitor ads scraping task: {str(e)}")
        # Update task status with error
        task_status = TaskStatus(
            task_id=task_id,
            status="failed",
            result={"error": str(e)}
        )
        db.add(task_status)
        db.commit() 


================================================
File: app/routers/competitors.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List, Optional, TYPE_CHECKING
import logging
from pydantic import BaseModel

from app.database import get_db
from app.models.dto.competitor_dto import (
    CompetitorCreateDTO,
    CompetitorUpdateDTO,
    CompetitorResponseDTO,
    CompetitorDetailResponseDTO,
    PaginatedCompetitorResponseDTO,
    CompetitorFilterParams,
    CompetitorStatsResponseDTO
)

# Import tasks for scraping
from app.tasks.facebook_ads_scraper_task import scrape_competitor_ads_task

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    from app.services.competitor_service import CompetitorService

router = APIRouter()
logger = logging.getLogger(__name__)

# Dependency factory to avoid circular imports
def get_competitor_service_dependency(db: Session = Depends(get_db)) -> "CompetitorService":
    """Factory function to create CompetitorService instance for dependency injection."""
    from app.services.competitor_service import CompetitorService
    return CompetitorService(db)

# ========================================
# Main Competitors CRUD Endpoints
# ========================================

@router.get("/", response_model=PaginatedCompetitorResponseDTO)
async def get_competitors(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    search: Optional[str] = Query(None, description="Search in competitor names and page IDs"),
    sort_by: Optional[str] = Query("created_at", description="Sort by field"),
    sort_order: Optional[str] = Query("desc", description="Sort order (asc/desc)"),
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> PaginatedCompetitorResponseDTO:
    """
    Get paginated and filtered list of competitors.
    
    This endpoint provides comprehensive competitor listing with:
    - Pagination support
    - Search functionality
    - Active/inactive filtering
    - Sorting options
    - Ads count for each competitor
    """
    try:
        # Create filter parameters
        filters = CompetitorFilterParams(
            page=page,
            page_size=page_size,
            is_active=is_active,
            search=search,
            sort_by=sort_by,
            sort_order=sort_order
        )
        
        # Get competitors using service
        result = competitor_service.get_competitors(filters)
        
        logger.info(f"Retrieved {len(result.data)} competitors for page {page}")
        return result
        
    except Exception as e:
        logger.error(f"Error fetching competitors: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitors: {str(e)}")

@router.get("/{competitor_id}", response_model=CompetitorDetailResponseDTO)
async def get_competitor(
    competitor_id: int,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorDetailResponseDTO:
    """
    Get detailed information for a specific competitor.
    
    Returns comprehensive competitor data including:
    - Basic competitor information
    - Total ads count
    - Active ads count
    - Analyzed ads count
    """
    try:
        competitor = competitor_service.get_competitor_by_id(competitor_id)
        return competitor
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor: {str(e)}")

@router.post("/", response_model=CompetitorResponseDTO)
async def create_competitor(
    competitor_data: CompetitorCreateDTO,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorResponseDTO:
    """
    Create a new competitor.
    
    Required fields:
    - name: Competitor name
    - page_id: Facebook page ID (must be unique)
    - is_active: Whether the competitor is active (default: True)
    """
    try:
        competitor = competitor_service.create_competitor(competitor_data)
        logger.info(f"Created new competitor: {competitor.name} (ID: {competitor.id})")
        return competitor
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating competitor: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error creating competitor: {str(e)}")

@router.put("/{competitor_id}", response_model=CompetitorDetailResponseDTO)
async def update_competitor(
    competitor_id: int,
    competitor_data: CompetitorUpdateDTO,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorDetailResponseDTO:
    """
    Update an existing competitor.
    
    All fields are optional:
    - name: Competitor name
    - page_id: Facebook page ID (must be unique if changed)
    - is_active: Whether the competitor is active
    """
    try:
        competitor = competitor_service.update_competitor(competitor_id, competitor_data)
        logger.info(f"Updated competitor: {competitor.name} (ID: {competitor.id})")
        return competitor
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating competitor: {str(e)}")

class BulkDeleteRequest(BaseModel):
    competitor_ids: List[int]

@router.delete("/")
async def bulk_delete_competitors(
    request: BulkDeleteRequest,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> dict:
    """
    Bulk delete competitors.
    - Soft deletes competitors with ads.
    - Hard deletes competitors without ads.
    """
    try:
        result = competitor_service.bulk_delete_competitors(request.competitor_ids)
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during bulk competitor deletion: {str(e)}")
        raise HTTPException(status_code=500, detail=f"An error occurred during bulk deletion: {str(e)}")

@router.delete("/{competitor_id}")
async def delete_competitor(
    competitor_id: int,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> dict:
    """
    Delete a competitor.
    
    Behavior:
    - If competitor has ads: Soft delete (sets is_active to False)
    - If competitor has no ads: Hard delete (permanent removal)
    """
    try:
        result = competitor_service.delete_competitor(competitor_id)
        logger.info(f"Deleted competitor ID {competitor_id}: {result['message']}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting competitor: {str(e)}")

# ========================================
# Competitors Statistics & Search
# ========================================

@router.get("/stats/overview", response_model=CompetitorStatsResponseDTO)
async def get_competitor_stats(
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorStatsResponseDTO:
    """
    Get comprehensive statistics about competitors.
    
    Returns:
    - Total competitors count
    - Active/inactive competitors count
    - Competitors with ads count
    - Total ads across all competitors
    - Average ads per competitor
    """
    try:
        stats = competitor_service.get_competitor_stats()
        return stats
        
    except Exception as e:
        logger.error(f"Error fetching competitor stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor stats: {str(e)}")

@router.get("/search/query", response_model=List[CompetitorResponseDTO])
async def search_competitors(
    q: str = Query(..., description="Search query"),
    limit: int = Query(50, ge=1, le=100, description="Maximum number of results"),
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> List[CompetitorResponseDTO]:
    """
    Search competitors by name or page ID.
    
    Searches in:
    - Competitor names (case-insensitive)
    - Facebook page IDs
    """
    try:
        results = competitor_service.search_competitors(q, limit)
        return results
        
    except Exception as e:
        logger.error(f"Error searching competitors: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error searching competitors: {str(e)}")

# ========================================
# Competitor Ads Scraping
# ========================================

class CompetitorScrapeRequest(BaseModel):
    """Request model for competitor ads scraping"""
    countries: Optional[List[str]] = ["AE"]
    max_pages: Optional[int] = 5
    delay_between_requests: Optional[int] = 1
    active_status: Optional[str] = "active"

class TaskResponse(BaseModel):
    """Response model for task operations"""
    task_id: str
    status: str
    message: str

@router.post("/{competitor_id}/scrape", response_model=TaskResponse)
async def scrape_competitor_ads(
    competitor_id: int,
    scrape_request: CompetitorScrapeRequest,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> TaskResponse:
    """
    Trigger ads scraping for a specific competitor.
    
    This will:
    1. Validate the competitor exists
    2. Start a background task to scrape ads from their Facebook page
    3. Return task ID for monitoring progress
    """
    try:
        # Validate competitor exists and is active
        competitor = competitor_service.get_competitor_by_id(competitor_id)
        
        if not competitor.is_active:
            raise HTTPException(
                status_code=400,
                detail=f"Competitor '{competitor.name}' is not active. Please activate it first."
            )
        
        # Start scraping task
        task = scrape_competitor_ads_task.delay(
            competitor_page_id=competitor.page_id,
            countries=scrape_request.countries,
            max_pages=scrape_request.max_pages,
            delay_between_requests=scrape_request.delay_between_requests,
            active_status=scrape_request.active_status
        )
        
        logger.info(f"Started scraping task for competitor {competitor.name} (ID: {competitor_id})")
        
        return TaskResponse(
            task_id=task.id,
            status="started",
            message=f"Ads scraping started for competitor '{competitor.name}'"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting scraping for competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting scraping: {str(e)}")

@router.get("/scrape/status/{task_id}")
async def get_scraping_status(task_id: str) -> dict:
    """
    Get the status of a competitor scraping task.
    
    Returns task status and progress information.
    """
    try:
        from celery.result import AsyncResult
        from app.celery_worker import celery_app
        
        task_result = AsyncResult(task_id, app=celery_app)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'status': 'Task is pending...'
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'result': task_result.result,
                'status': 'Task completed successfully'
            }
        elif task_result.state == 'FAILURE':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'error': str(task_result.info),
                'status': 'Task failed'
            }
        else:
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'info': task_result.info,
                'status': f'Task is {task_result.state.lower()}'
            }
        
        return response
        
    except Exception as e:
        logger.error(f"Error fetching scraping status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching scraping status: {str(e)}")

# ========================================
# Competitor Ads Management
# ========================================

@router.get("/{competitor_id}/ads")
async def get_competitor_ads(
    competitor_id: int,
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    has_analysis: Optional[bool] = Query(None, description="Filter by analysis availability"),
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
):
    """
    Get ads for a specific competitor with filtering and pagination.
    
    This endpoint redirects to the main ads endpoint with competitor filter.
    """
    try:
        # Validate competitor exists
        competitor = competitor_service.get_competitor_by_id(competitor_id)
        
        # Import here to avoid circular imports
        from app.services.ad_service import AdService
        from app.models.dto.ad_dto import AdFilterParams
        
        # Create ad service
        db = next(get_db())
        ad_service = AdService(db)
        
        # Create filter parameters
        filters = AdFilterParams(
            page=page,
            page_size=page_size,
            competitor_id=competitor_id,
            is_active=is_active,
            has_analysis=has_analysis,
            sort_by="created_at",
            sort_order="desc"
        )
        
        # Get ads using ad service
        result = ad_service.get_ads(filters)
        
        return {
            "competitor": {
                "id": competitor.id,
                "name": competitor.name,
                "page_id": competitor.page_id
            },
            "ads": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching ads for competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor ads: {str(e)}")

# Import required models 


================================================
File: app/routers/health.py
================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.database import get_db
from app.core.config import settings
import redis
import os
from datetime import datetime

router = APIRouter()

@router.get("/health")
async def health_check():
    """Basic health check endpoint - returns the exact format requested in the prompt"""
    return {"status": "ok"}

@router.get("/health/detailed")
async def detailed_health_check(db: Session = Depends(get_db)):
    """Detailed health check including database and Redis connectivity"""
    health_status = {
        "status": "ok",
        "timestamp": datetime.utcnow().isoformat(),
        "service": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "checks": {}
    }
    
    # Check database connectivity
    try:
        db.execute(text("SELECT 1"))
        health_status["checks"]["database"] = {
            "status": "ok", 
            "message": "Database connection successful"
        }
    except Exception as e:
        health_status["checks"]["database"] = {
            "status": "error", 
            "message": f"Database connection failed: {str(e)}"
        }
        health_status["status"] = "error"
    
    # Check Redis connectivity
    try:
        redis_client = redis.from_url(settings.REDIS_URL)
        redis_client.ping()
        health_status["checks"]["redis"] = {
            "status": "ok", 
            "message": "Redis connection successful"
        }
    except Exception as e:
        health_status["checks"]["redis"] = {
            "status": "error", 
            "message": f"Redis connection failed: {str(e)}"
        }
        health_status["status"] = "error"
    
    # Check Celery worker connectivity
    try:
        from app.celery_worker import celery_app
        inspect = celery_app.control.inspect()
        stats = inspect.stats()
        if stats:
            health_status["checks"]["celery"] = {
                "status": "ok", 
                "message": "Celery workers are active",
                "active_workers": len(stats)
            }
        else:
            health_status["checks"]["celery"] = {
                "status": "warning", 
                "message": "No active Celery workers found"
            }
    except Exception as e:
        health_status["checks"]["celery"] = {
            "status": "error", 
            "message": f"Celery check failed: {str(e)}"
        }
    
    if health_status["status"] == "error":
        raise HTTPException(status_code=503, detail=health_status)
    
    return health_status 


================================================
File: app/services/__init__.py
================================================
from .ingestion_service import DataIngestionService
from .ai_service import AIService, get_ai_service
from .ad_service import AdService, get_ad_service

__all__ = ["DataIngestionService", "AIService", "get_ai_service", "AdService", "get_ad_service"] 


================================================
File: app/services/ad_service.py
================================================
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import func, and_, or_, desc, asc, cast, String
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime, timedelta
import logging

from app.models import Ad, Competitor, AdAnalysis
from app.models.dto.ad_dto import (
    AdFilterParams, 
    AdResponseDTO, 
    AdDetailResponseDTO, 
    PaginatedAdResponseDTO, 
    PaginationMetadata,
    AdStatsResponseDTO,
    AdAnalysisResponseDTO,
    AdStats,
    Campaign,
    AdvertiserInfo,
    PaginatedAdResponse
)
from app.models.dto.competitor_dto import CompetitorResponseDTO

logger = logging.getLogger(__name__)


class AdService:
    """
    Service class for handling all ad-related database operations.
    Provides methods for querying, filtering, and paginating ad data.
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    def get_ads(self, filters: AdFilterParams) -> PaginatedAdResponseDTO:
        """
        Get paginated and filtered ads with AI analysis data.
        
        Args:
            filters: AdFilterParams containing pagination and filter parameters
            
        Returns:
            PaginatedAdResponseDTO with ads and pagination metadata
        """
        try:
            # Build base query with relationships
            query = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            )
            
            # Apply filters
            query = self._apply_filters(query, filters)
            
            # Apply sorting
            query = self._apply_sorting(query, filters.sort_by, filters.sort_order)
            
            # Get total count before pagination
            total_items = query.count()
            
            # Apply pagination
            offset = (filters.page - 1) * filters.page_size
            ads = query.offset(offset).limit(filters.page_size).all()
            
            # Convert to DTOs
            ad_dtos = [self._convert_to_dto(ad) for ad in ads]
            
            # Calculate pagination metadata
            total_pages = (total_items + filters.page_size - 1) // filters.page_size
            pagination = PaginationMetadata(
                page=filters.page,
                page_size=filters.page_size,
                total_items=total_items,
                total_pages=total_pages,
                has_next=filters.page < total_pages,
                has_previous=filters.page > 1
            )
            
            return PaginatedAdResponseDTO(
                data=ad_dtos,
                pagination=pagination
            )
            
        except Exception as e:
            logger.error(f"Error fetching ads: {str(e)}")
            raise
    
    def get_ad_by_id(self, ad_id: int) -> Optional[AdDetailResponseDTO]:
        """Get a single ad by ID."""
        ad = self.db.query(Ad).options(
            joinedload(Ad.competitor),
            joinedload(Ad.analysis)
        ).filter(Ad.id == ad_id).first()
        
        return self._convert_to_detail_dto(ad) if ad else None
    
    def get_ad_by_archive_id(self, ad_archive_id: str) -> Optional[Ad]:
        """Get ad by Facebook archive ID"""
        return self.db.query(Ad).filter(Ad.ad_archive_id == ad_archive_id).first()
    
    def get_ads_paginated(self, filters: AdFilterParams) -> PaginatedAdResponse:
        """Get paginated ads with filtering"""
        query = self.db.query(Ad)
        
        # Apply filters
        if filters.campaign_id:
            query = query.filter(Ad.campaign_id == filters.campaign_id)
        
        if filters.is_active is not None:
            query = query.filter(Ad.meta_is_active == filters.is_active)
            
        if filters.has_lead_form is not None:
            if filters.has_lead_form:
                # Has non-empty lead form fields
                query = query.filter(
                    or_(
                        Ad.lead_form_standalone_fields.cast(str) != '[]',
                        Ad.lead_form_questions.cast(str) != '{}'
                    )
                )
            else:
                # Empty lead form fields
                query = query.filter(
                    and_(
                        Ad.lead_form_standalone_fields.cast(str) == '[]',
                        Ad.lead_form_questions.cast(str) == '{}'
                    )
                )
        
        if filters.platform:
            query = query.filter(Ad.platforms.contains([filters.platform]))
        
        if filters.media_type:
            # Check in creatives array for media type
            query = query.filter(
                Ad.creatives.cast(str).like(f'%"type": "{filters.media_type}"%')
            )
        
        if filters.query:
            # Search in various text fields
            search_term = f"%{filters.query}%"
            query = query.filter(
                or_(
                    Ad.creatives.cast(str).ilike(search_term),
                    # Check for page name within the meta JSON
                    Ad.meta.op('->>')('page_name').ilike(search_term)
                )
            )
        
        # Count total results for pagination
        total = query.count()
        
        # Apply pagination
        query = query.order_by(desc(Ad.date_found))
        query = query.offset((filters.page - 1) * filters.limit).limit(filters.limit)
        
        # Get results
        items = query.all()
        
        # Calculate total pages
        total_pages = (total + filters.limit - 1) // filters.limit
        
        return PaginatedAdResponse(
            items=items,
            total=total,
            page=filters.page,
            limit=filters.limit,
            total_pages=total_pages
        )
    
    def get_campaigns(self) -> List[Campaign]:
        """Get all campaigns with their ads"""
        # Get unique campaign IDs
        campaign_ids = self.db.query(Ad.campaign_id).distinct().all()
        campaign_ids = [c[0] for c in campaign_ids if c[0]]
        
        campaigns = []
        for campaign_id in campaign_ids:
            # Get ads for this campaign
            ads = self.db.query(Ad).filter(Ad.campaign_id == campaign_id).all()
            if ads:
                # Extract platforms from the first ad
                platforms = ads[0].platforms or []
                
                campaigns.append(Campaign(
                    campaign_id=campaign_id,
                    platforms=platforms,
                    ads=ads
                ))
        
        return campaigns
    
    def get_ad_stats(self) -> AdStats:
        """Get ad statistics"""
        total_ads = self.db.query(func.count(Ad.id)).scalar() or 0
        active_ads = self.db.query(func.count(Ad.id)).filter(Ad.meta_is_active == True).scalar() or 0
        
        # Count ads with lead forms
        with_lead_form = self.db.query(func.count(Ad.id)).filter(
            or_(
                Ad.lead_form_standalone_fields.cast(str) != '[]',
                Ad.lead_form_questions.cast(str) != '{}'
            )
        ).scalar() or 0
        
        # Get platform stats
        platforms_stats = {}
        for ad in self.db.query(Ad.platforms).all():
            if ad.platforms:
                for platform in ad.platforms:
                    platforms_stats[platform] = platforms_stats.get(platform, 0) + 1
        
        # Get media type stats
        # This is more complex with JSON array, using a simplified approach
        media_types = {
            "Image": self.db.query(func.count(Ad.id)).filter(Ad.creatives.cast(str).like('%"type": "Image"%')).scalar() or 0,
            "Video": self.db.query(func.count(Ad.id)).filter(Ad.creatives.cast(str).like('%"type": "Video"%')).scalar() or 0,
            "Carousel": self.db.query(func.count(Ad.id)).filter(Ad.creatives.cast(str).like('%"type": "Carousel"%')).scalar() or 0,
            "Other": self.db.query(func.count(Ad.id)).filter(
                and_(
                    ~Ad.creatives.cast(str).like('%"type": "Image"%'),
                    ~Ad.creatives.cast(str).like('%"type": "Video"%'),
                    ~Ad.creatives.cast(str).like('%"type": "Carousel"%')
                )
            ).scalar() or 0
        }
        
        return AdStats(
            total_ads=total_ads,
            active_ads=active_ads,
            with_lead_form=with_lead_form,
            platforms=platforms_stats,
            media_types=media_types
        )
    
    def search_ads(self, query: str, limit: int = 50) -> List[AdResponseDTO]:
        """Search ads by text content."""
        try:
            search_filter = or_(
                Ad.creatives.cast(String).ilike(f'%{query}%'),
                Ad.meta.cast(String).ilike(f'%{query}%')
            )
            
            ads = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).join(Ad.competitor).filter(search_filter).limit(limit).all()
            
            return [self._convert_to_dto(ad) for ad in ads]
            
        except Exception as e:
            logger.error(f"Error searching ads: {str(e)}")
            raise
    
    def get_top_performing_ads(self, limit: int = 10) -> List[AdResponseDTO]:
        """
        Get top performing ads based on AI analysis scores.
        
        Args:
            limit: Number of top ads to return
            
        Returns:
            List of top performing ads
        """
        try:
            ads = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).join(AdAnalysis).order_by(
                desc(AdAnalysis.overall_score)
            ).limit(limit).all()
            
            return [self._convert_to_dto(ad) for ad in ads]
            
        except Exception as e:
            logger.error(f"Error fetching top performing ads: {str(e)}")
            raise
    
    def get_competitor_ads(self, competitor_id: int, limit: int = 50) -> List[AdResponseDTO]:
        """
        Get ads for a specific competitor.
        
        Args:
            competitor_id: ID of the competitor
            limit: Maximum number of ads to return
            
        Returns:
            List of competitor ads
        """
        try:
            ads = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).filter(Ad.competitor_id == competitor_id).limit(limit).all()
            
            return [self._convert_to_dto(ad) for ad in ads]
            
        except Exception as e:
            logger.error(f"Error fetching competitor ads: {str(e)}")
            raise
    
    def delete_ad(self, ad_id: int) -> bool:
        """
        Delete a specific ad by ID.
        
        Args:
            ad_id: ID of the ad to delete
            
        Returns:
            True if the ad was deleted, False if not found
        """
        try:
            ad = self.db.query(Ad).filter(Ad.id == ad_id).first()
            
            if not ad:
                return False
            
            # Delete associated analysis first (if any)
            if ad.analysis:
                self.db.delete(ad.analysis)
            
            # Delete the ad
            self.db.delete(ad)
            self.db.commit()
            
            logger.info(f"Successfully deleted ad {ad_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting ad {ad_id}: {str(e)}")
            self.db.rollback()
            raise
    
    def bulk_delete_ads(self, ad_ids: List[int]) -> int:
        """
        Delete multiple ads by IDs.
        
        Args:
            ad_ids: List of ad IDs to delete
            
        Returns:
            Number of ads successfully deleted
        """
        try:
            # Delete associated analyses first
            analyses_deleted = self.db.query(AdAnalysis).filter(
                AdAnalysis.ad_id.in_(ad_ids)
            ).delete(synchronize_session=False)
            
            # Delete the ads
            ads_deleted = self.db.query(Ad).filter(
                Ad.id.in_(ad_ids)
            ).delete(synchronize_session=False)
            
            self.db.commit()
            
            logger.info(f"Successfully deleted {ads_deleted} ads and {analyses_deleted} analyses")
            return ads_deleted
            
        except Exception as e:
            logger.error(f"Error bulk deleting ads: {str(e)}")
            self.db.rollback()
            raise
    
    def _apply_filters(self, query, filters: AdFilterParams):
        """Apply filters to the ad query."""
        
        if filters.competitor_id:
            query = query.filter(Ad.competitor_id == filters.competitor_id)
        
        if filters.competitor_name:
            query = query.join(Ad.competitor).filter(Competitor.name.ilike(f"%{filters.competitor_name}%"))
        
        if filters.media_type and filters.media_type != 'all':
            query = query.filter(Ad.creatives.cast(String).ilike(f'%"type": "{filters.media_type}"%'))
        
        if filters.is_active is not None:
            query = query.filter(cast(Ad.meta['is_active'], String) == str(filters.is_active).lower())

        if filters.has_analysis is not None:
            if filters.has_analysis:
                query = query.join(Ad.analysis).filter(AdAnalysis.id.isnot(None))
        
        if filters.min_overall_score is not None:
            query = query.join(Ad.analysis).filter(AdAnalysis.overall_score >= filters.min_overall_score)
            
        # Duration filters
        if filters.min_duration_days is not None:
            query = query.filter(Ad.duration_days >= filters.min_duration_days)
        if filters.max_duration_days is not None:
            query = query.filter(Ad.duration_days <= filters.max_duration_days)
            
        if filters.date_from:
            query = query.filter(Ad.date_found >= filters.date_from)
        if filters.date_to:
            query = query.filter(Ad.date_found <= filters.date_to)
        
        if filters.search:
            search_query = f"%{filters.search}%"
            query = query.join(Ad.competitor).filter(
                or_(
                    Ad.creatives.cast(String).ilike(search_query),
                    Competitor.name.ilike(search_query)
                )
            )
        
        return query
    
    def _apply_sorting(self, query, sort_by: str, sort_order: str):
        """Apply sorting to the query."""
        sort_mapping = {
            'created_at': Ad.created_at,
            'date_found': Ad.date_found,
            'duration_days': Ad.duration_days,
            'overall_score': AdAnalysis.overall_score,
            'hook_score': AdAnalysis.hook_score
        }
        
        column = sort_mapping.get(sort_by, Ad.created_at)
        
        if sort_by in ['overall_score', 'hook_score']:
            query = query.outerjoin(Ad.analysis)
        
        order_func = asc if sort_order.lower() == 'asc' else desc
        query = query.order_by(order_func(column))
        
        return query
    
    def _convert_to_dto(self, ad: Ad) -> AdResponseDTO:
        """Helper to convert Ad model to AdResponseDTO"""
        if not ad:
            return None
        
        analysis_dto = AdAnalysisResponseDTO.from_orm(ad.analysis) if ad.analysis else None
        
        competitor_dto = CompetitorResponseDTO(
            id=ad.competitor.id,
            name=ad.competitor.name,
            page_id=ad.competitor.page_id,
            page_url=ad.competitor.page_url,
            is_active=ad.competitor.is_active,
            ads_count=len(ad.competitor.ads)
        ) if ad.competitor else None

        # Extract data from nested fields to populate top-level DTO fields
        first_creative = ad.creatives[0] if ad.creatives else {}
        main_media = first_creative.get('media', [])[0] if first_creative.get('media') else {}

        return AdResponseDTO(
            id=ad.id,
            ad_archive_id=ad.ad_archive_id,
            competitor=competitor_dto,
            analysis=analysis_dto,
            meta=ad.meta,
            targeting=ad.targeting,
            lead_form=ad.lead_form,
            creatives=ad.creatives,
            date_found=ad.date_found,
            created_at=ad.created_at,
            updated_at=ad.updated_at,
            
            # Populated from nested data
            ad_copy=first_creative.get('body'),
            main_title=first_creative.get('headline'),
            cta_text=first_creative.get('cta', {}).get('text'),
            cta_type=first_creative.get('cta', {}).get('type'),
            media_type=main_media.get('type'),
            media_url=main_media.get('url'),
            is_active=ad.meta.get('is_active') if ad.meta else None,
            start_date=ad.meta.get('start_date') if ad.meta else None,
            end_date=ad.meta.get('end_date') if ad.meta else None,
            duration_days=ad.duration_days,
        )
    
    def _convert_to_detail_dto(self, ad: Ad) -> AdDetailResponseDTO:
        """Helper to convert Ad model to AdDetailResponseDTO"""
        if not ad:
            return None
            
        analysis_dto = AdAnalysisResponseDTO.from_orm(ad.analysis) if ad.analysis else None
        
        competitor_dto = CompetitorResponseDTO(
            id=ad.competitor.id,
            name=ad.competitor.name,
            page_id=ad.competitor.page_id,
            page_url=ad.competitor.page_url,
            is_active=ad.competitor.is_active,
            ads_count=len(ad.competitor.ads)
        ) if ad.competitor else None

        first_creative = ad.creatives[0] if ad.creatives else {}
        main_media = first_creative.get('media', [])[0] if first_creative.get('media') else {}

        # The AdDetailResponseDTO has more fields, so we map them explicitly
        return AdDetailResponseDTO(
            id=ad.id,
            ad_archive_id=ad.ad_archive_id,
            competitor=competitor_dto,
            analysis=analysis_dto,
            meta=ad.meta,
            targeting=ad.targeting,
            lead_form=ad.lead_form,
            creatives=ad.creatives,
            date_found=ad.date_found,
            created_at=ad.created_at,
            updated_at=ad.updated_at,
            raw_data=ad.raw_data,
            
            # Populated from nested data
            ad_copy=first_creative.get('body'),
            main_title=first_creative.get('headline'),
            cta_text=first_creative.get('cta', {}).get('text'),
            cta_type=first_creative.get('cta', {}).get('type'),
            media_type=main_media.get('type'),
            media_url=main_media.get('url'),
            is_active=ad.meta.get('is_active') if ad.meta else None,
            start_date=ad.meta.get('start_date') if ad.meta else None,
            end_date=ad.meta.get('end_date') if ad.meta else None,
        )

    def delete_all_ads(self) -> int:
        """
        Delete all ads and their analyses from the database.
        
        Returns:
            Number of ads deleted
        """
        try:
            # First delete all analyses
            analyses_deleted = self.db.query(AdAnalysis).delete(synchronize_session=False)
            
            # Then delete all ads
            ads_deleted = self.db.query(Ad).delete(synchronize_session=False)
            
            self.db.commit()
            
            logger.info(f"Successfully deleted ALL {ads_deleted} ads and {analyses_deleted} analyses")
            return ads_deleted
            
        except Exception as e:
            logger.error(f"Error deleting all ads: {str(e)}")
            self.db.rollback()
            raise


def get_ad_service(db: Session) -> AdService:
    """
    Factory function to get AdService instance.
    
    Args:
        db: Database session
        
    Returns:
        AdService instance
    """
    return AdService(db) 


================================================
File: app/services/ai_service.py
================================================
import json
import logging
from typing import Dict, Any, Optional
import google.generativeai as genai
from app.core.config import settings

logger = logging.getLogger(__name__)

class AIService:
    """
    Service class for interacting with Google Gemini AI API
    to analyze ad content and generate insights.
    """
    
    def __init__(self):
        """Initialize the AI service with Google AI configuration."""
        if not settings.GOOGLE_AI_API_KEY:
            raise ValueError("GOOGLE_AI_API_KEY is not configured")
        
        # Configure the Google AI API
        genai.configure(api_key=settings.GOOGLE_AI_API_KEY)
        self.model = genai.GenerativeModel(settings.GOOGLE_AI_MODEL)
        
    def analyze_ad_content(self, ad_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze ad content using Google Gemini AI API.
        
        Args:
            ad_data: Dictionary containing ad information including:
                - ad_copy: Main ad text
                - main_title: Ad title
                - main_body_text: Ad body text
                - main_caption: Ad caption
                - cta_text: Call-to-action text
                - media_type: Type of media (video, image, etc.)
                - page_name: Advertiser page name
                - targeted_countries: List of targeted countries
                - card_titles: List of card titles (for carousel ads)
                - card_bodies: List of card body texts (for carousel ads)
                
        Returns:
            Dict containing AI analysis results
        """
        try:
            # Construct the analysis prompt
            prompt = self._build_analysis_prompt(ad_data)
            
            logger.info(f"Sending analysis request to Google AI for ad {ad_data.get('ad_archive_id', 'unknown')}")
            
            # Generate response from AI
            response = self.model.generate_content(prompt)
            
            # Parse the response
            analysis_result = self._parse_ai_response(response.text)
            
            # Add metadata
            analysis_result['ai_prompts'] = {
                'analysis_prompt': prompt,
                'model_used': settings.GOOGLE_AI_MODEL
            }
            analysis_result['raw_ai_response'] = {
                'full_response': response.text,
                'model': settings.GOOGLE_AI_MODEL
            }
            analysis_result['analysis_version'] = 'v1.0-gemini'
            
            logger.info(f"Successfully analyzed ad {ad_data.get('ad_archive_id', 'unknown')}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Error analyzing ad content: {str(e)}")
            raise
    
    def _build_analysis_prompt(self, ad_data: Dict[str, Any]) -> str:
        """
        Build a comprehensive analysis prompt for the AI model.
        
        Args:
            ad_data: Dictionary containing ad information
            
        Returns:
            Formatted prompt string
        """
        # Extract relevant ad content
        ad_copy = ad_data.get('ad_copy', '')
        main_title = ad_data.get('main_title', '')
        main_body_text = ad_data.get('main_body_text', '')
        main_caption = ad_data.get('main_caption', '')
        cta_text = ad_data.get('cta_text', '')
        media_type = ad_data.get('media_type', '')
        page_name = ad_data.get('page_name', '')
        targeted_countries = ad_data.get('targeted_countries', [])
        card_titles = ad_data.get('card_titles', [])
        card_bodies = ad_data.get('card_bodies', [])
        
        # Build the comprehensive prompt
        prompt = f"""
You are an expert digital marketing analyst specializing in Facebook/Meta advertising. 
Analyze the following ad content and provide a comprehensive analysis in JSON format.

AD CONTENT TO ANALYZE:
======================
Advertiser: {page_name}
Main Title: {main_title}
Main Body Text: {main_body_text}
Ad Copy: {ad_copy}
Caption: {main_caption}
Call-to-Action: {cta_text}
Media Type: {media_type}
Target Countries: {', '.join(targeted_countries) if targeted_countries else 'Not specified'}
"""

        # Add carousel content if present
        if card_titles or card_bodies:
            prompt += f"\nCarousel Cards:\n"
            for i, (title, body) in enumerate(zip(card_titles or [], card_bodies or [])):
                prompt += f"Card {i+1} - Title: {title}, Body: {body}\n"

        # Add analysis instructions
        prompt += f"""

ANALYSIS REQUIREMENTS:
======================
Provide your analysis as a valid JSON object with the following structure:
{{
    "summary": "A comprehensive 2-3 sentence summary of the ad's main message and approach",
    "hook_score": 8.5,
    "overall_score": 7.2,
    "target_audience": "Specific target audience description",
    "content_themes": ["theme1", "theme2", "theme3"],
    "effectiveness_analysis": {{
        "strengths": ["strength1", "strength2"],
        "weaknesses": ["weakness1", "weakness2"],
        "recommendations": ["recommendation1", "recommendation2"]
    }},
    "ad_format_analysis": {{
        "format_effectiveness": "Analysis of the ad format choice",
        "media_type_appropriateness": "Assessment of media type choice",
        "cta_effectiveness": "Analysis of call-to-action effectiveness"
    }},
    "competitor_insights": {{
        "positioning": "How the brand positions itself",
        "unique_selling_points": ["USP1", "USP2"],
        "competitive_advantage": "Main competitive advantage highlighted"
    }},
    "performance_predictions": {{
        "predicted_engagement_rate": 2.5,
        "predicted_click_through_rate": 1.8,
        "audience_fit_score": 8.0,
        "conversion_potential": "high/medium/low"
    }},
    "confidence_score": 0.85
}}

SCORING CRITERIA:
=================
- hook_score (1-10): Rate how engaging and attention-grabbing the opening/hook is
- overall_score (1-10): Rate the overall effectiveness of the ad
- confidence_score (0-1): Your confidence level in this analysis

ANALYSIS FOCUS:
===============
- Identify the primary value proposition
- Assess the emotional appeal and psychological triggers
- Evaluate the clarity and persuasiveness of the message
- Consider the target market fit
- Analyze the call-to-action effectiveness
- Assess visual and textual harmony (if applicable)

Return ONLY the JSON object, no additional text or formatting.
"""
        
        return prompt
    
    def _parse_ai_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parse the AI response and extract structured data.
        
        Args:
            response_text: Raw response text from AI
            
        Returns:
            Parsed analysis results
        """
        try:
            # Try to find JSON in the response
            response_text = response_text.strip()
            
            # Remove any markdown formatting
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            # Parse JSON
            analysis_result = json.loads(response_text)
            
            # Validate required fields and set defaults
            analysis_result.setdefault('summary', 'AI analysis completed')
            analysis_result.setdefault('hook_score', 5.0)
            analysis_result.setdefault('overall_score', 5.0)
            analysis_result.setdefault('confidence_score', 0.7)
            analysis_result.setdefault('target_audience', 'General audience')
            analysis_result.setdefault('content_themes', [])
            
            # Ensure scores are within valid range
            analysis_result['hook_score'] = max(0, min(10, float(analysis_result['hook_score'])))
            analysis_result['overall_score'] = max(0, min(10, float(analysis_result['overall_score'])))
            analysis_result['confidence_score'] = max(0, min(1, float(analysis_result['confidence_score'])))
            
            return analysis_result
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response as JSON: {e}")
            logger.error(f"Response text: {response_text}")
            
            # Return fallback analysis
            return {
                'summary': 'AI analysis completed with parsing error',
                'hook_score': 5.0,
                'overall_score': 5.0,
                'confidence_score': 0.5,
                'target_audience': 'General audience',
                'content_themes': [],
                'parse_error': str(e),
                'raw_response': response_text
            }
        except Exception as e:
            logger.error(f"Error parsing AI response: {e}")
            raise


# Create a singleton instance
try:
    ai_service = AIService() if settings.AI_ANALYSIS_ENABLED and settings.GOOGLE_AI_API_KEY else None
except Exception as e:
    # If there's an error creating the service (e.g., missing dependencies), set to None
    logger.warning(f"Failed to create AI service: {e}")
    ai_service = None


def get_ai_service() -> Optional[AIService]:
    """
    Get the AI service instance.
    
    Returns:
        AIService instance or None if not configured
    """
    return ai_service 


================================================
File: app/services/competitor_service.py
================================================
from typing import List, Optional, Tuple
from sqlalchemy.orm import Session
from sqlalchemy import func, and_, or_
from fastapi import HTTPException
import logging
from datetime import datetime
import math

from app.models.competitor import Competitor
from app.models.ad import Ad
from app.models.ad_analysis import AdAnalysis
from app.models.dto.competitor_dto import (
    CompetitorCreateDTO,
    CompetitorUpdateDTO,
    CompetitorResponseDTO,
    CompetitorDetailResponseDTO,
    PaginatedCompetitorResponseDTO,
    CompetitorFilterParams,
    CompetitorStatsResponseDTO
)

logger = logging.getLogger(__name__)


class CompetitorService:
    """Service for managing competitor operations"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def create_competitor(self, competitor_data: CompetitorCreateDTO) -> CompetitorResponseDTO:
        """Create a new competitor"""
        try:
            # Check if competitor with same page_id already exists
            existing = self.db.query(Competitor).filter(
                Competitor.page_id == competitor_data.page_id
            ).first()
            
            if existing:
                raise HTTPException(
                    status_code=400,
                    detail=f"Competitor with page_id '{competitor_data.page_id}' already exists"
                )
            
            # Create new competitor
            competitor = Competitor(
                name=competitor_data.name,
                page_id=competitor_data.page_id,
                is_active=competitor_data.is_active
            )
            
            self.db.add(competitor)
            self.db.commit()
            self.db.refresh(competitor)
            
            logger.info(f"Created new competitor: {competitor.name} (ID: {competitor.id})")
            
            return CompetitorResponseDTO(
                id=competitor.id,
                name=competitor.name,
                page_id=competitor.page_id,
                is_active=competitor.is_active,
                ads_count=0,
                created_at=competitor.created_at,
                updated_at=competitor.updated_at
            )
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error creating competitor: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error creating competitor: {str(e)}")
    
    def get_competitors(self, filters: CompetitorFilterParams) -> PaginatedCompetitorResponseDTO:
        """Get paginated list of competitors with filtering"""
        try:
            # Build base query
            query = self.db.query(Competitor)
            
            # Apply filters
            if filters.is_active is not None:
                query = query.filter(Competitor.is_active == filters.is_active)
            
            if filters.search:
                search_term = f"%{filters.search}%"
                query = query.filter(
                    or_(
                        Competitor.name.ilike(search_term),
                        Competitor.page_id.ilike(search_term)
                    )
                )
            
            # Get total count before pagination
            total = query.count()
            
            # Apply sorting
            if filters.sort_by == "name":
                order_column = Competitor.name
            elif filters.sort_by == "page_id":
                order_column = Competitor.page_id
            elif filters.sort_by == "is_active":
                order_column = Competitor.is_active
            elif filters.sort_by == "updated_at":
                order_column = Competitor.updated_at
            else:
                order_column = Competitor.created_at
            
            if filters.sort_order == "asc":
                query = query.order_by(order_column.asc())
            else:
                query = query.order_by(order_column.desc())
            
            # Apply pagination
            offset = (filters.page - 1) * filters.page_size
            competitors = query.offset(offset).limit(filters.page_size).all()
            
            # Get ads count for each competitor
            competitor_ids = [comp.id for comp in competitors]
            ads_counts = {}
            
            if competitor_ids:
                ads_count_query = self.db.query(
                    Ad.competitor_id,
                    func.count(Ad.id).label('ads_count')
                ).filter(
                    Ad.competitor_id.in_(competitor_ids)
                ).group_by(Ad.competitor_id).all()
                
                ads_counts = {comp_id: count for comp_id, count in ads_count_query}
            
            # Build response
            competitor_data = []
            for competitor in competitors:
                competitor_data.append(CompetitorResponseDTO(
                    id=competitor.id,
                    name=competitor.name,
                    page_id=competitor.page_id,
                    is_active=competitor.is_active,
                    ads_count=ads_counts.get(competitor.id, 0),
                    created_at=competitor.created_at,
                    updated_at=competitor.updated_at
                ))
            
            # Calculate pagination info
            total_pages = math.ceil(total / filters.page_size)
            has_next = filters.page < total_pages
            has_previous = filters.page > 1
            
            return PaginatedCompetitorResponseDTO(
                data=competitor_data,
                total=total,
                page=filters.page,
                page_size=filters.page_size,
                total_pages=total_pages,
                has_next=has_next,
                has_previous=has_previous
            )
            
        except Exception as e:
            logger.error(f"Error fetching competitors: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error fetching competitors: {str(e)}")
    
    def get_competitor_by_id(self, competitor_id: int) -> CompetitorDetailResponseDTO:
        """Get detailed information about a specific competitor"""
        try:
            competitor = self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).first()
            
            if not competitor:
                raise HTTPException(status_code=404, detail="Competitor not found")
            
            # Get ads statistics
            total_ads = self.db.query(Ad).filter(Ad.competitor_id == competitor_id).count()
            active_ads = self.db.query(Ad).filter(
                Ad.competitor_id == competitor_id,
                Ad.meta['is_active'].as_boolean() == True
            ).count()
            
            # Get analyzed ads count
            analyzed_ads = self.db.query(AdAnalysis).join(
                Ad, AdAnalysis.ad_id == Ad.id
            ).filter(
                Ad.competitor_id == competitor_id
            ).count()
            
            # Variables already set above
            
            return CompetitorDetailResponseDTO(
                id=competitor.id,
                name=competitor.name,
                page_id=competitor.page_id,
                is_active=competitor.is_active,
                ads_count=total_ads,
                active_ads_count=active_ads,
                analyzed_ads_count=analyzed_ads,
                created_at=competitor.created_at,
                updated_at=competitor.updated_at
            )
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Error fetching competitor {competitor_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error fetching competitor: {str(e)}")
    
    def bulk_delete_competitors(self, competitor_ids: List[int]) -> dict:
        """
        Bulk delete competitors.
        - Soft delete competitors with ads.
        - Hard delete competitors without ads.
        """
        if not competitor_ids:
            raise HTTPException(status_code=400, detail="No competitor IDs provided")

        soft_deleted_count = 0
        hard_deleted_count = 0
        not_found_count = 0
        
        try:
            competitors_to_process = self.db.query(Competitor).filter(
                Competitor.id.in_(competitor_ids)
            ).all()
            
            found_ids = {c.id for c in competitors_to_process}
            not_found_count = len(competitor_ids) - len(found_ids)

            ids_to_check_ads = [comp.id for comp in competitors_to_process]
            
            ads_counts_query = self.db.query(
                Ad.competitor_id,
                func.count(Ad.id).label('ads_count')
            ).filter(
                Ad.competitor_id.in_(ids_to_check_ads)
            ).group_by(Ad.competitor_id).all()

            ads_counts = {comp_id: count for comp_id, count in ads_counts_query}
            
            ids_for_soft_delete = []
            ids_for_hard_delete = []

            for comp_id in ids_to_check_ads:
                if ads_counts.get(comp_id, 0) > 0:
                    ids_for_soft_delete.append(comp_id)
                else:
                    ids_for_hard_delete.append(comp_id)
            
            # Perform soft deletes
            if ids_for_soft_delete:
                self.db.query(Competitor).filter(
                    Competitor.id.in_(ids_for_soft_delete)
                ).update(
                    {'is_active': False, 'updated_at': datetime.utcnow()},
                    synchronize_session=False
                )
                soft_deleted_count = len(ids_for_soft_delete)
            
            # Perform hard deletes
            if ids_for_hard_delete:
                self.db.query(Competitor).filter(
                    Competitor.id.in_(ids_for_hard_delete)
                ).delete(synchronize_session=False)
                hard_deleted_count = len(ids_for_hard_delete)
            
            self.db.commit()
            
            logger.info(
                f"Bulk delete completed. Soft deleted: {soft_deleted_count}, "
                f"Hard deleted: {hard_deleted_count}, Not found: {not_found_count}"
            )
            
            return {
                "message": "Bulk delete operation completed.",
                "soft_deleted_count": soft_deleted_count,
                "hard_deleted_count": hard_deleted_count,
                "not_found_count": not_found_count
            }

        except Exception as e:
            self.db.rollback()
            logger.error(f"Error during bulk competitor deletion: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"An error occurred during bulk deletion: {str(e)}"
            )
    
    def update_competitor(
        self, 
        competitor_id: int, 
        competitor_data: CompetitorUpdateDTO
    ) -> CompetitorDetailResponseDTO:
        """Update an existing competitor"""
        try:
            competitor = self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).first()
            
            if not competitor:
                raise HTTPException(status_code=404, detail="Competitor not found")
            
            # Check if page_id is being updated and if it conflicts with existing competitor
            if competitor_data.page_id and competitor_data.page_id != competitor.page_id:
                existing = self.db.query(Competitor).filter(
                    and_(
                        Competitor.page_id == competitor_data.page_id,
                        Competitor.id != competitor_id
                    )
                ).first()
                
                if existing:
                    raise HTTPException(
                        status_code=400,
                        detail=f"Another competitor with page_id '{competitor_data.page_id}' already exists"
                    )
            
            # Update fields
            if competitor_data.name is not None:
                competitor.name = competitor_data.name
            if competitor_data.page_id is not None:
                competitor.page_id = competitor_data.page_id
            if competitor_data.is_active is not None:
                competitor.is_active = competitor_data.is_active
            
            competitor.updated_at = datetime.utcnow()
            
            self.db.commit()
            self.db.refresh(competitor)
            
            logger.info(f"Updated competitor: {competitor.name} (ID: {competitor.id})")
            
            # Return detailed response
            return self.get_competitor_by_id(competitor_id)
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error updating competitor {competitor_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error updating competitor: {str(e)}")
    
    def delete_competitor(self, competitor_id: int) -> dict:
        """Delete a competitor (soft delete by setting is_active to False)"""
        try:
            competitor = self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).first()
            
            if not competitor:
                raise HTTPException(status_code=404, detail="Competitor not found")
            
            # Check if competitor has ads
            ads_count = self.db.query(Ad).filter(Ad.competitor_id == competitor_id).count()
            
            if ads_count > 0:
                # Soft delete - just deactivate
                competitor.is_active = False
                competitor.updated_at = datetime.utcnow()
                self.db.commit()
                
                logger.info(f"Soft deleted competitor: {competitor.name} (ID: {competitor.id}) - {ads_count} ads retained")
                
                return {
                    "message": f"Competitor '{competitor.name}' has been deactivated. {ads_count} ads are retained.",
                    "competitor_id": competitor_id,
                    "ads_count": ads_count,
                    "soft_delete": True
                }
            else:
                # Hard delete - no ads associated
                competitor_name = competitor.name
                self.db.delete(competitor)
                self.db.commit()
                
                logger.info(f"Hard deleted competitor: {competitor_name} (ID: {competitor_id}) - no ads")
                
                return {
                    "message": f"Competitor '{competitor_name}' has been permanently deleted.",
                    "competitor_id": competitor_id,
                    "ads_count": 0,
                    "soft_delete": False
                }
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error deleting competitor {competitor_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error deleting competitor: {str(e)}")
    
    def get_competitor_stats(self) -> CompetitorStatsResponseDTO:
        """Get comprehensive statistics about competitors"""
        try:
            # Basic counts
            total_competitors = self.db.query(Competitor).count()
            active_competitors = self.db.query(Competitor).filter(Competitor.is_active == True).count()
            inactive_competitors = total_competitors - active_competitors
            
            # Competitors with ads
            competitors_with_ads = self.db.query(Competitor.id).distinct().join(Ad).count()
            
            # Total ads across all competitors
            total_ads = self.db.query(Ad).count()
            
            # Average ads per competitor
            avg_ads_per_competitor = total_ads / total_competitors if total_competitors > 0 else 0
            
            return CompetitorStatsResponseDTO(
                total_competitors=total_competitors,
                active_competitors=active_competitors,
                inactive_competitors=inactive_competitors,
                competitors_with_ads=competitors_with_ads,
                total_ads_across_competitors=total_ads,
                avg_ads_per_competitor=round(avg_ads_per_competitor, 2)
            )
            
        except Exception as e:
            logger.error(f"Error fetching competitor stats: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error fetching competitor stats: {str(e)}")
    
    def search_competitors(self, search_term: str, limit: int = 50) -> List[CompetitorResponseDTO]:
        """Search competitors by name or page_id"""
        try:
            search_pattern = f"%{search_term}%"
            
            competitors = self.db.query(Competitor).filter(
                or_(
                    Competitor.name.ilike(search_pattern),
                    Competitor.page_id.ilike(search_pattern)
                )
            ).limit(limit).all()
            
            # Get ads count for each competitor
            competitor_ids = [comp.id for comp in competitors]
            ads_counts = {}
            
            if competitor_ids:
                ads_count_query = self.db.query(
                    Ad.competitor_id,
                    func.count(Ad.id).label('ads_count')
                ).filter(
                    Ad.competitor_id.in_(competitor_ids)
                ).group_by(Ad.competitor_id).all()
                
                ads_counts = {comp_id: count for comp_id, count in ads_count_query}
            
            return [
                CompetitorResponseDTO(
                    id=competitor.id,
                    name=competitor.name,
                    page_id=competitor.page_id,
                    is_active=competitor.is_active,
                    ads_count=ads_counts.get(competitor.id, 0),
                    created_at=competitor.created_at,
                    updated_at=competitor.updated_at
                )
                for competitor in competitors
            ]
            
        except Exception as e:
            logger.error(f"Error searching competitors: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error searching competitors: {str(e)}") 


================================================
File: app/services/enhanced_ad_extraction.py
================================================
import json
from datetime import datetime, date
from typing import Dict, List, Optional, Any, Tuple
import logging
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from app.models import Ad, Competitor
from app.database import get_db

logger = logging.getLogger(__name__)


class EnhancedAdExtractionService:
    """Service for enhanced ad data extraction matching frontend_payload_final.json format"""
    
    EXTRACTION_VERSION = "1.0.0"
    
    def __init__(self, db: Session):
        self.db = db
        self.logger = logging.getLogger(__name__)
    
    def convert_timestamp_to_date(self, ts: Any) -> Optional[str]:
        """Converts a UNIX timestamp to a 'YYYY-MM-DD' formatted string."""
        if not ts: 
            return None
        try:
            return datetime.fromtimestamp(ts).strftime('%Y-%m-%d')
        except (ValueError, TypeError):
            return None
    
    def calculate_duration_days(self, start_date_str: Optional[str], end_date_str: Optional[str], is_active: bool = False) -> Optional[int]:
        """Calculate duration in days between start_date and end_date (or current date if active)"""
        if not start_date_str:
            return None
        
        try:
            start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
            
            if is_active or not end_date_str:
                # If ad is active or no end date, calculate up to today
                end_date = date.today()
            else:
                end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
            
            duration = (end_date - start_date).days
            return max(duration, 1)  # Ensure minimum of 1 day
            
        except (ValueError, TypeError) as e:
            logger.warning(f"Error calculating duration: start={start_date_str}, end={end_date_str}, error={e}")
            return None
    
    def parse_dynamic_lead_form(self, extra_texts: List[Dict]) -> Dict:
        """
        Dynamically parses lead form questions and options from extra_texts.
        This is the key function from test_ad_extraction.py
        """
        if not extra_texts:
            return {}
        
        form_details = {"questions": {}, "standalone_fields": []}
        standalone_field_keywords = {"full name", "email", "phone number", "country"}
        current_question = None
        
        for item in extra_texts:
            text = (item.get("text") or "").strip()
            if not text:
                continue
            
            text_lower = text.lower()
            
            # Check if this is a standalone field
            if any(keyword in text_lower for keyword in standalone_field_keywords) and len(text.split()) < 4:
                form_details["standalone_fields"].append(text)
                current_question = None
                continue
            
            # Check if this is a question (ends with ':')
            if text.endswith(':'):
                current_question = text
                form_details["questions"][current_question] = []
            elif current_question and len(text.split()) < 6 and "\n" not in text:
                # This is likely an option for the current question
                form_details["questions"][current_question].append(text)
        
        # Clean up questions with no options
        form_details["questions"] = {q: opts for q, opts in form_details["questions"].items() if opts}
        
        # Remove duplicates from standalone fields and sort
        form_details["standalone_fields"] = sorted(list(set(form_details["standalone_fields"])))
        
        return form_details
    
    def build_detailed_creative_object(self, creative_data: Dict) -> Optional[Dict]:
        """
        Builds a highly detailed object for a single creative (card).
        This matches the logic from test_ad_extraction.py
        """
        if not isinstance(creative_data, dict):
            return None
        
        # Safely extract and strip text fields
        headline = (creative_data.get("title") or "").strip()
        body_data = creative_data.get("body")
        body = (body_data.get("text") or "").strip() if isinstance(body_data, dict) else (body_data or "").strip()
        caption = (creative_data.get("caption") or "").strip()

        # Transform media URLs into a list of media objects
        media_list = []
        if url := creative_data.get("video_hd_url"):
            media_list.append({"type": "Video", "url": url})
        if url := creative_data.get("video_sd_url"):
            media_list.append({"type": "Video", "url": url})
        if url := creative_data.get("original_image_url"):
            media_list.append({"type": "Image", "url": url})
        if url := creative_data.get("resized_image_url"):
            media_list.append({"type": "Image", "url": url})
        if url := creative_data.get("video_preview_image_url"):
             media_list.append({"type": "Image", "url": url})

        creative_object = {
            "headline": headline or None,
            "body": body or None,
            "cta": {
                "text": creative_data.get("cta_text"), 
                "type": creative_data.get("cta_type")
            },
            "media": media_list,
            "link": {
                "url": creative_data.get("link_url"), 
                "caption": caption or None
            }
        }
        
        # Clean up empty values in dictionaries
        for key in ["cta", "link"]:
            if isinstance(creative_object[key], dict):
                creative_object[key] = {k: v for k, v in creative_object[key].items() if v}
        
        return creative_object
    
    def extract_targeting_data(self, ad_data: Dict) -> Dict:
        """
        Extracts detailed targeting and reach data, filtering for countries with non-zero reach.
        This is the enhanced logic from test_ad_extraction.py
        """
        transparency_data = (ad_data.get("enrichment_response", {})
                            .get("data", {})
                            .get("ad_library_main", {})
                            .get("ad_details", {})
                            .get("transparency_by_location", {}))
        
        targeting_info = {
            "locations": [], 
            "age_range": None, 
            "gender": None, 
            "reach_breakdown": [], 
            "total_reach": None
        }
        
        # Check both UK and EU transparency data
        for region in ["uk_transparency", "eu_transparency"]:
            if region_data := transparency_data.get(region):
                targeting_info["locations"] = region_data.get("location_audience", [])
                targeting_info["age_range"] = region_data.get("age_audience")
                targeting_info["gender"] = region_data.get("gender_audience")
                targeting_info["total_reach"] = (region_data.get("total_reach") or 
                                               region_data.get("eu_total_reach"))
                
                # Smart Filtering Logic - only include countries with non-zero reach
                filtered_reach_breakdown = []
                for country_breakdown in region_data.get("age_country_gender_reach_breakdown", []):
                    total_country_reach = 0
                    for age_gender in country_breakdown.get("age_gender_breakdowns", []):
                        total_country_reach += age_gender.get("male") or 0
                        total_country_reach += age_gender.get("female") or 0
                        total_country_reach += age_gender.get("unknown") or 0
                    
                    if total_country_reach > 0:
                        filtered_reach_breakdown.append(country_breakdown)
                
                targeting_info["reach_breakdown"] = filtered_reach_breakdown
                break
        
        # Return only non-empty values
        return {k: v for k, v in targeting_info.items() if v}
    
    def build_clean_ad_object(self, ad_data: Dict) -> Optional[Dict]:
        """
        Transforms a single raw ad variation into a clean, structured object.
        This is the main transformation logic from test_ad_extraction.py
        """
        snapshot = ad_data.get("snapshot", {})
        if not snapshot:
            return None
        
        ad_object = {
            "ad_archive_id": ad_data.get("ad_archive_id"),
            "meta": {
                "is_active": ad_data.get("is_active", False),
                "cta_type": snapshot.get("cta_type"),
                "display_format": snapshot.get("display_format"),
            },
            "targeting": self.extract_targeting_data(ad_data),
            "lead_form": self.parse_dynamic_lead_form(snapshot.get("extra_texts", [])),
            "creatives": []
        }
        
        # Process cards/creatives
        creatives_source = []
        if snapshot.get("cards"):
            creatives_source.extend(snapshot["cards"])
        
        if not creatives_source:
            if snapshot.get("videos"):
                creatives_source.extend(snapshot["videos"])
            if snapshot.get("images"):
                 creatives_source.extend(snapshot["images"])

        if not creatives_source and (snapshot.get('title') or snapshot.get('body')):
             creatives_source = [snapshot]

        for i, creative_data in enumerate(creatives_source):
            detailed_creative = self.build_detailed_creative_object(creative_data)
            if detailed_creative:
                detailed_creative['id'] = f"{ad_object['ad_archive_id']}-{i}"
                ad_object['creatives'].append(detailed_creative)
        
        return ad_object
    
    def transform_raw_data_to_enhanced_format(self, file_data_list: List[Dict]) -> Dict:
        """
        Main function to process raw JSON responses and transform them into
        clean, frontend-ready payload matching frontend_payload_final.json
        """
        campaigns = {}
        advertiser_info = {}
        
        for data in file_data_list:
            edges = (data.get("data", {})
                    .get("ad_library_main", {})
                    .get("search_results_connection", {})
                    .get("edges", []))
            
            if not edges:
                continue
            
            # Extract advertiser info from first ad if not already set
            if not advertiser_info:
                try:
                    first_ad = edges[0]['node']['collated_results'][0]
                    advertiser_info = {
                        "page_id": first_ad.get("page_id"),
                        "page_name": first_ad.get("page_name"),
                        "page_url": first_ad.get("snapshot", {}).get("page_profile_uri"),
                        "page_likes": first_ad.get("snapshot", {}).get("page_like_count"),
                        "page_profile_picture": first_ad.get("snapshot", {}).get("page_profile_picture_url"),
                    }
                except (IndexError, KeyError) as e:
                    logger.warning(f"Could not extract advertiser info: {e}")
            
            # Process each ad variation
            for edge in edges:
                for ad_variation_data in edge.get("node", {}).get("collated_results", []):
                    campaign_id = ad_variation_data.get("collation_id")
                    if not campaign_id:
                        continue
                    
                    # Initialize campaign if not exists
                    if campaign_id not in campaigns:
                        campaigns[campaign_id] = {
                            "campaign_id": campaign_id,
                            "platforms": set(),
                            "ads": {}
                        }
                    
                    # Add platforms
                    for platform in ad_variation_data.get("publisher_platform", []):
                        campaigns[campaign_id]["platforms"].add(platform)
                    
                    # Process individual ad
                    ad_archive_id = ad_variation_data.get("ad_archive_id")
                    if ad_archive_id not in campaigns[campaign_id]["ads"]:
                        clean_ad = self.build_clean_ad_object(ad_variation_data)
                        if clean_ad:
                            # Add date information to meta
                            clean_ad["meta"]["start_date"] = self.convert_timestamp_to_date(
                                ad_variation_data.get("start_date")
                            )
                            clean_ad["meta"]["end_date"] = self.convert_timestamp_to_date(
                                ad_variation_data.get("end_date")
                            )
                            
                            campaigns[campaign_id]["ads"][ad_archive_id] = clean_ad
        
        # Finalize campaigns structure
        final_campaigns = []
        for cid, camp_data in campaigns.items():
            camp_data["platforms"] = sorted(list(camp_data["platforms"]))
            camp_data["ads"] = list(camp_data["ads"].values())
            final_campaigns.append(camp_data)
        
        return {
            "advertiser_info": advertiser_info, 
            "campaigns": final_campaigns
        }
    
    def save_enhanced_ads_to_database(self, enhanced_data: Dict) -> Dict:
        """
        Save enhanced ads data to database using the new Ad model structure
        """
        stats = {
            'total_processed': 0,
            'created': 0,
            'updated': 0,
            'errors': 0,
            'competitors_created': 0,
            'competitors_updated': 0,
            'campaigns_processed': 0
        }
        
        try:
            advertiser_info = enhanced_data.get("advertiser_info", {})
            campaigns = enhanced_data.get("campaigns", [])
            
            # Process each campaign
            for campaign in campaigns:
                stats['campaigns_processed'] += 1
                campaign_id = campaign.get("campaign_id")
                platforms = campaign.get("platforms", [])
                
                # Process each ad in the campaign
                for ad_data in campaign.get("ads", []):
                    try:
                        stats['total_processed'] += 1
                        
                        # Find or create competitor
                        competitor = self._find_or_create_competitor_from_advertiser(advertiser_info)
                        if competitor:
                            if hasattr(competitor, '_is_new'):
                                stats['competitors_created'] += 1
                            else:
                                stats['competitors_updated'] += 1
                        
                        # Create or update ad with enhanced data
                        ad, is_new = self._create_or_update_enhanced_ad(
                            ad_data, competitor.id, campaign_id, platforms, advertiser_info
                        )
                        
                        if is_new:
                            stats['created'] += 1
                        else:
                            stats['updated'] += 1
                        
                    except Exception as e:
                        stats['errors'] += 1
                        logger.error(f"Error saving enhanced ad {ad_data.get('ad_archive_id', 'unknown')}: {e}")
            
            # Commit all changes
            self.db.commit()
            logger.info(f"Successfully saved enhanced ads data. Stats: {stats}")
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Failed to commit enhanced database changes: {e}")
            stats['errors'] = stats['total_processed']
            stats['created'] = 0
            stats['updated'] = 0
        
        return stats
    
    def _find_or_create_competitor_from_advertiser(self, advertiser_info: Dict) -> Optional[Competitor]:
        """Find existing competitor from advertiser info (no longer creates new ones)"""
        page_id = advertiser_info.get('page_id')
        page_name = advertiser_info.get('page_name')
        page_url = advertiser_info.get('page_url')

        if not page_id or not page_name:
            logger.warning("Missing page_id or page_name in advertiser info")
            return None
        
        # Try to find existing competitor
        existing_competitor = self.db.query(Competitor).filter_by(page_id=page_id).first()
        
        if existing_competitor:
            # Update competitor info
            existing_competitor.name = page_name
            existing_competitor.page_url = page_url
            existing_competitor.updated_at = datetime.utcnow()
            return existing_competitor
        
        # No longer create new competitors
        logger.info(f"Competitor not found with page_id: {page_id}, skipping")
        return None
    
    def _create_or_update_enhanced_ad(self, ad_data: Dict, competitor_id: int, 
                                     campaign_id: str, platforms: List[str], 
                                     advertiser_info: Dict) -> Tuple[Ad, bool]:
        """
        Creates a new ad or updates an existing one based on the enhanced data.
        The 'campaign_id' and 'platforms' are currently for logging/future use
        and are not directly saved to the Ad model.
        """
        ad_archive_id = ad_data.get("ad_archive_id")
        session = self.db
        
        # Try to find an existing ad
        existing_ad = session.query(Ad).filter(Ad.ad_archive_id == ad_archive_id).first()
        
        # Prepare the data for insertion/update
        meta_data = ad_data.get("meta", {})
        start_date = meta_data.get("start_date")
        end_date = meta_data.get("end_date") 
        is_active = meta_data.get("is_active", False)
        
        # Calculate duration in days
        duration_days = self.calculate_duration_days(start_date, end_date, is_active)
        
        db_data = {
            "competitor_id": competitor_id,
            "ad_archive_id": ad_archive_id,
            "duration_days": duration_days,
            "meta": ad_data.get("meta"),
            "targeting": ad_data.get("targeting"),
            "lead_form": ad_data.get("lead_form"),
            "creatives": ad_data.get("creatives"),
            "raw_data": ad_data,
        }
        
        if existing_ad:
            # Update existing ad
            for key, value in db_data.items():
                setattr(existing_ad, key, value)
            
            self.logger.info(f"Updating enhanced ad: {ad_archive_id}")
            created = False
        else:
            # Create new ad
            db_data["date_found"] = datetime.now()
            existing_ad = Ad(**db_data)
            self.db.add(existing_ad)
            self.logger.info(f"Creating new enhanced ad: {ad_archive_id}")
            created = True
        
        return existing_ad, created
    
    def process_raw_responses(self, raw_responses: List[Dict]) -> Tuple[Dict, Dict]:
        """
        Process raw JSON responses from Facebook Ads Library and save to database
        
        Args:
            raw_responses: List of raw JSON responses from Facebook Ads Library
            
        Returns:
            Tuple of (enhanced_data, stats)
        """
        self.logger.info(f"Processing {len(raw_responses)} raw responses with enhanced extraction")
        
        # Transform raw data to enhanced format
        enhanced_data = self.transform_raw_data_to_enhanced_format(raw_responses)
        
        # Save enhanced data to database
        stats = self.save_enhanced_ads_to_database(enhanced_data)
        
        return enhanced_data, stats 


================================================
File: app/services/facebook_ads_scraper.py
================================================
import requests
import json
import csv
import time
import uuid
from urllib.parse import urlencode
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import logging
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
import re
import urllib.parse

from app.models import Ad, Competitor
from app.database import get_db
from app.services.enhanced_ad_extraction import EnhancedAdExtractionService

logger = logging.getLogger(__name__)


class FacebookAdsScraperConfig:
    """
    Configuration class for Facebook Ads scraper.
    
    To keep this script working, you may need to update the session-related
    values below. You can get these by inspecting the network requests in your
    browser's developer tools when browsing the Facebook Ad Library.
    
    1. Open the Ad Library in a new browser tab.
    2. Open the Developer Tools (F12) and go to the "Network" tab.
    3. Perform a search for an advertiser.
    4. Find a 'graphql' request, right-click it, and copy the cookie string
       and other relevant values like 'lsd' and 'jazoest'.
    """
    
    def __init__(
        self,
        # --- Search parameters ---
        active_status: str = 'active',
        ad_type: str = 'ALL',
        countries: List[str] = None,
        search_type: str = 'page',
        media_type: str = 'all',
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        query_string: str = '',
        page_ids: List[str] = None,
        view_all_page_id: str = '1591077094491398',
        cursor: Optional[str] = None,
        first: int = 30,
        is_targeted_country: bool = False,
        
        # --- Session parameters ---
        session_id: str = '2937f803-041c-481d-b565-81ae712d5209',
        collation_token: str = str(uuid.uuid4()),  # Random UUID for collation token
        lsd_token: str = 'AVq28ysAAt0',
        jazoest: str = '2900',
        cookie: str = 'datr=n14YaOmLyHCUO9eDBSPbd0bo; sb=n14YaN0pXid7MQ54q1q9-XHz; ps_l=1; ps_n=1; dpr=2.5; fr=1vnX3O3Y99uUSRWaj.AWchuA7IeOh24kEy8WvwbrIwUUScHnrL8m0ga2ev4Py-aNisJdM.BoaRgt..AAA.0.0.BoaRgt.AWcCsNYL2f427xwHLtWS3CN2SlU; wd=891x831',

        # --- Dynamic payload components (may need updating) ---
        search_payload_dyn: str = '7xeUmwlECdwn8K2Wmh0no6u5U4e1Fx-ewSAwHwNw9G2S2q0_EtxG4o0B-qbwgE1EEb87C1xwEwgo9oO0n24oaEd86a3a1YwBgao6C0Mo6i588Etw8WfK1LwPxe2GewbCXwJwmEtwse5o4q0HU1IEGdw46wbLwrU6C2-0VE6O1Fw59G2O1TwmU3ywo8',
        search_payload_csr: str = 'htOh24lsOWjORb9uQAheC8KVpaGuHGF8GBx2UKp2qzVUiCBxm6GwTBwBwFBx216G15whrx6482TKEuzU8E6aUdU2qwgo8E7jwsE1BU2axy0RUkxC8w4dwTw10K0aswOU02yHyE07_h00iVE04IK06ofwbG00ymQ032q03TN1i0bQw35E0Gq09pw6Yg0OG',
        search_payload_hsdp: str = 'ggPhf5icj4pbiO0KgG1awwwCpoaoKGCQb81lw8mq4nQ1kwyw2xo4t08uE33hWvN5eE-293Q4827wJwc-q0mm1qwQw8e0abDg1dE3pw6kw86UB08217w1la085w0JTw0Rzw0tk8',
        search_payload_hblp: str = '03280oFw36o0OK0k6U1Ko0nqw2hEnw0Jvw2BE4e0se06ro0Bq02y20eCg0OW0WE15E1xUjw70wbm0XE5q7E0juwRw',

        # --- General settings ---
        max_pages: Optional[int] = 1,
        delay_between_requests: int = 1,
        save_json: bool = False
    ):
        # Search parameters
        self.active_status = active_status
        self.ad_type = ad_type
        self.countries = countries or ['AE']
        self.search_type = search_type
        self.media_type = media_type
        self.start_date = start_date
        self.end_date = end_date
        self.query_string = query_string
        self.page_ids = page_ids or []
        self.view_all_page_id = view_all_page_id
        self.cursor = cursor
        self.first = first
        self.is_targeted_country = is_targeted_country
        
        # Session parameters
        self.session_id = session_id
        self.collation_token = collation_token or str(uuid.uuid4())
        self.lsd_token = lsd_token
        self.jazoest = jazoest
        self.cookie = cookie
        
        # Dynamic payload components
        self.search_payload_dyn = search_payload_dyn
        self.search_payload_csr = search_payload_csr
        self.search_payload_hsdp = search_payload_hsdp
        self.search_payload_hblp = search_payload_hblp

        # General settings
        self.max_pages = max_pages
        self.delay_between_requests = delay_between_requests
        self.save_json = save_json
        
        # Hardcoded headers (only user-agent and other static values)
        self.headers = {
            'authority': 'www.facebook.com',
            'accept': '*/*',
            'accept-language': 'en-US,en;q=0.9',
            'content-type': 'application/x-www-form-urlencoded',
            'origin': 'https://www.facebook.com',
            'priority': 'u=1, i',
            'referer': 'https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=US&sort_data[direction]=desc&sort_data[mode]=relevancy_monthly_grouped&search_type=page&media_type=all',
            'sec-ch-ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Brave";v="138"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'sec-gpc': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'x-asbd-id': '359341',
            'x-fb-friendly-name': 'AdLibrarySearchPaginationQuery'
        }


class FacebookAdsScraperService:
    """Service for scraping Facebook Ads Library data"""
    
    def __init__(self, db: Session):
        self.db = db
        self.enhanced_extractor = EnhancedAdExtractionService(db)

    def build_variables(self, config: FacebookAdsScraperConfig) -> str:
        """Build the variables JSON object from config"""
        variables = {
            "activeStatus": config.active_status,
            "adType": config.ad_type,
            "bylines": [],
            "collationToken": config.collation_token,
            "contentLanguages": [],
            "countries": config.countries,
            "cursor": config.cursor,
            "excludedIDs": None,
            "first": config.first,
            "isTargetedCountry": config.is_targeted_country,
            "location": None,
            "mediaType": config.media_type,
            "multiCountryFilterMode": None,
            "pageIDs": config.page_ids,
            "potentialReachInput": None,
            "publisherPlatforms": [],
            "queryString": config.query_string,
            "regions": None,
            "searchType": config.search_type,
            "sessionID": config.session_id,
            "sortData": None,
            "source": None,
            "startDate": config.start_date,
            "v": "608791",
            "viewAllPageID": config.view_all_page_id
        }
        return json.dumps(variables, separators=(',', ':'))

    def build_dynamic_payload(self, config: FacebookAdsScraperConfig) -> str:
        """Build payload with dynamic variables while keeping original structure"""
        variables_json = self.build_variables(config)
        
        payload = (
            f"av=0&__aaid=0&__user=0&__a=1&__req=a&__hs=20275.HYP%3Acomet_plat_default_pkg.2.1...0&dpr=3&"
            f"__ccg=GOOD&__rev=1024475506&__s=oeivzd%3Aa1ohcs%3A7a3kxu&__hsi=7524059447383386224&"
            f"__dyn={config.search_payload_dyn}&"
            f"__csr={config.search_payload_csr}&"
            f"__hsdp={config.search_payload_hsdp}&"
            f"__hblp={config.search_payload_hblp}&"
            f"__comet_req=94&lsd={config.lsd_token}&jazoest={config.jazoest}&__spin_r=1024475506&__spin_b=trunk&__spin_t=1751831604&"
            f"__jssesw=1&fb_api_caller_class=RelayModern&fb_api_req_friendly_name=AdLibrarySearchPaginationQuery&"
            f"variables={urllib.parse.quote(variables_json)}&server_timestamps=true&doc_id=24394279933540792"
        )
        return payload

    def build_referer_url(self, config: FacebookAdsScraperConfig) -> str:
        """Build the referer URL based on config"""
        params = {
            'active_status': config.active_status,
            'ad_type': 'all',
            'country': ','.join(config.countries),
            'is_targeted_country': str(config.is_targeted_country).lower(),
            'media_type': config.media_type,
            'search_type': config.search_type,
            'view_all_page_id': config.view_all_page_id
        }
        
        if config.query_string:
            params['q'] = config.query_string
        
        query_string = urlencode(params)
        return f"https://www.facebook.com/ads/library/?{query_string}"

    def fetch_ads_page(self, config: FacebookAdsScraperConfig) -> Optional[Dict]:
        """
        Fetch a single page of ads data using the Facebook Ads Library API
        
        Args:
            config: Scraper configuration
            
        Returns:
            Dictionary containing the response data
        """
        url = "https://www.facebook.com/api/graphql/"
        
        payload = self.build_dynamic_payload(config)
        
        headers = config.headers.copy()
        headers['referer'] = self.build_referer_url(config)
        headers['Cookie'] = config.cookie
        headers['x-fb-lsd'] = config.lsd_token
        
        try:
            response = requests.post(url, headers=headers, data=payload)
            response.raise_for_status()
            
            data = response.json()
            return data
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error: {e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            logger.error(f"Response text that failed to parse: {response.text[:1000]}")
            return None

    def scrape_ads(self, config: FacebookAdsScraperConfig) -> Tuple[List[Dict], List[Dict], Dict, Dict]:
        """
        Scrape Facebook Ads and process them with enhanced extraction
        
        Args:
            config: Scraper configuration
        
        Returns:
            Tuple of (all_ads_data, all_json_responses, enhanced_data, stats)
        """
        logger.info(f"Starting data collection for {config.view_all_page_id} using enhanced extraction")
        
        try:
            all_json_responses = []
            page_count = 0
            
            stats = {
                "total_processed": 0,
                "created": 0,
                "updated": 0,
                "errors": 0,
                'competitors_created': 0,
                'competitors_updated': 0,
                'campaigns_processed': 0
            }
            
            while True:
                page_count += 1
                if config.max_pages and page_count > config.max_pages:
                    logger.info(f"Reached max pages limit of {config.max_pages}.")
                    break

                logger.info(f"Scraping page {page_count}")
                
                response_data = self.fetch_ads_page(config)

                if not response_data:
                    logger.warning(f"No response data on page {page_count}. Stopping scrape.")
                    break
                
                all_json_responses.append(response_data)
                
                try:
                    search_results = response_data['data']['ad_library_main']['search_results_connection']
                    edges = search_results.get('edges', [])
                    page_info = search_results.get('page_info', {})

                    if not edges:
                        logger.info("No ads found on this page. Stopping scrape.")
                        break
                    
                    logger.info(f"Page {page_count}: Found {len(edges)} ad groups. Processing with enhanced extraction.")

                    enhanced_data, _ = self.enhanced_extractor.process_raw_responses([response_data])
                    
                    extraction_stats = self.enhanced_extractor.save_enhanced_ads_to_database(enhanced_data)
                    
                    for key, value in extraction_stats.items():
                        if key in stats:
                            stats[key] += value
                    
                    has_next_page = page_info.get('has_next_page', False)
                    end_cursor = page_info.get('end_cursor')

                    if not has_next_page:
                        logger.info("No more pages available. Finished.")
                        break
                    
                    config.cursor = end_cursor
                    
                    if config.delay_between_requests > 0:
                        logger.info(f"Waiting {config.delay_between_requests} seconds...")
                        time.sleep(config.delay_between_requests)

                except (KeyError, TypeError) as e:
                    logger.error(f"Error parsing response data on page {page_count}: {e}")
                    logger.error(f"Response data that caused error: {response_data}")
                    break
            
            logger.info(f"Scraping complete. Final stats: {stats}")
            return [], all_json_responses, enhanced_data, stats
        
        except Exception as e:
            logger.error(f"Error in ads scraping: {str(e)}")
            raise 


================================================
File: app/services/ingestion_service.py
================================================
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from datetime import datetime
import logging
from typing import Optional, Tuple, List, Dict, Any

from app.models import Competitor, Ad
from app.models.dto import AdCreate, AdIngestionResponse

logger = logging.getLogger(__name__)


class DataIngestionService:
    """
    Centralized service for handling ad data ingestion.
    
    This service acts as the single entry point for all scraped ad data,
    ensuring consistent processing and triggering of downstream tasks.
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    async def ingest_ad(self, ad_data: AdCreate) -> AdIngestionResponse:
        """
        Main method for ingesting ad data.
        
        This method:
        1. Validates the input data
        2. Finds or creates the competitor
        3. Creates or updates the ad record
        4. Triggers the AI analysis task
        
        Args:
            ad_data: Validated ad data from external scraper
            
        Returns:
            AdIngestionResponse with operation results
        """
        try:
            logger.info(f"Starting ad ingestion for ad_archive_id: {ad_data.ad_archive_id}")
            
            # Step 1: Find or create competitor
            competitor = await self._find_or_create_competitor(ad_data.competitor)
            logger.info(f"Using competitor: {competitor.name} (ID: {competitor.id})")
            
            # Step 2: Create or update ad record
            ad, is_new = await self._create_or_update_ad(ad_data, competitor.id)
            logger.info(f"{'Created new' if is_new else 'Updated existing'} ad: {ad.id}")
            
            # Step 3: Trigger AI analysis task
            analysis_task_id = await self._trigger_analysis_task(ad.id)
            logger.info(f"Triggered AI analysis task: {analysis_task_id}")
            
            # Step 4: Commit transaction
            self.db.commit()
            
            return AdIngestionResponse(
                success=True,
                ad_id=ad.id,
                competitor_id=competitor.id,
                analysis_task_id=analysis_task_id,
                message=f"Ad {ad_data.ad_archive_id} {'ingested' if is_new else 'updated'} successfully and analysis task dispatched"
            )
            
        except IntegrityError as e:
            self.db.rollback()
            logger.error(f"Database integrity error during ad ingestion: {e}")
            return AdIngestionResponse(
                success=False,
                message=f"Database integrity error: Duplicate ad_archive_id or constraint violation"
            )
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Unexpected error during ad ingestion: {e}")
            return AdIngestionResponse(
                success=False,
                message=f"Ingestion failed: {str(e)}"
            )
    
    async def _find_or_create_competitor(self, competitor_data) -> Competitor:
        """
        Find existing competitor or create new one.
        
        Args:
            competitor_data: CompetitorCreateDTO data
            
        Returns:
            Competitor instance
        """
        # Try to find existing competitor by page_id
        existing_competitor = self.db.query(Competitor).filter_by(
            page_id=competitor_data.page_id
        ).first()
        
        if existing_competitor:
            # Update competitor info if name has changed
            if existing_competitor.name != competitor_data.name:
                logger.info(f"Updating competitor name from '{existing_competitor.name}' to '{competitor_data.name}'")
                existing_competitor.name = competitor_data.name
                existing_competitor.updated_at = datetime.utcnow()
            
            # Reactivate if it was deactivated
            if not existing_competitor.is_active and competitor_data.is_active:
                logger.info(f"Reactivating competitor: {existing_competitor.name}")
                existing_competitor.is_active = True
                existing_competitor.updated_at = datetime.utcnow()
            
            return existing_competitor
        
        # Create new competitor
        logger.info(f"Creating new competitor: {competitor_data.name}")
        new_competitor = Competitor(
            name=competitor_data.name,
            page_id=competitor_data.page_id,
            is_active=competitor_data.is_active
        )
        
        self.db.add(new_competitor)
        self.db.flush()  # Get the ID without committing
        
        return new_competitor
    
    async def _create_or_update_ad(self, ad_data: AdCreate, competitor_id: int) -> Tuple[Ad, bool]:
        """
        Create new ad or update existing one.
        
        Args:
            ad_data: AdCreateDTO data
            competitor_id: ID of the associated competitor
            
        Returns:
            Tuple of (Ad instance, is_new_record)
        """
        # Check if ad already exists
        existing_ad = self.db.query(Ad).filter_by(
            ad_archive_id=ad_data.ad_archive_id
        ).first()
        
        # Extract enhanced data from raw data
        enhanced_data = self._extract_enhanced_data(ad_data)
        
        # Prepare ad data for database
        ad_dict = {
            "competitor_id": competitor_id,
            "ad_archive_id": ad_data.ad_archive_id,
            "ad_copy": ad_data.ad_copy,  # Keep original ad_copy as fallback
            "media_type": ad_data.media_type,
            "media_url": ad_data.media_url,
            "landing_page_url": ad_data.landing_page_url,
            "date_found": ad_data.date_found,
            "page_name": ad_data.page_name,
            "publisher_platform": ad_data.publisher_platform,
            "impressions_text": ad_data.impressions_text,
            "end_date": ad_data.end_date,
            "cta_text": ad_data.cta_text,
            "cta_type": ad_data.cta_type,
            "raw_data": ad_data.raw_data,
            # Enhanced data fields
            "targeted_countries": enhanced_data.get("targeted_countries"),
            "form_details": enhanced_data.get("form_details"),
            "running_countries": enhanced_data.get("targeted_countries"),
            "extra_texts": enhanced_data.get("extra_texts", ad_data.extra_texts),
            "main_title": enhanced_data.get("main_title"),
            "main_body_text": enhanced_data.get("main_body_text"),
            "main_caption": enhanced_data.get("main_caption"),
            "card_count": enhanced_data.get("card_count"),
            "card_bodies": enhanced_data.get("card_bodies"),
            "card_titles": enhanced_data.get("card_titles"),
            "card_cta_texts": enhanced_data.get("card_cta_texts"),
            "card_urls": enhanced_data.get("card_urls"),
            "main_image_urls": enhanced_data.get("main_image_urls"),
            "main_video_urls": enhanced_data.get("main_video_urls")
        }
        
        if existing_ad:
            # Update existing ad
            logger.info(f"Updating existing ad: {ad_data.ad_archive_id}")
            
            for key, value in ad_dict.items():
                if key != "competitor_id":  # Don't change competitor association
                    setattr(existing_ad, key, value)
            
            existing_ad.updated_at = datetime.utcnow()
            return existing_ad, False
        
        # Create new ad
        logger.info(f"Creating new ad: {ad_data.ad_archive_id}")
        new_ad = Ad(**ad_dict)
        
        self.db.add(new_ad)
        self.db.flush()  # Get the ID without committing
        
        return new_ad, True
    
    def _extract_enhanced_data(self, ad_data: AdCreate) -> Dict[str, Any]:
        """
        Extract enhanced data from raw ad data.
        
        Args:
            ad_data: AdCreateDTO with raw_data
            
        Returns:
            Dict with extracted enhanced data
        """
        enhanced = {}
        
        if not ad_data.raw_data:
            return enhanced
        
        try:
            # Extract data from snapshot if available
            snapshot = ad_data.raw_data.get('snapshot', {})
            
            # Extract main content fields
            enhanced['main_title'] = snapshot.get('title', '')
            enhanced['main_body_text'] = snapshot.get('body', {}).get('text', '')
            enhanced['main_caption'] = snapshot.get('caption', '')
            
            # Extract form details from extra_texts
            extra_texts = snapshot.get('extra_texts', [])
            enhanced['form_details'] = self._extract_form_details(extra_texts)
            
            # Also store the original extra_texts for backward compatibility
            enhanced['extra_texts'] = extra_texts
            
            # Extract targeted countries
            enhanced['targeted_countries'] = self._extract_targeted_countries(ad_data.raw_data)
            
            # Extract card data
            cards = snapshot.get('cards', [])
            if cards:
                enhanced['card_count'] = len(cards)
                enhanced['card_bodies'] = [card.get('body', '') for card in cards]
                enhanced['card_titles'] = [card.get('title', '') for card in cards]
                enhanced['card_cta_texts'] = [card.get('cta_text', '') for card in cards]
                enhanced['card_urls'] = [card.get('link_url', '') for card in cards]
            else:
                # If no separate cards but has main body, treat it as a single card
                main_body = snapshot.get('body', {}).get('text', '')
                main_title = snapshot.get('title', '')
                if main_body:
                    enhanced['card_count'] = 1
                    enhanced['card_bodies'] = [main_body]
                    enhanced['card_titles'] = [main_title] if main_title else ['']
                    enhanced['card_cta_texts'] = [snapshot.get('cta_text', '')]
                    enhanced['card_urls'] = [snapshot.get('link_url', '')]
                
            # Extract media URLs from cards or main content
            image_urls = []
            video_urls = []
            
            if cards:
                # Extract from actual cards
                for card in cards:
                    if card.get('video_hd_url'):
                        video_urls.append(card['video_hd_url'])
                    elif card.get('video_sd_url'):
                        video_urls.append(card['video_sd_url'])
                    if card.get('video_preview_image_url'):
                        image_urls.append(card['video_preview_image_url'])
                    if card.get('resized_image_url'):
                        image_urls.append(card['resized_image_url'])
            else:
                # Extract from main snapshot level
                videos = snapshot.get('videos', [])
                images = snapshot.get('images', [])
                
                for video in videos:
                    if isinstance(video, dict):
                        if video.get('video_hd_url'):
                            video_urls.append(video['video_hd_url'])
                        elif video.get('video_sd_url'):
                            video_urls.append(video['video_sd_url'])
                        if video.get('video_preview_image_url'):
                            image_urls.append(video['video_preview_image_url'])
                
                for image in images:
                    if isinstance(image, dict) and image.get('resized_image_url'):
                        image_urls.append(image['resized_image_url'])
            
            enhanced['main_video_urls'] = video_urls
            enhanced['main_image_urls'] = image_urls
            
            logger.info(f"Enhanced data extracted for ad {ad_data.ad_archive_id}: "
                       f"Countries: {enhanced.get('targeted_countries', [])}, "
                       f"Form fields: {len(enhanced.get('form_details', []))}, "
                       f"Cards: {enhanced.get('card_count', 0)}")
            
        except Exception as e:
            logger.error(f"Error extracting enhanced data: {e}")
        
        return enhanced
    
    def _extract_enhanced_data_from_raw(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract enhanced data from raw ad data (for reprocessing existing ads).
        
        Args:
            raw_data: Raw Facebook API data
            
        Returns:
            Dict with extracted enhanced data
        """
        enhanced = {}
        
        if not raw_data:
            return enhanced
        
        try:
            # Extract data from snapshot if available
            snapshot = raw_data.get('snapshot', {})
            
            # Extract main content fields
            enhanced['main_title'] = snapshot.get('title', '')
            enhanced['main_body_text'] = snapshot.get('body', {}).get('text', '')
            enhanced['main_caption'] = snapshot.get('caption', '')
            
            # Extract form details from extra_texts
            extra_texts = snapshot.get('extra_texts', [])
            logger.info(f"Reprocessing: Extracting form details from {len(extra_texts)} extra_texts")
            enhanced['form_details'] = self._extract_form_details(extra_texts)
            logger.info(f"Reprocessing: Extracted {len(enhanced.get('form_details', []))} form details")
            
            # Extract targeted countries
            enhanced['running_countries'] = self._extract_targeted_countries(raw_data)
            
            # Extract card data
            cards = snapshot.get('cards', [])
            if cards:
                enhanced['card_count'] = len(cards)
                enhanced['card_bodies'] = [card.get('body', '') for card in cards]
                enhanced['card_titles'] = [card.get('title', '') for card in cards]
                enhanced['card_cta_texts'] = [card.get('cta_text', '') for card in cards]
                enhanced['card_urls'] = [card.get('link_url', '') for card in cards]
            else:
                # If no separate cards but has main body, treat it as a single card
                main_body = snapshot.get('body', {}).get('text', '')
                main_title = snapshot.get('title', '')
                if main_body:
                    enhanced['card_count'] = 1
                    enhanced['card_bodies'] = [main_body]
                    enhanced['card_titles'] = [main_title] if main_title else ['']
                    enhanced['card_cta_texts'] = [snapshot.get('cta_text', '')]
                    enhanced['card_urls'] = [snapshot.get('link_url', '')]
            
            logger.info(f"Enhanced data extracted for reprocessing: "
                       f"Countries: {enhanced.get('running_countries', [])}, "
                       f"Form fields: {len(enhanced.get('form_details', []))}, "
                       f"Cards: {enhanced.get('card_count', 0)}")
            
        except Exception as e:
            logger.error(f"Error extracting enhanced data for reprocessing: {e}")
        
        return enhanced
    

    
    def _extract_form_details(self, extra_texts: List[Any]) -> List[str]:
        """
        Extract form details from extra_texts - treat all extra_texts as form details.
        """
        form_details = []
        
        for extra in extra_texts:
            if isinstance(extra, dict):
                text = extra.get('text', '')
            elif isinstance(extra, str):
                text = extra
            else:
                continue
                
            if text.strip():  # Only add non-empty texts
                form_details.append(text)
        
        return form_details
    
    def _extract_targeted_countries(self, raw_data: Dict[str, Any]) -> List[str]:
        """
        Extract targeted countries from various fields in raw data.
        """
        countries = []
        
        # Check targeted_or_reached_countries
        targeted_countries = raw_data.get('targeted_or_reached_countries', [])
        if targeted_countries:
            countries.extend(targeted_countries)
        
        # Check political_countries
        political_countries = raw_data.get('political_countries', [])
        if political_countries:
            countries.extend(political_countries)
        
        # Check snapshot for country info
        snapshot = raw_data.get('snapshot', {})
        country_iso = snapshot.get('country_iso_code')
        if country_iso:
            countries.append(country_iso)
        
        # Remove duplicates and empty values
        countries = list(set([c for c in countries if c]))
        
        # If no countries found, try to infer from other data
        if not countries:
            # Check if this looks like a specific region based on content
            content_text = ""
            if snapshot.get('body', {}).get('text'):
                content_text += snapshot['body']['text']
            for card in snapshot.get('cards', []):
                if card.get('body'):
                    content_text += " " + card['body']
            
            # Simple heuristics for common markets
            content_lower = content_text.lower()
            if any(keyword in content_lower for keyword in ['dubai', 'uae', 'emirates']):
                countries.append('AE')
            elif any(keyword in content_lower for keyword in ['riyadh', 'saudi', 'ksa']):
                countries.append('SA')
            elif any(keyword in content_lower for keyword in ['doha', 'qatar']):
                countries.append('QA')
        
        return countries
    
    async def _trigger_analysis_task(self, ad_id: int) -> str:
        """
        Trigger AI analysis task for the given ad.
        
        Args:
            ad_id: ID of the ad to analyze
            
        Returns:
            Task ID of the dispatched analysis task
        """
        try:
            # Import here to avoid circular import
            from app.tasks.ai_analysis_tasks import ai_analysis_task
            
            # Dispatch the AI analysis task
            task = ai_analysis_task.delay(ad_id)
            logger.info(f"AI analysis task dispatched: {task.id} for ad: {ad_id}")
            return task.id
        
        except Exception as e:
            logger.error(f"Failed to dispatch AI analysis task for ad {ad_id}: {e}")
            # Don't fail the entire ingestion if task dispatch fails
            # The ad is still saved, analysis can be triggered manually later
            return "failed-to-dispatch"
    
    async def batch_ingest_ads(self, ads_data: list[AdCreate]) -> dict:
        """
        Batch ingest multiple ads at once.
        
        Args:
            ads_data: List of AdCreateDTO objects
            
        Returns:
            Dict with batch processing results
        """
        logger.info(f"Starting batch ingestion of {len(ads_data)} ads")
        
        results = []
        successful_count = 0
        failed_count = 0
        
        for ad_data in ads_data:
            try:
                result = await self.ingest_ad(ad_data)
                results.append({
                    "ad_archive_id": ad_data.ad_archive_id,
                    "success": result.success,
                    "ad_id": result.ad_id,
                    "message": result.message
                })
                
                if result.success:
                    successful_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"Failed to ingest ad {ad_data.ad_archive_id}: {e}")
                results.append({
                    "ad_archive_id": ad_data.ad_archive_id,
                    "success": False,
                    "error": str(e)
                })
                failed_count += 1
        
        logger.info(f"Batch ingestion completed: {successful_count} successful, {failed_count} failed")
        
        return {
            "total_ads": len(ads_data),
            "successful": successful_count,
            "failed": failed_count,
            "results": results,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def get_ingestion_stats(self) -> dict:
        """
        Get statistics about ingested data.
        
        Returns:
            Dict with ingestion statistics
        """
        try:
            total_ads = self.db.query(Ad).count()
            total_competitors = self.db.query(Competitor).count()
            active_competitors = self.db.query(Competitor).filter_by(is_active=True).count()
            
            # Get recent ingestion activity (last 24 hours)
            from datetime import timedelta
            yesterday = datetime.utcnow() - timedelta(days=1)
            recent_ads = self.db.query(Ad).filter(Ad.created_at >= yesterday).count()
            
            return {
                "total_ads": total_ads,
                "total_competitors": total_competitors,
                "active_competitors": active_competitors,
                "recent_ads_24h": recent_ads,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to get ingestion stats: {e}")
            return {
                "error": "Failed to retrieve statistics",
                "timestamp": datetime.utcnow().isoformat()
            } 



================================================
File: app/tasks/__init__.py
================================================
from .basic_tasks import add_together, test_task
from .ai_analysis_tasks import ai_analysis_task, batch_ai_analysis_task
from .facebook_ads_scraper_task import scrape_facebook_ads_task, scrape_competitor_ads_task, get_facebook_ads_task_status

__all__ = [
    "add_together", 
    "test_task", 
    "ai_analysis_task", 
    "batch_ai_analysis_task",
    "scrape_facebook_ads_task",
    "scrape_competitor_ads_task", 
    "get_facebook_ads_task_status"
] 


================================================
File: app/tasks/ai_analysis_tasks.py
================================================
from celery import shared_task
from datetime import datetime
import logging
from typing import Dict, Any
from app.services.ai_service import get_ai_service

logger = logging.getLogger(__name__)

@shared_task(bind=True)
def ai_analysis_task(self, ad_id: int) -> Dict[str, Any]:
    """
    Placeholder AI analysis task that will be triggered after ad ingestion.
    This task will analyze the ad content and generate insights.
    
    Args:
        ad_id: The ID of the ad to analyze
        
    Returns:
        Dict with analysis results
    """
    task_id = self.request.id
    logger.info(f"Starting AI analysis for ad {ad_id} (task: {task_id})")
    
    try:
        # Import here to avoid circular imports
        from app.database import SessionLocal
        from app.models import Ad, AdAnalysis
        
        db = SessionLocal()
        
        # Get the ad record
        ad = db.query(Ad).filter(Ad.id == ad_id).first()
        if not ad:
            raise ValueError(f"Ad with ID {ad_id} not found")
        
        logger.info(f"Analyzing ad: {ad.ad_archive_id} from competitor: {ad.competitor.name}")
        
        # Get AI service instance
        ai_service = get_ai_service()
        
        if not ai_service:
            logger.warning("AI service not configured, using fallback analysis")
            analysis_data = {
                "summary": f"AI analysis not available - fallback analysis for ad {ad.ad_archive_id}",
                "hook_score": 5.0,
                "overall_score": 5.0,
                "target_audience": "General audience",
                "content_themes": [],
                "analysis_version": "v1.0-fallback",
                "confidence_score": 0.0,
                "ai_prompts": {"note": "AI service not configured"},
                "raw_ai_response": {"note": "AI service not configured"},
                "performance_predictions": {},
                "competitor_insights": {},
                "ad_format_analysis": {},
                "effectiveness_analysis": {}
            }
        else:
            # Prepare ad data for AI analysis
            ad_data = {
                "ad_archive_id": ad.ad_archive_id,
                "ad_copy": ad.ad_copy,
                "main_title": ad.main_title,
                "main_body_text": ad.main_body_text,
                "main_caption": ad.main_caption,
                "cta_text": ad.cta_text,
                "media_type": ad.media_type,
                "page_name": ad.page_name,
                "targeted_countries": ad.targeted_countries,
                "card_titles": ad.card_titles,
                "card_bodies": ad.card_bodies
            }
            
            # Call AI service for analysis
            try:
                analysis_data = ai_service.analyze_ad_content(ad_data)
                logger.info(f"AI analysis completed for ad {ad.ad_archive_id}")
            except Exception as ai_error:
                logger.error(f"AI analysis failed for ad {ad.ad_archive_id}: {str(ai_error)}")
                # Use fallback analysis on AI failure
                analysis_data = {
                    "summary": f"AI analysis failed - fallback analysis for ad {ad.ad_archive_id}",
                    "hook_score": 5.0,
                    "overall_score": 5.0,
                    "target_audience": "General audience",
                    "content_themes": [],
                    "analysis_version": "v1.0-fallback",
                    "confidence_score": 0.0,
                    "ai_prompts": {"error": str(ai_error)},
                    "raw_ai_response": {"error": str(ai_error)},
                    "performance_predictions": {},
                    "competitor_insights": {},
                    "ad_format_analysis": {},
                    "effectiveness_analysis": {}
                }
        
        # Check if analysis already exists
        existing_analysis = db.query(AdAnalysis).filter(AdAnalysis.ad_id == ad_id).first()
        
        if existing_analysis:
            # Update existing analysis
            for key, value in analysis_data.items():
                setattr(existing_analysis, key, value)
            existing_analysis.updated_at = datetime.utcnow()
            analysis = existing_analysis
            logger.info(f"Updated existing analysis for ad {ad_id}")
        else:
            # Create new analysis
            analysis = AdAnalysis(
                ad_id=ad_id,
                **analysis_data
            )
            db.add(analysis)
            logger.info(f"Created new analysis for ad {ad_id}")
        
        db.commit()
        
        result = {
            "task_id": task_id,
            "ad_id": ad_id,
            "analysis_id": analysis.id,
            "status": "completed",
            "summary": analysis_data["summary"],
            "overall_score": analysis_data["overall_score"],
            "hook_score": analysis_data["hook_score"],
            "timestamp": datetime.utcnow().isoformat()
        }
        
        logger.info(f"AI analysis completed for ad {ad_id}")
        return result
        
    except Exception as exc:
        logger.error(f"AI analysis failed for ad {ad_id}: {str(exc)}")
        # Update task state with error info
        self.update_state(
            state='FAILURE',
            meta={
                'error': str(exc),
                'ad_id': ad_id,
                'timestamp': datetime.utcnow().isoformat()
            }
        )
        raise exc
    finally:
        if 'db' in locals():
            db.close()

@shared_task
def batch_ai_analysis_task(ad_ids: list) -> Dict[str, Any]:
    """
    Batch AI analysis task for processing multiple ads at once.
    
    Args:
        ad_ids: List of ad IDs to analyze
        
    Returns:
        Dict with batch analysis results
    """
    logger.info(f"Starting batch AI analysis for {len(ad_ids)} ads")
    
    results = []
    failed_ads = []
    
    for ad_id in ad_ids:
        try:
            # Dispatch individual analysis task
            task = ai_analysis_task.delay(ad_id)
            results.append({
                "ad_id": ad_id,
                "task_id": task.id,
                "status": "dispatched"
            })
        except Exception as e:
            logger.error(f"Failed to dispatch analysis for ad {ad_id}: {e}")
            failed_ads.append({
                "ad_id": ad_id,
                "error": str(e)
            })
    
    return {
        "total_ads": len(ad_ids),
        "successful_dispatches": len(results),
        "failed_dispatches": len(failed_ads),
        "results": results,
        "failures": failed_ads,
        "timestamp": datetime.utcnow().isoformat()
    } 


================================================
File: app/tasks/basic_tasks.py
================================================
from celery import shared_task
from datetime import datetime
import time
import logging

logger = logging.getLogger(__name__)

@shared_task
def add_together(x: int, y: int) -> dict:
    """
    Simple test task that adds two numbers together.
    This is the test task requested in the prompt.
    """
    logger.info(f"Adding {x} + {y}")
    result = x + y
    
    return {
        "task_name": "add_together",
        "inputs": {"x": x, "y": y},
        "result": result,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }

@shared_task(bind=True)
def test_task(self, message: str = "Hello from Celery!") -> dict:
    """
    Another simple test task to verify Celery is working properly.
    """
    task_id = self.request.id
    logger.info(f"Test task {task_id} started with message: {message}")
    
    # Simulate some work
    time.sleep(2)
    
    result = {
        "task_id": task_id,
        "message": message,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }
    
    logger.info(f"Test task {task_id} completed")
    return result

@shared_task(bind=True)
def long_running_task(self, duration: int = 10) -> dict:
    """
    A task that simulates long-running work.
    Useful for testing task monitoring and cancellation.
    """
    task_id = self.request.id
    logger.info(f"Long running task {task_id} started for {duration} seconds")
    
    for i in range(duration):
        time.sleep(1)
        # Update task state
        self.update_state(
            state='PROGRESS',
            meta={
                'current': i + 1,
                'total': duration,
                'status': f'Processing... {i + 1}/{duration}'
            }
        )
    
    result = {
        "task_id": task_id,
        "duration": duration,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }
    
    logger.info(f"Long running task {task_id} completed")
    return result 


================================================
File: app/tasks/facebook_ads_scraper_task.py
================================================
from celery import current_task
from sqlalchemy.orm import Session
from datetime import datetime
import logging
from typing import Dict, List, Optional
import json

from app.celery_worker import celery_app
from app.database import get_db
from app.services.facebook_ads_scraper import FacebookAdsScraperService, FacebookAdsScraperConfig

logger = logging.getLogger(__name__)


@celery_app.task(bind=True, name="facebook_ads_scraper.scrape_ads")
def scrape_facebook_ads_task(
    self,
    config: Dict = None,
    view_all_page_id: str = None,
    countries: List[str] = None,
    max_pages: int = 10,
    delay_between_requests: int = 1
):
    """
    Celery task for scraping Facebook Ads Library data
    
    Args:
        config: Optional dictionary with scraper configuration
        view_all_page_id: Page ID to scrape ads from
        countries: List of country codes to search in
        max_pages: Maximum number of pages to scrape
        delay_between_requests: Delay between requests in seconds
    """
    task_id = current_task.request.id
    logger.info(f"Starting Facebook Ads scraping task: {task_id}")

    # Get database session
    db = next(get_db())

    try:
        # Create scraper service
        scraper = FacebookAdsScraperService(db)
        # Create configuration
        scraper_config = FacebookAdsScraperConfig(
            view_all_page_id=view_all_page_id or (config.get('view_all_page_id') if config else None),
            countries=countries or (config.get('countries') if config else None),
            max_pages=max_pages,
            delay_between_requests=delay_between_requests
        )
        # Update config with any additional parameters
        if config:
            for key, value in config.items():
                if hasattr(scraper_config, key) and key not in ['view_all_page_id', 'countries', 'max_pages', 'delay_between_requests']:
                    setattr(scraper_config, key, value)
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper.scrape_ads(scraper_config)
        # Save to database if requested
        if scraper_config.save_json and all_json_responses:
            # Save raw JSON responses to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_ads_{scraper_config.view_all_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")
        
        # Return stats
        return {
            "task_id": task_id,
            "stats": stats,
            "config": {
                "view_all_page_id": scraper_config.view_all_page_id,
                "countries": scraper_config.countries,
                "max_pages": scraper_config.max_pages
            },
            "enhanced_data_summary": {
                "advertiser_info": enhanced_data.get("advertiser_info", {}),
                "campaigns_count": len(enhanced_data.get("campaigns", [])),
                "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
            }
        }
    except Exception as e:
        logger.error(f"Error in Facebook Ads scraping task: {str(e)}")
        raise
    finally:
        db.close()


@celery_app.task(bind=True, name="facebook_ads_scraper.scrape_competitor_ads")
def scrape_competitor_ads_task(
    self,
    competitor_page_id: str,
    countries: List[str] = None,
    max_pages: int = 5,
    delay_between_requests: int = 1,
    active_status: str = "ALL",
    ad_type: str = "ALL",
    media_type: str = "ALL",
    save_json: bool = False
):
    """
    Celery task for scraping ads from a specific competitor page
    
    Args:
        competitor_page_id: Facebook page ID of the competitor
        countries: List of country codes to search in
        max_pages: Maximum number of pages to scrape
        delay_between_requests: Delay between requests in seconds
        active_status: Filter by ad status (ALL, ACTIVE, INACTIVE)
        ad_type: Filter by ad type (ALL, POLITICAL_AND_ISSUE_ADS, etc.)
        media_type: Filter by media type (ALL, VIDEO, IMAGE, etc.)
        save_json: Whether to save raw JSON responses to file
    """
    task_id = self.request.id
    logger.info(f"Starting competitor ads scraping task: {task_id} for page ID: {competitor_page_id}")
    
    # Get database session
    db = next(get_db())
    
    try:
        # Check if competitor exists in database
        from app.models.competitor import Competitor
        competitor = db.query(Competitor).filter_by(page_id=competitor_page_id).first()
        if not competitor:
            logger.warning(f"Competitor with page_id {competitor_page_id} not found, skipping scraping")
            return {
                'success': False,
                'warning': f"Competitor with page_id {competitor_page_id} not found, skipping scraping",
                'competitor_page_id': competitor_page_id,
                'task_id': task_id,
                'completion_time': datetime.utcnow().isoformat()
            }
        
        # Create scraper service
        scraper_service = FacebookAdsScraperService(db)
        
        # Create scraper configuration for specific competitor
        scraper_config = FacebookAdsScraperConfig(
            view_all_page_id=competitor_page_id,
            countries=countries or ['AE'],
            max_pages=max_pages,
            delay_between_requests=delay_between_requests,
            search_type='page',  # Ensure we're searching by page
            active_status=active_status,
            ad_type=ad_type,
            media_type=media_type,
            save_json=save_json
            )
            
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper_service.scrape_ads(scraper_config)
            
        # Save raw JSON responses to file if requested
        if save_json and all_json_responses:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_ads_competitor_{competitor_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")

        # Prepare final results
        results = {
            'success': True,
            'competitor_page_id': competitor_page_id,
            'total_ads_scraped': stats.get('total_processed', 0),
            'database_stats': stats,
            'completion_time': datetime.utcnow().isoformat(),
            'task_id': task_id,
            'enhanced_data_summary': {
                "advertiser_info": enhanced_data.get("advertiser_info", {}),
                "campaigns_count": len(enhanced_data.get("campaigns", [])),
                "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
            }
        }
        logger.info(f"Competitor ads scraping task completed successfully. Results: {results}")
        return results
    except Exception as e:
        logger.error(f"Error in competitor ads scraping task: {str(e)}")
        db.rollback()
        return {
            'success': False,
            'error': str(e),
            'competitor_page_id': competitor_page_id,
            'task_id': task_id,
            'completion_time': datetime.utcnow().isoformat()
        }
    finally:
        db.commit()
        db.close()


@celery_app.task(bind=True, name="facebook_ads_scraper.get_task_status")
def get_facebook_ads_task_status(self, task_id: str):
    """
    Get the status of a Facebook Ads scraping task
    
    Args:
        task_id: ID of the task to check
    """
    try:
        # Get task result
        task_result = celery_app.AsyncResult(task_id)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'status': 'Task is waiting to be processed'
            }
        elif task_result.state == 'PROGRESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'progress': task_result.info
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'result': task_result.result
            }
        else:
            # Task failed
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'error': str(task_result.info)
            }
        
        return response
        
    except Exception as e:
        logger.error(f"Error getting task status: {str(e)}")
        return {
            'task_id': task_id,
            'state': 'ERROR',
            'error': f'Failed to get task status: {str(e)}'
        } 


