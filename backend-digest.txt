Directory structure:
└── backend/
    ├── Dockerfile
    ├── alembic.ini
    ├── celerybeat-schedule
    ├── drop_all_ads.py
    ├── group_ads.py
    ├── image_comparator_streamed.py
    ├── requirements.txt
    ├── test_ad_grouping.py
    ├── test_creative_comparison.py
    ├── video_comparator_fast.py
    ├── alembic/
    │   ├── README
    │   ├── env.py
    │   ├── script.py.mako
    │   └── versions/
    │       ├── 014e83d13d0a_create_campaign_model_and_link_to_ad.py
    │       ├── 0a968eddd8d7_enhanced_extraction_fields_and_cleanup.py
    │       ├── 31ab1628a920_add_page_url_to_competitor.py
    │       ├── 4b419cac5311_initial_migration_create_competitors_.py
    │       ├── 568f9fde44b0_add_enhanced_extraction_fields_and_.py
    │       ├── 5a527db67d43_enhanced_extraction_fields_and_cleanup.py
    │       ├── 6168e4625f40_add_enhanced_extraction_fields_and_.py
    │       ├── 779734a0c91c_add_comprehensive_facebook_ads_data_.py
    │       ├── 8965d492a259_update_adset_content_signature_to_.py
    │       ├── 8a69c9ce3517_add_duration_days_to_ads.py
    │       ├── a220f98e38c2_add_form_details_running_countries_.py
    │       ├── a2feb1e20f39_add_meta_targeting_lead_form_and_.py
    │       ├── a7031f8c7467_add_enhanced_extraction_fields_and_keep_.py
    │       ├── b1c2d3e4f5g6_add_date_range_to_ad_sets.py
    │       ├── d26d6ee785d3_add_enhanced_extraction_fields_hybrid_.py
    │       ├── df763544010c_update_adset_content_signature_to_.py
    │       ├── e9be095c41d0_create_ad_sets_table_and_link_to_ads.py
    │       ├── ea1c3a5b9c21_enable_pg_trgm_and_index_ad_sets.py
    │       ├── f2b75a2c1596_add_is_active_field_to_adset.py
    │       └── f3e2b1c4a5d6_add_ad_set_id_to_ads.py
    └── app/
        ├── __init__.py
        ├── celery.py
        ├── celery_worker.py
        ├── database.py
        ├── main.py
        ├── __pycache__/
        ├── api/
        │   ├── __init__.py
        │   └── internal_api.py
        ├── core/
        │   ├── config.py
        │   └── __pycache__/
        ├── models/
        │   ├── __init__.py
        │   ├── ad.py
        │   ├── ad_analysis.py
        │   ├── ad_set.py
        │   ├── competitor.py
        │   ├── task_status.py
        │   ├── __pycache__/
        │   └── dto/
        │       ├── __init__.py
        │       ├── ad_dto.py
        │       ├── competitor_dto.py
        │       └── __pycache__/
        ├── routers/
        │   ├── __init__.py
        │   ├── ads.py
        │   ├── competitors.py
        │   └── health.py
        ├── services/
        │   ├── __init__.py
        │   ├── ad_service.py
        │   ├── ai_service.py
        │   ├── competitor_service.py
        │   ├── creative_comparison_service.py
        │   ├── enhanced_ad_extraction.py
        │   ├── facebook_ads_scraper.py
        │   ├── ingestion_service.py
        │   └── __pycache__/
        └── tasks/
            ├── __init__.py
            ├── ai_analysis_tasks.py
            ├── basic_tasks.py
            ├── facebook_ads_scraper_task.py
            └── __pycache__/

================================================
File: Dockerfile
================================================
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV PYTHONPATH /app

# Set work directory
WORKDIR /app

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        postgresql-client \
        build-essential \
        libpq-dev \
        curl \
        # Dependencies for PyAV (video processing)
        ffmpeg \
        libavformat-dev \
        libavcodec-dev \
        libavdevice-dev \
        libavutil-dev \
        libswscale-dev \
        libswresample-dev \
        libavfilter-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir -r requirements.txt

# Copy project
COPY . .

# Create uploads directory
RUN mkdir -p uploads

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"] 


================================================
File: alembic.ini
================================================
# A generic, single database configuration.

[alembic]
# path to migration scripts
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the
# "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# sqlalchemy.url = driver://user:pass@localhost/dbname
# The URL will be set programmatically in env.py using environment variables


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S



================================================
File: celerybeat-schedule
================================================
[Non-text file]


================================================
File: drop_all_ads.py
================================================
import os
from sqlalchemy import create_engine, update
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv

from app.models.ad import Ad
from app.models.ad_analysis import AdAnalysis
from app.models.ad_set import AdSet

# Load environment variables from .env file
load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@localhost:5432/ads_db")

# When running this script from the host machine, the Docker container's hostname 'db'
# is not resolvable. We replace it with 'localhost' to allow direct connection
# to the database, which should be exposed by Docker.
if 'db:5432' in DATABASE_URL:
    DATABASE_URL = DATABASE_URL.replace('db:5432', 'localhost:5432')
    print("Adjusted DATABASE_URL for host execution.")

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def drop_all_ads():
    """
    Deletes all records from the 'ads', 'ad_analyses', and 'ad_sets' tables.
    """
    db = SessionLocal()
    try:
        # First, update ad_sets to set best_ad_id to NULL
        db.query(AdSet).update({AdSet.best_ad_id: None}, synchronize_session=False)
        db.flush()
        
        # Second, update ads to set ad_set_id to NULL
        db.query(Ad).update({Ad.ad_set_id: None}, synchronize_session=False)
        db.flush()
        
        # Get count of ads for reporting
        num_ads = db.query(Ad).count()
        
        # Get count of ad sets for reporting
        num_ad_sets = db.query(AdSet).count()
        
        # Now we can delete ad_sets since nothing references them
        db.query(AdSet).delete(synchronize_session=False)
        
        # Then delete all ads (which will cascade delete analyses)
        db.query(Ad).delete(synchronize_session=False)
        
        db.commit()

        print(f"Successfully deleted {num_ads} ad(s) and {num_ad_sets} ad set(s).")
        
    except Exception as e:
        db.rollback()
        print(f"An error occurred: {e}")
    finally:
        db.close()

if __name__ == "__main__":
    print("This script will delete all ads, ad sets, and their analyses from the database.")
    choice = input("Are you sure you want to continue? (y/n): ")
    if choice.lower() == 'y':
        drop_all_ads()
    else:
        print("Operation cancelled.")



================================================
File: group_ads.py
================================================
"""
Ad Grouping Orchestrator (High-Performance Version)
===================================================

This script takes a JSON file of ads and groups them into "ad sets" based on
the visual similarity of their media content.

It uses a highly efficient, parallel pre-computation strategy:
1.  Collect all unique media URLs (images and videos) that need to be compared.
2.  Process this entire workload in parallel, using a thread pool to overlap
    all network I/O and hashing operations.
3.  Use the pre-computed cache of hashes to perform the final grouping, which
    becomes computationally trivial.
"""

from __future__ import annotations

import json
import time
from typing import List, Dict, Any, Set, Tuple, Union
import concurrent.futures
from tqdm import tqdm

# --- Import your existing comparison functions ---
try:
    # We need the low-level hashing functions directly
    from image_comparator_streamed import _download_and_hash as get_image_hash
    from video_comparator_fast import _sample_hashes as get_video_hashes
except ImportError as e:
    print(f"Error: Could not import helper functions. Ensure '_download_and_hash' from 'image_comparator_streamed.py' and '_sample_hashes' from 'video_comparator_fast.py' are accessible. Details: {e}")
    exit(1)

# --- Configuration ---
IMAGE_HASH_CUTOFF = 5
VIDEO_SIMILARITY_THRESHOLD = 0.90
VIDEO_SAMPLES = 6
MAX_WORKERS = 16  # Number of parallel downloads. Increase if your connection is fast.


# This cache will be populated by the parallel pre-computation phase.
# It will store URL -> (hash or list of hashes).
MEDIA_HASH_CACHE: Dict[str, Any] = {}

def process_media_item(url: str, media_type: str) -> Tuple[str, Any]:
    """Worker function to process a single URL. Returns the URL and its hash."""
    if media_type == "image":
        try:
            return url, get_image_hash(url)
        except Exception:
            return url, None
    elif media_type == "video":
        try:
            # Note: _sample_hashes returns a list of hashes
            return url, get_video_hashes(url, samples=VIDEO_SAMPLES)
        except Exception:
            return url, None
    return url, None


def are_ads_similar_cached(ad1: Dict[str, Any], ad2: Dict[str, Any]) -> bool:
    """
    Compares two ads using the pre-computed hash cache. This is extremely fast.
    """
    if ad1.get("media_type") != ad2.get("media_type"):
        return False

    media_type = ad1.get("media_type")

    if media_type == "Image":
        url1, url2 = ad1.get("media_url"), ad2.get("media_url")
        if not url1 or not url2: return False
        if url1 == url2: return True

        hash1 = MEDIA_HASH_CACHE.get(url1)
        hash2 = MEDIA_HASH_CACHE.get(url2)
        if hash1 and hash2:
            return (hash1 - hash2) <= IMAGE_HASH_CUTOFF
        return False

    elif media_type == "Video":
        # 1. High-quality video URL match
        if ad1.get("media_url") and ad1.get("media_url") == ad2.get("media_url"):
            return True

        # 2. Thumbnail comparison
        thumb1_urls, thumb2_urls = ad1.get("main_image_urls", []), ad2.get("main_image_urls", [])
        if thumb1_urls and thumb2_urls:
            thumb1, thumb2 = thumb1_urls[0], thumb2_urls[0]
            if thumb1 == thumb2: return True
            
            hash1 = MEDIA_HASH_CACHE.get(thumb1)
            hash2 = MEDIA_HASH_CACHE.get(thumb2)
            if hash1 and hash2 and (hash1 - hash2) <= IMAGE_HASH_CUTOFF:
                return True

        # 3. Low-quality video stream comparison
        vid1_urls, vid2_urls = ad1.get("main_video_urls", []), ad2.get("main_video_urls", [])
        if len(vid1_urls) > 1 and len(vid2_urls) > 1:
            vid1, vid2 = vid1_urls[1], vid2_urls[1]
            if vid1 == vid2: return True

            hashes1 = MEDIA_HASH_CACHE.get(vid1)
            hashes2 = MEDIA_HASH_CACHE.get(vid2)

            if hashes1 and hashes2:
                common = min(len(hashes1), len(hashes2))
                if common == 0: return False
                matches = sum(1 for h1, h2 in zip(hashes1, hashes2) if (h1 - h2) <= IMAGE_HASH_CUTOFF)
                score = matches / common
                return score >= VIDEO_SIMILARITY_THRESHOLD

    return False


def group_ads_fast(ads: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """Groups ads using the pre-computed cache."""
    ad_sets: List[List[Dict[str, Any]]] = []
    assigned_ad_ids: Set[int] = set()

    print("\n--- Phase 3: Grouping ads using cached hashes ---")
    for ad1 in tqdm(ads, desc="Grouping Ads"):
        ad1_id = ad1['id']
        if ad1_id in assigned_ad_ids:
            continue
        
        new_set = [ad1]
        assigned_ad_ids.add(ad1_id)

        for ad2 in ads:
            ad2_id = ad2['id']
            if ad2_id in assigned_ad_ids or ad1_id == ad2_id:
                continue

            if are_ads_similar_cached(ad1, ad2):
                new_set.append(ad2)
                assigned_ad_ids.add(ad2_id)
        
        ad_sets.append(new_set)

    return ad_sets


if __name__ == "__main__":
    start_time = time.perf_counter()

    try:
        with open("ads.json", "r", encoding="utf-8") as f:
            all_ads_data = json.load(f)
        ads_list = all_ads_data.get("data", [])
        print(f"Loaded {len(ads_list)} ads from 'ads.json'.")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error: Could not load or parse 'ads.json'. Please check the file. Details: {e}")
        exit(1)

    if not ads_list:
        print("No ads found in the JSON file.")
        exit(0)

    # --- Phase 1: Collect all unique media URLs to process ---
    print("\n--- Phase 1: Collecting unique media URLs to process ---")
    urls_to_process: Set[Tuple[str, str]] = set()
    for ad in ads_list:
        media_type = ad.get("media_type")
        if media_type == "Image":
            if ad.get("media_url"):
                urls_to_process.add((ad["media_url"], "image"))
        elif media_type == "Video":
            # Add thumbnail URL
            if ad.get("main_image_urls"):
                urls_to_process.add((ad["main_image_urls"][0], "image"))
            # Add low-quality video URL
            if ad.get("main_video_urls") and len(ad["main_video_urls"]) > 1:
                urls_to_process.add((ad["main_video_urls"][1], "video"))
    
    print(f"Found {len(urls_to_process)} unique media items to hash.")

    # --- Phase 2: Process all unique URLs in parallel ---
    print(f"\n--- Phase 2: Hashing all media in parallel (using up to {MAX_WORKERS} workers) ---")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Use a map to submit all jobs and wrap with tqdm for a progress bar
        future_to_url = {executor.submit(process_media_item, url, m_type): (url, m_type) for url, m_type in urls_to_process}
        
        for future in tqdm(concurrent.futures.as_completed(future_to_url), total=len(urls_to_process), desc="Hashing Media"):
            url, result = future.result()
            if result:
                MEDIA_HASH_CACHE[url] = result
            else:
                original_url, original_type = future_to_url[future]
                print(f"\n[Warning] Failed to process {original_type} URL: {original_url[:80]}...")

    print(f"Successfully hashed {len(MEDIA_HASH_CACHE)} of {len(urls_to_process)} items.")

    # --- Phase 3: Group ads using the now-complete cache ---
    final_ad_sets = group_ads_fast(ads_list)
    
    # --- Print the results ---
    print("\n\n--- Grouping Complete ---")
    print(f"Found {len(final_ad_sets)} unique ad sets.")
    print("-" * 25)

    for i, ad_set in enumerate(final_ad_sets):
        representative_ad = ad_set[0]
        ad_ids_in_set = sorted([ad['id'] for ad in ad_set])
        print(f"Ad Set {i+1} ({len(ad_set)} ads, type: {representative_ad.get('media_type', 'N/A')})")
        print(f"  - Representative Ad ID: {representative_ad['id']}")
        print(f"  - All Ad IDs in this set: {ad_ids_in_set}")
        print(f"  - Media URL: {representative_ad.get('media_url', 'N/A')[:80]}...")
        print("-" * 15)

    end_time = time.perf_counter()
    print(f"\nTotal time taken: {end_time - start_time:.2f} seconds.")


================================================
File: image_comparator_streamed.py
================================================
# To run this script, you may need to install the following libraries:
# pip install requests Pillow imagehash

import requests
from PIL import Image
import imagehash
import concurrent.futures
import time

def _download_and_hash(url: str):
    """Download an image via streaming and return its perceptual hash."""
    try:
        resp = requests.get(url, stream=True, timeout=10)
        resp.raise_for_status()
        # Read raw bytes in chunks then hash (still efficient for one image)
        img = Image.open(resp.raw)  # type: ignore[arg-type]
        return imagehash.average_hash(img)  # average_hash is faster than phash
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"Failed to process {url[:60]}â€¦ â€“ {exc}") from exc


def are_images_same(image_url1, image_url2, cutoff: int = 5):
    """Return True/False if two remote images are similar, or None on error.

    The two downloads + hash computations run in parallel to minimise wall time.
    Using average_hash (8Ã—8) for speed; tweak hash function or size if needed.
    """

    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        fut1 = executor.submit(_download_and_hash, image_url1)
        fut2 = executor.submit(_download_and_hash, image_url2)
        try:
            hash1 = fut1.result()
            hash2 = fut2.result()
        except RuntimeError as err:
            print(err)
            return None

    diff = hash1 - hash2
    print(f"Hash difference between images is: {diff}")
    return diff <= cutoff

if __name__ == "__main__":
    # Using the updated URLs from your last change
    url1 = "https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.35426-6/497848029_3948913998658947_7351662239018085718_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=nxQnOMee6oIQ7kNvwFAx7sU&_nc_oc=AdlIJKDedVpaNBSDhXS2dVZD94AR25DE8oQjZxis_mkF1P8kNePAGuMBLaCSql3Ipv0&_nc_zt=14&_nc_ht=scontent.fdxb2-1.fna&_nc_gid=A-0ybV9RG211oqoD8LV0Ng&oh=00_AfTlW674YCIy4_4JhXlSp6W6GtOaJmaQkSx73_6rI501Lw&oe=6872E288"
    url2 = "https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.35426-6/501270508_1047968767280818_2943739564029986859_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=xCL4PLf_bW4Q7kNvwFC--Bs&_nc_oc=Adkn1EPamPrpTCKoWtm5Uc-uR_kcpeRsSp5CdAtYcgWorMhauURqMGyrACx3hOC3w1A&_nc_zt=14&_nc_ht=scontent.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfTMulWaurNs0WEVyZY-W1Vcc4VxIs1JBAG7p8rNFqDPkA&oe=687439F6"

    print("--- Comparing two specific images using streaming ---")
    start = time.perf_counter()
    result = are_images_same(url1, url2)
    elapsed = time.perf_counter() - start
    if result is True:
        print("\nConclusion: The two images are the SAME.")
    elif result is False:
        print("\nConclusion: The two images are DIFFERENT.")
    else:
        print("\nConclusion: Could not compare images due to an error.")

    print(f"\nTime taken: {elapsed:.2f} seconds") 


================================================
File: requirements.txt
================================================
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
alembic==1.13.1
psycopg2-binary==2.9.9
pydantic==2.5.0
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6
celery==5.3.4
redis==5.0.1
httpx==0.25.2
python-dotenv==1.0.0
pytest==7.4.3
pytest-asyncio==0.23.2
requests==2.31.0
pandas==2.1.4
numpy==1.26.2
Pillow==10.1.0
python-facebook-api==0.18.0
aiofiles==23.2.1
jinja2==3.1.2
email-validator==2.1.0
google-generativeai==0.3.2
av==11.0.0
imagehash==4.3.1 


================================================
File: test_ad_grouping.py
================================================
#!/usr/bin/env python3
"""
Test script for the new ad grouping logic.
This demonstrates how the direct comparison approach works for 
placing new ads into existing AdSets.
"""

import sys
import logging
import json
import uuid
from datetime import datetime
from sqlalchemy.orm import Session
from app.database import get_db
from app.models import Ad, AdSet, Competitor
from app.services.enhanced_ad_extraction import EnhancedAdExtractionService

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("test_ad_grouping")

# Sample ad data for testing
def create_sample_ad(ad_id, title, body_text, image_url=None, video_url=None):
    """Create a sample ad with the given parameters"""
    
    # Determine media type
    media_type = "unknown"
    if image_url and not video_url:
        media_type = "image"
    elif video_url and not image_url:
        media_type = "video"
    elif image_url and video_url:
        media_type = "carousel"
        
    # Create sample ad data
    ad_data = {
        "ad_archive_id": ad_id,
        "media_type": media_type,
        "meta": {
            "page_name": "Test Advertiser",
            "page_id": "123456789",
            "start_date": "2023-01-01",
            "end_date": None,
            "is_active": True
        },
        "creatives": [
            {
                "id": f"{ad_id}-0",
                "title": title,
                "body": body_text,
                "media": []
            }
        ]
    }
    
    # Add media URLs if provided
    if image_url:
        ad_data["main_image_urls"] = [image_url]
        ad_data["media_url"] = image_url
        ad_data["creatives"][0]["media"].append({
            "type": "image",
            "url": image_url
        })
        
    if video_url:
        ad_data["main_video_urls"] = [video_url]
        if not image_url:  # Only set media_url to video if no image
            ad_data["media_url"] = video_url
        ad_data["creatives"][0]["media"].append({
            "type": "video",
            "url": video_url
        })
    
    return ad_data

def main():
    """Run the ad grouping test"""
    # Connect to database
    db = next(get_db())
    
    try:
        # Create service
        extraction_service = EnhancedAdExtractionService(db)
        
        # First, find or create a test competitor
        competitor = db.query(Competitor).filter_by(name="Test Advertiser").first()
        if not competitor:
            competitor = Competitor(
                name="Test Advertiser",
                page_id="123456789",
                is_active=True,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            db.add(competitor)
            db.flush()
            logger.info(f"Created test competitor: {competitor.name} (ID: {competitor.id})")
        
        # Generate unique IDs for our test ads to avoid conflicts
        test_id_prefix = f"test_{uuid.uuid4().hex[:8]}"
        
        # Create sample ads for testing
        ad1 = create_sample_ad(
            f"{test_id_prefix}_ad_1", 
            "First Ad Title", 
            "This is a test ad with some sample text.",
            image_url="https://example.com/test_image1.jpg"
        )
        
        # Similar ad with nearly identical text and same image
        ad2 = create_sample_ad(
            f"{test_id_prefix}_ad_2", 
            "First Ad Title", 
            "This is a test ad with some sample text!",  # Small difference
            image_url="https://example.com/test_image1.jpg"  # Same image
        )
        
        # Different ad with different text and image
        ad3 = create_sample_ad(
            f"{test_id_prefix}_ad_3", 
            "Completely Different Title", 
            "This ad has different content and should create a new group.",
            image_url="https://example.com/test_image2.jpg"  # Different image
        )
        
        # Step 1: Create the first ad (should create a new AdSet)
        logger.info("\n\n=== STEP 1: Creating first ad ===")
        ad1_obj, is_new1 = extraction_service._create_or_update_enhanced_ad(
            ad1, competitor.id, "test_campaign", ["facebook"], {"name": competitor.name}
        )
        
        if ad1_obj:
            logger.info(f"Ad 1 created: {ad1_obj.id}, is_new: {is_new1}, ad_set_id: {ad1_obj.ad_set_id}")
            
            # Step 2: Create similar ad (should be grouped with the first ad)
            logger.info("\n\n=== STEP 2: Creating similar ad ===")
            ad2_obj, is_new2 = extraction_service._create_or_update_enhanced_ad(
                ad2, competitor.id, "test_campaign", ["facebook"], {"name": competitor.name}
            )
            
            if ad2_obj:
                logger.info(f"Ad 2 created: {ad2_obj.id}, is_new: {is_new2}, ad_set_id: {ad2_obj.ad_set_id}")
                
                # Check if they're in the same AdSet
                if ad1_obj.ad_set_id == ad2_obj.ad_set_id:
                    logger.info("SUCCESS: Similar ads were grouped together!")
                else:
                    logger.warning("FAIL: Similar ads were not grouped together!")
                
                # Step 3: Create different ad (should create a new AdSet)
                logger.info("\n\n=== STEP 3: Creating different ad ===")
                ad3_obj, is_new3 = extraction_service._create_or_update_enhanced_ad(
                    ad3, competitor.id, "test_campaign", ["facebook"], {"name": competitor.name}
                )
                
                if ad3_obj:
                    logger.info(f"Ad 3 created: {ad3_obj.id}, is_new: {is_new3}, ad_set_id: {ad3_obj.ad_set_id}")
                    
                    # Check if it's in a different AdSet
                    if ad1_obj.ad_set_id != ad3_obj.ad_set_id:
                        logger.info("SUCCESS: Different ads were placed in different groups!")
                    else:
                        logger.warning("FAIL: Different ads were incorrectly grouped together!")
                    
                    # Get AdSet info
                    ad_sets = db.query(AdSet).filter(
                        AdSet.id.in_([ad1_obj.ad_set_id, ad3_obj.ad_set_id])
                    ).all()
                    for ad_set in ad_sets:
                        logger.info(f"AdSet {ad_set.id}: {ad_set.variant_count} variants, best_ad_id: {ad_set.best_ad_id}")
                else:
                    logger.error("Failed to create ad 3")
            else:
                logger.error("Failed to create ad 2")
        else:
            logger.error("Failed to create ad 1")
        
        # Commit changes
        db.commit()
        logger.info("Test completed successfully!")
        
    except Exception as e:
        logger.error(f"Error in test: {e}")
        db.rollback()
        raise
    finally:
        db.close()

if __name__ == "__main__":
    main() 


================================================
File: test_creative_comparison.py
================================================
import json
import logging
from sqlalchemy.orm import Session
from app.database import get_db
from app.services.creative_comparison_service import CreativeComparisonService

# Configure logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_image_comparison():
    """Test image comparison functionality"""
    # Sample image URLs
    image_url1 = "https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.35426-6/497848029_3948913998658947_7351662239018085718_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=nxQnOMee6oIQ7kNvwFAx7sU&_nc_oc=AdlIJKDedVpaNBSDhXS2dVZD94AR25DE8oQjZxis_mkF1P8kNePAGuMBLaCSql3Ipv0&_nc_zt=14&_nc_ht=scontent.fdxb2-1.fna&_nc_gid=A-0ybV9RG211oqoD8LV0Ng&oh=00_AfTlW674YCIy4_4JhXlSp6W6GtOaJmaQkSx73_6rI501Lw&oe=6872E288"
    image_url2 = "https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.35426-6/501270508_1047968767280818_2943739564029986859_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=xCL4PLf_bW4Q7kNvwFC--Bs&_nc_oc=Adkn1EPamPrpTCKoWtm5Uc-uR_kcpeRsSp5CdAtYcgWorMhauURqMGyrACx3hOC3w1A&_nc_zt=14&_nc_ht=scontent.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfTMulWaurNs0WEVyZY-W1Vcc4VxIs1JBAG7p8rNFqDPkA&oe=687439F6"
    
    # Get database session
    db = next(get_db())
    comparison_service = CreativeComparisonService(db)
    
    # Compare images
    logger.info("Testing image comparison...")
    similar, diff = comparison_service.compare_images(image_url1, image_url2)
    
    logger.info(f"Images similar: {similar}")
    logger.info(f"Hash difference: {diff}")
    
    return similar, diff

def test_video_comparison():
    """Test video comparison functionality"""
    # Sample video URLs
    video_url1 = "https://video.fdxb2-1.fna.fbcdn.net/v/t42.1790-2/500776772_1046243970804703_2260296777868258061_n.?_nc_cat=111&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=jAl65SwP4FYQ7kNvwGcj7Xq&_nc_oc=AdmTppZ4pnXD3SdF2gAHCNIAoeCUq9YcYnfeZHpjKppaaRA_mZCx-5aFQBciHhu-NeM&_nc_zt=28&_nc_ht=video.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfSYoH938bmQF49QYjshSO3p3Q7FqG63s2DUsDlxKw8v3w&oe=68741178"
    video_url2 = "https://video.fdxb5-1.fna.fbcdn.net/v/t42.1790-2/500585620_1041942271377990_6731322398365148630_n.mp4?_nc_cat=100&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=sUfEhKHH6_QQ7kNvwGGKjZB&_nc_oc=AdnohsNPxbZfgLPAW-XeZOvzdf-P45TYm_z0XqS_OnGcY7eFl2qiQRt-tNKyH06-6Co&_nc_zt=28&_nc_ht=video.fdxb5-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfQYySYuYTIdMpPDiNCFrJpHyWfmkX9RuSIzqXNJXy7tKQ&oe=68743CB4"
    
    # Get database session
    db = next(get_db())
    comparison_service = CreativeComparisonService(db)
    
    # Compare videos
    logger.info("Testing video comparison...")
    similar, score = comparison_service.compare_videos(video_url1, video_url2)
    
    logger.info(f"Videos similar: {similar}")
    logger.info(f"Similarity score: {score:.2f}")
    
    return similar, score

def test_ad_creative_comparison():
    """Test complete ad creative comparison"""
    # Sample ad creatives with different types of media
    creative1 = {
        "id": "1610955929600115-0",
        "title": None,
        "body": "Check out our amazing offer!",
        "caption": None,
        "link_url": None,
        "link_description": None,
        "media": [
            {
                "url": "https://video.fdxb2-1.fna.fbcdn.net/v/t42.1790-2/500776772_1046243970804703_2260296777868258061_n.?_nc_cat=111&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=jAl65SwP4FYQ7kNvwGcj7Xq&_nc_oc=AdmTppZ4pnXD3SdF2gAHCNIAoeCUq9YcYnfeZHpjKppaaRA_mZCx-5aFQBciHhu-NeM&_nc_zt=28&_nc_ht=video.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfSYoH938bmQF49QYjshSO3p3Q7FqG63s2DUsDlxKw8v3w&oe=68741178",
                "type": "Video"
            }
        ]
    }
    
    creative2 = {
        "id": "1610955929600116-0",
        "title": None,
        "body": "Check out our amazing offer!",
        "caption": None,
        "link_url": None,
        "link_description": None,
        "media": [
            {
                "url": "https://video.fdxb5-1.fna.fbcdn.net/v/t42.1790-2/500585620_1041942271377990_6731322398365148630_n.mp4?_nc_cat=100&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=sUfEhKHH6_QQ7kNvwGGKjZB&_nc_oc=AdnohsNPxbZfgLPAW-XeZOvzdf-P45TYm_z0XqS_OnGcY7eFl2qiQRt-tNKyH06-6Co&_nc_zt=28&_nc_ht=video.fdxb5-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfQYySYuYTIdMpPDiNCFrJpHyWfmkX9RuSIzqXNJXy7tKQ&oe=68743CB4",
                "type": "Video"
            }
        ]
    }
    
    creative3 = {
        "id": "1610955929600117-0",
        "title": None,
        "body": "A completely different ad creative!",
        "caption": None,
        "link_url": None,
        "link_description": None,
        "media": [
            {
                "url": "https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.35426-6/501270508_1047968767280818_2943739564029986859_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=xCL4PLf_bW4Q7kNvwFC--Bs&_nc_oc=Adkn1EPamPrpTCKoWtm5Uc-uR_kcpeRsSp5CdAtYcgWorMhauURqMGyrACx3hOC3w1A&_nc_zt=14&_nc_ht=scontent.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfTMulWaurNs0WEVyZY-W1Vcc4VxIs1JBAG7p8rNFqDPkA&oe=687439F6",
                "type": "Image"
            }
        ]
    }
    
    # Get database session
    db = next(get_db())
    comparison_service = CreativeComparisonService(db)
    
    # Compare creatives
    logger.info("Testing ad creative comparison...")
    
    # Compare similar video creatives
    similar1, score1, type1 = comparison_service.compare_ad_creatives(creative1, creative2)
    logger.info(f"Creatives 1 & 2: similar={similar1}, score={score1:.2f}, type={type1}")
    
    # Compare different creatives (video vs image)
    similar2, score2, type2 = comparison_service.compare_ad_creatives(creative1, creative3)
    logger.info(f"Creatives 1 & 3: similar={similar2}, score={score2:.2f}, type={type2}")
    
    return {
        "similar_creatives": similar1,
        "different_creatives": similar2
    }

def test_complete_ad_grouping():
    """Test complete ad grouping with full ad objects"""
    # Sample ads with creatives 
    ad1 = {
        "ad_archive_id": "1000000000001",
        "meta": {
            "page_id": "123456789",
            "page_name": "Test Page",
            "campaign_id": "campaign1"
        },
        "creatives": [
            {
                "id": "1610955929600115-0",
                "body": "Check out our amazing offer!",
                "media": [
                    {
                        "url": "https://video.fdxb2-1.fna.fbcdn.net/v/t42.1790-2/500776772_1046243970804703_2260296777868258061_n.?_nc_cat=111&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=jAl65SwP4FYQ7kNvwGcj7Xq&_nc_oc=AdmTppZ4pnXD3SdF2gAHCNIAoeCUq9YcYnfeZHpjKppaaRA_mZCx-5aFQBciHhu-NeM&_nc_zt=28&_nc_ht=video.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfSYoH938bmQF49QYjshSO3p3Q7FqG63s2DUsDlxKw8v3w&oe=68741178",
                        "type": "Video"
                    }
                ]
            }
        ]
    }
    
    ad2 = {
        "ad_archive_id": "1000000000002",
        "meta": {
            "page_id": "123456789",
            "page_name": "Test Page",
            "campaign_id": "campaign1"
        },
        "creatives": [
            {
                "id": "1610955929600116-0",
                "body": "Check out our amazing offer!",
                "media": [
                    {
                        "url": "https://video.fdxb5-1.fna.fbcdn.net/v/t42.1790-2/500585620_1041942271377990_6731322398365148630_n.mp4?_nc_cat=100&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=sUfEhKHH6_QQ7kNvwGGKjZB&_nc_oc=AdnohsNPxbZfgLPAW-XeZOvzdf-P45TYm_z0XqS_OnGcY7eFl2qiQRt-tNKyH06-6Co&_nc_zt=28&_nc_ht=video.fdxb5-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfQYySYuYTIdMpPDiNCFrJpHyWfmkX9RuSIzqXNJXy7tKQ&oe=68743CB4",
                        "type": "Video"
                    }
                ]
            }
        ]
    }
    
    ad3 = {
        "ad_archive_id": "1000000000003",
        "meta": {
            "page_id": "123456789",
            "page_name": "Test Page",
            "campaign_id": "campaign1"
        },
        "creatives": [
            {
                "id": "1610955929600117-0",
                "body": "A completely different ad creative!",
                "media": [
                    {
                        "url": "https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.35426-6/501270508_1047968767280818_2943739564029986859_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=xCL4PLf_bW4Q7kNvwFC--Bs&_nc_oc=Adkn1EPamPrpTCKoWtm5Uc-uR_kcpeRsSp5CdAtYcgWorMhauURqMGyrACx3hOC3w1A&_nc_zt=14&_nc_ht=scontent.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfTMulWaurNs0WEVyZY-W1Vcc4VxIs1JBAG7p8rNFqDPkA&oe=687439F6",
                        "type": "Image"
                    }
                ]
            }
        ]
    }
    
    # Get database session
    db = next(get_db())
    comparison_service = CreativeComparisonService(db)
    
    # Test ad grouping
    logger.info("Testing ad grouping...")
    should_group_1_2 = comparison_service.should_group_ads(ad1, ad2)
    should_group_1_3 = comparison_service.should_group_ads(ad1, ad3)
    
    logger.info(f"Should group ads 1 & 2: {should_group_1_2}")
    logger.info(f"Should group ads 1 & 3: {should_group_1_3}")
    
    return {
        "should_group_similar_ads": should_group_1_2,
        "should_group_different_ads": should_group_1_3
    }

if __name__ == "__main__":
    logger.info("Running creative comparison tests...")
    
    # Run test functions
    image_results = test_image_comparison()
    video_results = test_video_comparison()
    creative_results = test_ad_creative_comparison()
    grouping_results = test_complete_ad_grouping()
    
    # Print summary
    logger.info("\n=== Test Results Summary ===")
    logger.info(f"Image comparison: {image_results}")
    logger.info(f"Video comparison: {video_results}")
    logger.info(f"Creative comparison: {creative_results}")
    logger.info(f"Ad grouping: {grouping_results}") 


================================================
File: video_comparator_fast.py
================================================
"""Fast Online Video Comparator
================================

This script compares **two remote videos** (HTTP/S URLs) quickly by:
1. Seeking to *N* evenly-spaced timestamps (default 12) instead of decoding the
   whole stream.
2. Hashing those frames with a lightweight `average_hash` (faster than phash).
3. Running the sampling of both videos in parallel threads to overlap network
   I/O and decoding.

Dependencies (install with pip):
    pip install av pillow imagehash requests

Note: PyAV wheels package FFmpeg so you usually don't need FFmpeg installed
separately.
"""

from __future__ import annotations

import concurrent.futures
import requests
from typing import List, Tuple, Any

import av  # type: ignore
from PIL import Image
import imagehash
import time

# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------

def _duration_seconds(stream: av.video.stream.VideoStream, container: av.container.input.InputContainer) -> float | None:  # type: ignore[name-defined]
    """Return duration of the video stream in seconds (best effort)."""
    if stream.duration and stream.time_base:  # Prefer per-stream duration
        return float(stream.duration * stream.time_base)
    if container.duration:
        # container.duration is in micro-seconds
        return float(container.duration / 1_000_000)
    return None


def _sample_hashes(
    url: str,
    samples: int = 12,
    resize: int = 8,
) -> List[imagehash.ImageHash]:
    """Return a list of perceptual hashes sampled from the remote video.

    It seeks to *samples* evenly spaced timestamps to avoid decoding the entire
    video. Falls back to sequential reading if seeking isn't supported.
    """
    container = av.open(url, timeout=15)
    video_stream = next(s for s in container.streams if s.type == "video")
    dur_s = _duration_seconds(video_stream, container)

    hashes: List[imagehash.ImageHash] = []

    def _hash_frame(frame: Any):  # accept any type for typing flexibility
        pil_img: Image.Image = frame.to_image()
        hashes.append(imagehash.average_hash(pil_img, hash_size=resize))

    if dur_s and dur_s > 0:
        # Seek method
        step = dur_s / (samples + 1)
        for i in range(samples):
            ts = (i + 1) * step
            # Convert seconds to stream timestamp units (time_base)
            tb = float(video_stream.time_base) if video_stream.time_base else 1.0
            pts = int(ts / tb)
            try:
                container.seek(pts, any_frame=False, backward=True, stream=video_stream)
                frame = next(container.decode(video_stream))
                _hash_frame(frame)
            except (StopIteration, av.AVError):  # type: ignore[attr-defined]
                # Fallback: cannot seek/decode
                break
    else:
        # Unknown duration â€“ sequentially iterate until we collect enough hashes
        for frame in container.decode(video_stream):  # type: ignore[arg-type]
            _hash_frame(frame)
            if len(hashes) >= samples:
                break

    container.close()
    return hashes


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def compare_videos_fast(
    url1: str,
    url2: str,
    samples: int = 6,
    hash_cutoff: int = 6,
    similarity_threshold: float = 0.9,
) -> Tuple[bool, float]:
    """Compare two online videos quickly.

    Args:
        url1, url2: Remote video URLs (HTTP/S).
        samples: Number of frames to compare (higher = more robust, slower).
        hash_cutoff: Maximum Hamming distance for two frame hashes to match.
        similarity_threshold: Fraction of matching samples (0-1) to call videos similar.

    Returns:
        (is_similar, similarity_score) â€“ similarity_score is 0-1.
    """

    # 0. Quick metadata check via parallel HEAD requests.
    def _head(url: str):
        try:
            r = requests.head(url, timeout=5, allow_redirects=True)
            return r.headers.get("ETag"), r.headers.get("Content-Length")
        except requests.RequestException:
            return None, None

    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as pool:
        etag1, length1 = pool.submit(_head, url1).result()
        etag2, length2 = pool.submit(_head, url2).result()

    # If both ETags present and equal, videos are identical.
    if etag1 and etag2 and etag1 == etag2:
        print("[âœ“] Videos considered identical via ETag match.")
        return True, 1.0

    # If Content-Length identical and >0, assume high likelihood of same.
    if length1 and length2 and length1 == length2:
        print("[i] Content-Length values match â€“ performing quick frame check (2 samples).")
        # Still need small verification; reduce samples to 2 for speed.
        samples = min(samples, 2)

    # 1. Collect hashes in parallel to overlap network I/O.
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        fut1 = executor.submit(_sample_hashes, url1, samples)
        fut2 = executor.submit(_sample_hashes, url2, samples)
        hashes1 = fut1.result()
        hashes2 = fut2.result()

    common = min(len(hashes1), len(hashes2))
    if common == 0:
        return False, 0.0

    matches = sum(1 for h1, h2 in zip(hashes1[:common], hashes2[:common]) if (h1 - h2) <= hash_cutoff)
    score = matches / common

    method_msg = f"[i] Compared {common} sampled frames; {matches} matched (hash diff â‰¤ {hash_cutoff})."
    print(method_msg)

    return score >= similarity_threshold, score


# ---------------------------------------------------------------------------
# Example usage
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    URL_A = (
        "https://video.fdxb2-1.fna.fbcdn.net/v/t42.1790-2/501595635_710078641600098_9001144302157184023_n.?"
        "_nc_cat=111&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=u__D-ijGQ7YQ7kNvwEQaajI&_nc_oc=AdmVb4xUEFDDNRY7whObNWmA6fSP3AHooyy17_sMJCrWYGNWDxTnFwzy4n-5HAr_Z4k"
        "&_nc_zt=28&_nc_ht=video.fdxb2-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfR_r3phsy_uKkFbKyZsF_nPJiuGLC3j1-MwVZWAHfA2ZA&oe=68742594"
    )

    URL_B = (
        "https://video.fdxb5-1.fna.fbcdn.net/v/t42.1790-2/499806541_1037702368427204_3792614188364162071_n.?_nc_cat=103&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=Gi_YamPRjn4Q7kNvwE1WWGm&_nc_oc=Admthu2TR-oU9FwH7SbQJZqyWyXRdhu7z81YlAR_JRiY8xYL0C_0-eHSSG_xEFkylxs&_nc_zt=28&_nc_ht=video.fdxb5-1.fna&_nc_gid=FRdl93qaepNksvDaMMRp7Q&oh=00_AfRlqxwwn0Cz7oiLPVNiB57nAf1Fhc9J81gBdyLtx-I8RQ&oe=6874153D"
    )

    URL_C = (
        "https://video.fdxb5-1.fna.fbcdn.net/v/t42.1790-2/502124283_2150294738726470_8517492775858613391_n.?_nc_cat=100&ccb=1-7&_nc_sid=c53f8f&_nc_ohc=n40RreZnnGEQ7kNvwE-CX5D&_nc_oc=AdmnJGwngXeMqLmPO5ObS6cIPr6irdcz_cAqVkt4PlrjZBeHIluNC6IwOhI2AuFdSxo&_nc_zt=28&_nc_ht=video.fdxb5-1.fna&_nc_gid=eDNnjQ2zGQ9Ir197_BVt_w&oh=00_AfS8OfikW75sJ1OusLfILoXkeS-TyT-groGp8TZx2jhVzA&oe=68743694" )

    print("â€” Fast Online Video Comparison â€”")
    start = time.perf_counter()
    similar, similarity = compare_videos_fast(URL_A, URL_C)
    elapsed = time.perf_counter() - start
    print(f"Similarity score: {similarity:.2%}")
    print("Videos are", "SIMILAR" if similar else "DIFFERENT")
    print(f"Time taken: {elapsed:.2f} seconds") 


================================================
File: alembic/README
================================================
Generic single-database configuration.


================================================
File: alembic/env.py
================================================
from logging.config import fileConfig
import os
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Import our models and database configuration
from app.database import Base
from app.models import Competitor, Ad, AdAnalysis

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata

# Set the database URL from environment variables
database_url = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@db:5432/ads_db")
config.set_main_option("sqlalchemy.url", database_url)

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()



================================================
File: alembic/script.py.mako
================================================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}



================================================
File: alembic/versions/014e83d13d0a_create_campaign_model_and_link_to_ad.py
================================================
"""create_campaign_model_and_link_to_ad

Revision ID: 014e83d13d0a
Revises: 31ab1628a920
Create Date: 2025-07-06 21:55:01.128553

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '014e83d13d0a'
down_revision: Union[str, None] = '31ab1628a920'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('campaign_id', sa.Integer(), nullable=False))
    op.create_index(op.f('ix_ads_campaign_id'), 'ads', ['campaign_id'], unique=False)
    op.create_foreign_key(None, 'ads', 'campaigns', ['campaign_id'], ['id'])
    op.drop_column('ads', 'creatives')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('creatives', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'ads', type_='foreignkey')
    op.drop_index(op.f('ix_ads_campaign_id'), table_name='ads')
    op.drop_column('ads', 'campaign_id')
    # ### end Alembic commands ###



================================================
File: alembic/versions/0a968eddd8d7_enhanced_extraction_fields_and_cleanup.py
================================================
"""enhanced_extraction_fields_and_cleanup

Revision ID: 0a968eddd8d7
Revises: d26d6ee785d3
Create Date: 2025-07-06 15:07:39.350024

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0a968eddd8d7'
down_revision: Union[str, None] = 'd26d6ee785d3'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('creatives_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.Boolean(), nullable=False))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('landing_page_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.String(), nullable=True))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'extraction_version')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'form_details')
    op.drop_column('ads', 'extraction_date')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'main_title')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'creatives')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'targeted_countries')
    op.drop_column('ads', 'main_image_urls')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('main_image_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('targeted_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('creatives', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('running_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_categories', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('spend', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_titles', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_title', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('currency', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extraction_date', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('form_details', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_links', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_bodies', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extraction_version', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'landing_page_url')
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'is_enhanced_processed')
    op.drop_column('ads', 'creatives_data')
    # ### end Alembic commands ###



================================================
File: alembic/versions/31ab1628a920_add_page_url_to_competitor.py
================================================
"""add page_url to competitor

Revision ID: 31ab1628a920
Revises: a2feb1e20f39
Create Date: 2025-07-06 20:54:51.622764

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '31ab1628a920'
down_revision: Union[str, None] = 'a2feb1e20f39'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('competitors', sa.Column('page_url', sa.String(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('competitors', 'page_url')
    # ### end Alembic commands ###



================================================
File: alembic/versions/4b419cac5311_initial_migration_create_competitors_.py
================================================
"""Initial migration: Create competitors, ads, and ad_analyses tables

Revision ID: 4b419cac5311
Revises: 
Create Date: 2025-07-03 14:33:19.713938

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '4b419cac5311'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('competitors',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('page_id', sa.String(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_competitors_id'), 'competitors', ['id'], unique=False)
    op.create_index(op.f('ix_competitors_name'), 'competitors', ['name'], unique=False)
    op.create_index(op.f('ix_competitors_page_id'), 'competitors', ['page_id'], unique=True)
    op.create_table('ads',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('competitor_id', sa.Integer(), nullable=False),
    sa.Column('ad_archive_id', sa.String(), nullable=False),
    sa.Column('ad_copy', sa.Text(), nullable=True),
    sa.Column('media_type', sa.String(), nullable=True),
    sa.Column('media_url', sa.String(), nullable=True),
    sa.Column('landing_page_url', sa.String(), nullable=True),
    sa.Column('date_found', sa.DateTime(timezone=True), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('raw_data', sa.JSON(), nullable=True),
    sa.Column('page_name', sa.String(), nullable=True),
    sa.Column('publisher_platform', sa.JSON(), nullable=True),
    sa.Column('impressions_text', sa.String(), nullable=True),
    sa.Column('end_date', sa.DateTime(timezone=True), nullable=True),
    sa.Column('cta_text', sa.String(), nullable=True),
    sa.Column('cta_type', sa.String(), nullable=True),
    sa.ForeignKeyConstraint(['competitor_id'], ['competitors.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_ads_ad_archive_id'), 'ads', ['ad_archive_id'], unique=True)
    op.create_index(op.f('ix_ads_date_found'), 'ads', ['date_found'], unique=False)
    op.create_index(op.f('ix_ads_id'), 'ads', ['id'], unique=False)
    op.create_table('ad_analyses',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('ad_id', sa.Integer(), nullable=False),
    sa.Column('summary', sa.Text(), nullable=True),
    sa.Column('hook_score', sa.Float(), nullable=True),
    sa.Column('overall_score', sa.Float(), nullable=True),
    sa.Column('ai_prompts', sa.JSON(), nullable=True),
    sa.Column('raw_ai_response', sa.JSON(), nullable=True),
    sa.Column('target_audience', sa.String(), nullable=True),
    sa.Column('ad_format_analysis', sa.JSON(), nullable=True),
    sa.Column('competitor_insights', sa.JSON(), nullable=True),
    sa.Column('content_themes', sa.JSON(), nullable=True),
    sa.Column('performance_predictions', sa.JSON(), nullable=True),
    sa.Column('analysis_version', sa.String(), nullable=True),
    sa.Column('confidence_score', sa.Float(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['ad_id'], ['ads.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('ad_id')
    )
    op.create_index(op.f('ix_ad_analyses_id'), 'ad_analyses', ['id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_ad_analyses_id'), table_name='ad_analyses')
    op.drop_table('ad_analyses')
    op.drop_index(op.f('ix_ads_id'), table_name='ads')
    op.drop_index(op.f('ix_ads_date_found'), table_name='ads')
    op.drop_index(op.f('ix_ads_ad_archive_id'), table_name='ads')
    op.drop_table('ads')
    op.drop_index(op.f('ix_competitors_page_id'), table_name='competitors')
    op.drop_index(op.f('ix_competitors_name'), table_name='competitors')
    op.drop_index(op.f('ix_competitors_id'), table_name='competitors')
    op.drop_table('competitors')
    # ### end Alembic commands ###



================================================
File: alembic/versions/568f9fde44b0_add_enhanced_extraction_fields_and_.py
================================================
"""add_enhanced_extraction_fields_and_cleanup_legacy

Revision ID: 568f9fde44b0
Revises: a220f98e38c2
Create Date: 2025-07-06 14:33:53.996725

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '568f9fde44b0'
down_revision: Union[str, None] = 'a220f98e38c2'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('campaign_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_is_active', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('meta_cta_type', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_display_format', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_start_date', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('meta_end_date', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('targeting_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('lead_form_questions', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('lead_form_standalone_fields', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('creatives_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_name', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_likes', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_profile_picture', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('platforms', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.Boolean(), nullable=False))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.String(), nullable=True))
    op.drop_index('ix_ads_page_id', table_name='ads')
    op.create_index(op.f('ix_ads_advertiser_page_id'), 'ads', ['advertiser_page_id'], unique=False)
    op.create_index(op.f('ix_ads_campaign_id'), 'ads', ['campaign_id'], unique=False)
    op.drop_column('ads', 'start_date_readable')
    op.drop_column('ads', 'main_title')
    op.drop_column('ads', 'form_details')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'end_date_readable')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'page_number')
    op.drop_column('ads', 'main_image_urls')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'collation_count')
    op.drop_column('ads', 'card_media_types')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'structured_ad_copy')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'targeted_countries')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('targeted_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('structured_ad_copy', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('spend', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_media_types', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('collation_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_categories', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_image_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_number', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_titles', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_links', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_urls', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('currency', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('card_bodies', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('extra_texts', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('end_date_readable', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('running_countries', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('form_details', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('main_title', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('start_date_readable', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.drop_index(op.f('ix_ads_campaign_id'), table_name='ads')
    op.drop_index(op.f('ix_ads_advertiser_page_id'), table_name='ads')
    op.create_index('ix_ads_page_id', 'ads', ['page_id'], unique=False)
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'is_enhanced_processed')
    op.drop_column('ads', 'platforms')
    op.drop_column('ads', 'advertiser_page_profile_picture')
    op.drop_column('ads', 'advertiser_page_likes')
    op.drop_column('ads', 'advertiser_page_url')
    op.drop_column('ads', 'advertiser_page_name')
    op.drop_column('ads', 'advertiser_page_id')
    op.drop_column('ads', 'creatives_data')
    op.drop_column('ads', 'lead_form_standalone_fields')
    op.drop_column('ads', 'lead_form_questions')
    op.drop_column('ads', 'targeting_data')
    op.drop_column('ads', 'meta_end_date')
    op.drop_column('ads', 'meta_start_date')
    op.drop_column('ads', 'meta_display_format')
    op.drop_column('ads', 'meta_cta_type')
    op.drop_column('ads', 'meta_is_active')
    op.drop_column('ads', 'campaign_id')
    # ### end Alembic commands ###



================================================
File: alembic/versions/5a527db67d43_enhanced_extraction_fields_and_cleanup.py
================================================
"""enhanced_extraction_fields_and_cleanup

Revision ID: 5a527db67d43
Revises: 0a968eddd8d7
Create Date: 2025-07-06 17:19:37.862077

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '5a527db67d43'
down_revision: Union[str, None] = '0a968eddd8d7'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/6168e4625f40_add_enhanced_extraction_fields_and_.py
================================================
"""add_enhanced_extraction_fields_and_cleanup_legacy

Revision ID: 6168e4625f40
Revises: 568f9fde44b0
Create Date: 2025-07-06 14:39:39.986058

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '6168e4625f40'
down_revision: Union[str, None] = '568f9fde44b0'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/779734a0c91c_add_comprehensive_facebook_ads_data_.py
================================================
"""Add comprehensive Facebook ads data fields

Revision ID: 779734a0c91c
Revises: 4b419cac5311
Create Date: 2025-07-03 16:52:23.691781

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '779734a0c91c'
down_revision: Union[str, None] = '4b419cac5311'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('page_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('collation_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('is_active', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('start_date', sa.DateTime(timezone=True), nullable=True))
    op.add_column('ads', sa.Column('currency', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('spend', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('display_format', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('page_categories', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('targeted_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('main_title', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_image_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('card_bodies', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_titles', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_media_types', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_links', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('start_date_readable', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('end_date_readable', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_number', sa.Integer(), nullable=True))
    op.create_index(op.f('ix_ads_page_id'), 'ads', ['page_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_ads_page_id'), table_name='ads')
    op.drop_column('ads', 'page_number')
    op.drop_column('ads', 'end_date_readable')
    op.drop_column('ads', 'start_date_readable')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'card_media_types')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'main_image_urls')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'main_title')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'targeted_countries')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'display_format')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'start_date')
    op.drop_column('ads', 'is_active')
    op.drop_column('ads', 'collation_count')
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'page_id')
    # ### end Alembic commands ###



================================================
File: alembic/versions/8965d492a259_update_adset_content_signature_to_.py
================================================
"""update_adset_content_signature_to_visual_hash

Revision ID: 8965d492a259
Revises: f2b75a2c1596
Create Date: 2025-07-11 08:32:14.694810

"""
from typing import Sequence, Union
import logging

from alembic import op
import sqlalchemy as sa
from sqlalchemy.orm import Session

# Import necessary models and services for the migration
from app.models import AdSet, Ad
from app.services.enhanced_ad_extraction import EnhancedAdExtractionService

logger = logging.getLogger(__name__)

# revision identifiers, used by Alembic.
revision: str = '8965d492a259'
down_revision: Union[str, None] = 'f2b75a2c1596'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """
    Backfill existing AdSets with perceptual hashes of their best_ad media.
    This migration updates all existing content_signature fields to use 
    visual identifiers instead of content-based hashes.
    """
    print("Starting migration to update AdSet content_signature to visual hash...")
    
    # Get database connection from Alembic
    bind = op.get_bind()
    session = Session(bind=bind)
    
    try:
        # Initialize the service for hash calculations
        extraction_service = EnhancedAdExtractionService(session)
        
        # Get all existing AdSets
        ad_sets = session.query(AdSet).all()
        print(f"Found {len(ad_sets)} AdSets to update")
        
        updated_count = 0
        failed_count = 0
        
        for ad_set in ad_sets:
            try:
                print(f"Processing AdSet {ad_set.id}...")
                
                # Get the best_ad for this AdSet
                if not ad_set.best_ad_id:
                    print(f"AdSet {ad_set.id} has no best_ad_id, skipping...")
                    continue
                
                best_ad = session.query(Ad).filter(Ad.id == ad_set.best_ad_id).first()
                if not best_ad:
                    print(f"AdSet {ad_set.id} has best_ad_id {ad_set.best_ad_id} but ad not found, skipping...")
                    continue
                
                # Convert the best ad to dict format for hash calculation
                try:
                    best_ad_data = best_ad.to_enhanced_format()
                    if not best_ad_data:
                        print(f"Could not convert best ad {best_ad.id} to dict format, skipping AdSet {ad_set.id}")
                        continue
                    
                    # Calculate new perceptual hash
                    new_signature = extraction_service._generate_content_signature(best_ad_data)
                    if new_signature:
                        # Update the content_signature
                        old_signature = ad_set.content_signature
                        ad_set.content_signature = new_signature
                        
                        print(f"Updated AdSet {ad_set.id}: {old_signature[:10]}... -> {new_signature[:10]}...")
                        updated_count += 1
                    else:
                        print(f"Could not generate perceptual hash for AdSet {ad_set.id}")
                        failed_count += 1
                        
                except Exception as e:
                    print(f"Error processing AdSet {ad_set.id}: {e}")
                    failed_count += 1
                    continue
                    
            except Exception as e:
                print(f"Error processing AdSet {ad_set.id}: {e}")
                failed_count += 1
                continue
        
        # Commit all changes
        session.commit()
        print(f"Migration completed: {updated_count} AdSets updated, {failed_count} failed")
        
    except Exception as e:
        print(f"Migration failed with error: {e}")
        session.rollback()
        raise
    finally:
        session.close()


def downgrade() -> None:
    """
    Downgrade is not supported for this migration as it would require 
    storing the original content-based signatures, which are no longer needed.
    """
    print("Downgrade not supported for content_signature migration - original hashes are not preserved")
    pass



================================================
File: alembic/versions/8a69c9ce3517_add_duration_days_to_ads.py
================================================
"""add_duration_days_to_ads

Revision ID: 8a69c9ce3517
Revises: 014e83d13d0a
Create Date: 2025-07-08 11:42:09.030863

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '8a69c9ce3517'
down_revision: Union[str, None] = '014e83d13d0a'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    # First drop foreign key constraint and campaign_id column
    op.drop_index('ix_ads_campaign_id', table_name='ads')
    op.drop_constraint('ads_campaign_id_fkey', 'ads', type_='foreignkey')
    op.drop_column('ads', 'campaign_id')
    
    # Then add the new duration_days column
    op.add_column('ads', sa.Column('duration_days', sa.Integer(), nullable=True))
    op.create_index(op.f('ix_ads_duration_days'), 'ads', ['duration_days'], unique=False)
    
    # Finally drop the campaigns table and its indexes
    op.drop_index('ix_campaigns_content_signature', table_name='campaigns')
    op.drop_index('ix_campaigns_id', table_name='campaigns')
    op.drop_table('campaigns')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    # First recreate the campaigns table and its indexes
    op.create_table('campaigns',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('content_signature', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('creatives', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name='campaigns_pkey')
    )
    op.create_index('ix_campaigns_id', 'campaigns', ['id'], unique=False)
    op.create_index('ix_campaigns_content_signature', 'campaigns', ['content_signature'], unique=True)
    
    # Then remove duration_days column and index
    op.drop_index(op.f('ix_ads_duration_days'), table_name='ads')
    op.drop_column('ads', 'duration_days')
    
    # Finally add back campaign_id column and foreign key
    op.add_column('ads', sa.Column('campaign_id', sa.INTEGER(), autoincrement=False, nullable=True))
    op.create_index('ix_ads_campaign_id', 'ads', ['campaign_id'], unique=False)
    op.create_foreign_key('ads_campaign_id_fkey', 'ads', 'campaigns', ['campaign_id'], ['id'])
    # ### end Alembic commands ###



================================================
File: alembic/versions/a220f98e38c2_add_form_details_running_countries_.py
================================================
"""add_form_details_running_countries_structured_ad_copy_to_ads

Revision ID: a220f98e38c2
Revises: 779734a0c91c
Create Date: 2025-07-04 13:44:11.317930

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'a220f98e38c2'
down_revision: Union[str, None] = '779734a0c91c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('form_details', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('running_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('structured_ad_copy', sa.Text(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('ads', 'structured_ad_copy')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'form_details')
    # ### end Alembic commands ###



================================================
File: alembic/versions/a2feb1e20f39_add_meta_targeting_lead_form_and_.py
================================================
"""Add meta, targeting, lead_form, and creatives columns and remove obsolete fields

Revision ID: a2feb1e20f39
Revises: 5a527db67d43
Create Date: 2025-07-06 19:37:34.588344

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'a2feb1e20f39'
down_revision: Union[str, None] = '5a527db67d43'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('meta', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('targeting', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('lead_form', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('creatives', sa.JSON(), nullable=True))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_index('ix_ads_advertiser_page_id', table_name='ads')
    op.drop_index('ix_ads_campaign_id', table_name='ads')
    op.create_index(op.f('ix_ads_competitor_id'), 'ads', ['competitor_id'], unique=False)
    op.drop_column('ads', 'targeting_data')
    op.drop_column('ads', 'lead_form_standalone_fields')
    op.drop_column('ads', 'landing_page_url')
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'page_id')
    op.drop_column('ads', 'advertiser_page_url')
    op.drop_column('ads', 'advertiser_page_id')
    op.drop_column('ads', 'advertiser_page_likes')
    op.drop_column('ads', 'ad_copy')
    op.drop_column('ads', 'lead_form_questions')
    op.drop_column('ads', 'impressions_text')
    op.drop_column('ads', 'advertiser_page_name')
    op.drop_column('ads', 'is_active')
    op.drop_column('ads', 'campaign_id')
    op.drop_column('ads', 'cta_text')
    op.drop_column('ads', 'meta_start_date')
    op.drop_column('ads', 'meta_display_format')
    op.drop_column('ads', 'advertiser_page_profile_picture')
    op.drop_column('ads', 'start_date')
    op.drop_column('ads', 'platforms')
    op.drop_column('ads', 'cta_type')
    op.drop_column('ads', 'meta_is_active')
    op.drop_column('ads', 'media_url')
    op.drop_column('ads', 'meta_end_date')
    op.drop_column('ads', 'is_enhanced_processed')
    op.drop_column('ads', 'page_name')
    op.drop_column('ads', 'media_type')
    op.drop_column('ads', 'end_date')
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'display_format')
    op.drop_column('ads', 'creatives_data')
    op.drop_column('ads', 'meta_cta_type')
    op.drop_column('ads', 'publisher_platform')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('publisher_platform', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_cta_type', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('creatives_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('display_format', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('end_date', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('media_type', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.BOOLEAN(), autoincrement=False, nullable=False))
    op.add_column('ads', sa.Column('meta_end_date', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('media_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_is_active', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('cta_type', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('platforms', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('start_date', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_profile_picture', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_display_format', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('meta_start_date', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('cta_text', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('campaign_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('is_active', sa.BOOLEAN(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('impressions_text', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('lead_form_questions', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('ad_copy', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_likes', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('advertiser_page_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('page_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('landing_page_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('lead_form_standalone_fields', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('targeting_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_index(op.f('ix_ads_competitor_id'), table_name='ads')
    op.create_index('ix_ads_campaign_id', 'ads', ['campaign_id'], unique=False)
    op.create_index('ix_ads_advertiser_page_id', 'ads', ['advertiser_page_id'], unique=False)
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'creatives')
    op.drop_column('ads', 'lead_form')
    op.drop_column('ads', 'targeting')
    op.drop_column('ads', 'meta')
    # ### end Alembic commands ###



================================================
File: alembic/versions/a7031f8c7467_add_enhanced_extraction_fields_and_keep_.py
================================================
"""add_enhanced_extraction_fields_and_keep_legacy

Revision ID: a7031f8c7467
Revises: 6168e4625f40
Create Date: 2025-07-06 14:49:31.579960

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'a7031f8c7467'
down_revision: Union[str, None] = '6168e4625f40'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/b1c2d3e4f5g6_add_date_range_to_ad_sets.py
================================================
"""add_date_range_to_ad_sets

Revision ID: b1c2d3e4f5g6
Revises: ea1c3a5b9c21
Create Date: 2025-07-11 13:00:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

revision: str = 'b1c2d3e4f5g6'
down_revision: Union[str, None] = 'ea1c3a5b9c21'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # 1. Add new columns
    op.add_column('ad_sets', sa.Column('first_seen_date', sa.DateTime(timezone=True), nullable=True))
    op.add_column('ad_sets', sa.Column('last_seen_date', sa.DateTime(timezone=True), nullable=True))
    op.create_index('ix_ad_sets_first_seen_date', 'ad_sets', ['first_seen_date'])
    op.create_index('ix_ad_sets_last_seen_date', 'ad_sets', ['last_seen_date'])

    # 2. Backfill existing rows with min/max date_found from ads table
    backfill_sql = """
    UPDATE ad_sets AS s
    SET first_seen_date = sub.min_date,
        last_seen_date = sub.max_date
    FROM (
        SELECT ad_set_id, MIN(date_found) AS min_date, MAX(date_found) AS max_date
        FROM ads
        WHERE ad_set_id IS NOT NULL
        GROUP BY ad_set_id
    ) AS sub
    WHERE s.id = sub.ad_set_id;
    """
    op.execute(backfill_sql)


def downgrade() -> None:
    op.drop_index('ix_ad_sets_last_seen_date', table_name='ad_sets')
    op.drop_index('ix_ad_sets_first_seen_date', table_name='ad_sets')
    op.drop_column('ad_sets', 'last_seen_date')
    op.drop_column('ad_sets', 'first_seen_date') 


================================================
File: alembic/versions/d26d6ee785d3_add_enhanced_extraction_fields_hybrid_.py
================================================
"""add_enhanced_extraction_fields_hybrid_model

Revision ID: d26d6ee785d3
Revises: a7031f8c7467
Create Date: 2025-07-06 14:54:11.470819

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'd26d6ee785d3'
down_revision: Union[str, None] = 'a7031f8c7467'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('main_title', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_body_text', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_caption', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_link_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('main_link_description', sa.Text(), nullable=True))
    op.add_column('ads', sa.Column('main_image_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('main_video_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('page_like_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('page_categories', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_uri', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('page_profile_picture_url', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('targeted_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('impressions_index', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('spend', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('currency', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('card_count', sa.Integer(), nullable=True))
    op.add_column('ads', sa.Column('card_titles', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_bodies', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_cta_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_urls', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('card_complete_data', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_texts', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extra_links', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('form_details', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('running_countries', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('contains_sensitive_content', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('contains_digital_created_media', sa.Boolean(), nullable=True))
    op.add_column('ads', sa.Column('creatives', sa.JSON(), nullable=True))
    op.add_column('ads', sa.Column('extraction_version', sa.String(), nullable=True))
    op.add_column('ads', sa.Column('extraction_date', sa.DateTime(timezone=True), nullable=True))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'collation_id')
    op.drop_column('ads', 'creatives_data')
    op.drop_column('ads', 'enhanced_processing_version')
    op.drop_column('ads', 'landing_page_url')
    op.drop_column('ads', 'is_enhanced_processed')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('ads', sa.Column('is_enhanced_processed', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=False))
    op.add_column('ads', sa.Column('landing_page_url', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('enhanced_processing_version', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('creatives_data', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('ads', sa.Column('collation_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.alter_column('ads', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('ads', 'created_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.drop_column('ads', 'extraction_date')
    op.drop_column('ads', 'extraction_version')
    op.drop_column('ads', 'creatives')
    op.drop_column('ads', 'contains_digital_created_media')
    op.drop_column('ads', 'contains_sensitive_content')
    op.drop_column('ads', 'running_countries')
    op.drop_column('ads', 'form_details')
    op.drop_column('ads', 'extra_links')
    op.drop_column('ads', 'extra_texts')
    op.drop_column('ads', 'card_complete_data')
    op.drop_column('ads', 'card_urls')
    op.drop_column('ads', 'card_cta_texts')
    op.drop_column('ads', 'card_bodies')
    op.drop_column('ads', 'card_titles')
    op.drop_column('ads', 'card_count')
    op.drop_column('ads', 'currency')
    op.drop_column('ads', 'spend')
    op.drop_column('ads', 'impressions_index')
    op.drop_column('ads', 'targeted_countries')
    op.drop_column('ads', 'page_profile_picture_url')
    op.drop_column('ads', 'page_profile_uri')
    op.drop_column('ads', 'page_categories')
    op.drop_column('ads', 'page_like_count')
    op.drop_column('ads', 'main_video_urls')
    op.drop_column('ads', 'main_image_urls')
    op.drop_column('ads', 'main_link_description')
    op.drop_column('ads', 'main_link_url')
    op.drop_column('ads', 'main_caption')
    op.drop_column('ads', 'main_body_text')
    op.drop_column('ads', 'main_title')
    # ### end Alembic commands ###



================================================
File: alembic/versions/df763544010c_update_adset_content_signature_to_.py
================================================
"""update_adset_content_signature_to_visual_hash

Revision ID: df763544010c
Revises: e9be095c41d0
Create Date: 2025-07-11 07:33:54.874053

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'df763544010c'
down_revision: Union[str, None] = 'e9be095c41d0'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    pass


def downgrade() -> None:
    pass



================================================
File: alembic/versions/e9be095c41d0_create_ad_sets_table_and_link_to_ads.py
================================================
"""create_ad_sets_table_and_link_to_ads

Revision ID: e9be095c41d0
Revises: 8a69c9ce3517
Create Date: 2025-07-09 07:11:21.019365

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'e9be095c41d0'
down_revision: Union[str, None] = '8a69c9ce3517'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###



================================================
File: alembic/versions/ea1c3a5b9c21_enable_pg_trgm_and_index_ad_sets.py
================================================
"""enable_pg_trgm_and_index_ad_sets

Revision ID: ea1c3a5b9c21
Revises: f3e2b1c4a5d6
Create Date: 2025-07-11 12:30:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

revision: str = 'ea1c3a5b9c21'
down_revision: Union[str, None] = 'f3e2b1c4a5d6'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Enable pg_trgm extension (safe if already enabled)
    op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")
    # Create GIN index on content_signature using pg_trgm for similarity search
    op.execute(
        "CREATE INDEX IF NOT EXISTS ix_ad_sets_content_signature_gin "
        "ON ad_sets USING gin (content_signature gin_trgm_ops);"
    )


def downgrade() -> None:
    # Drop the GIN index
    op.execute("DROP INDEX IF EXISTS ix_ad_sets_content_signature_gin;")
    # Optionally keep pg_trgm extension (safe); do not drop to avoid affecting other objects 


================================================
File: alembic/versions/f2b75a2c1596_add_is_active_field_to_adset.py
================================================
"""add_is_active_field_to_adset

Revision ID: f2b75a2c1596
Revises: df763544010c
Create Date: 2025-07-11 08:05:30.722222

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'f2b75a2c1596'
down_revision: Union[str, None] = 'df763544010c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    pass


def downgrade() -> None:
    pass



================================================
File: alembic/versions/f3e2b1c4a5d6_add_ad_set_id_to_ads.py
================================================
"""add_ad_set_id_to_ads

Revision ID: f3e2b1c4a5d6
Revises: 8965d492a259
Create Date: 2025-07-11 12:00:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision: str = 'f3e2b1c4a5d6'
down_revision: Union[str, None] = '8965d492a259'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Add ad_set_id column to ads
    op.add_column('ads', sa.Column('ad_set_id', sa.Integer(), nullable=True))
    # Create foreign key constraint
    op.create_foreign_key(
        'ads_ad_set_id_fkey',
        source_table='ads',
        referent_table='ad_sets',
        local_cols=['ad_set_id'],
        remote_cols=['id'],
        ondelete='SET NULL'
    )
    # Create index
    op.create_index('ix_ads_ad_set_id', 'ads', ['ad_set_id'])


def downgrade() -> None:
    op.drop_index('ix_ads_ad_set_id', table_name='ads')
    op.drop_constraint('ads_ad_set_id_fkey', 'ads', type_='foreignkey')
    op.drop_column('ads', 'ad_set_id') 


================================================
File: app/__init__.py
================================================
# Empty file to make this directory a Python package 


================================================
File: app/celery.py
================================================
from celery import Celery
import os
from dotenv import load_dotenv

load_dotenv()

# Create Celery instance
celery = Celery(
    "ads_app",
    broker=os.getenv("CELERY_BROKER_URL", "redis://localhost:6379"),
    backend=os.getenv("CELERY_RESULT_BACKEND", "redis://localhost:6379"),
    include=[
        "app.tasks.basic_tasks",
        "app.tasks.ai_analysis_tasks", 
        "app.tasks.facebook_ads_scraper_task"
    ]
)

# Celery configuration
celery.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutes
    task_soft_time_limit=25 * 60,  # 25 minutes
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Configure periodic tasks - using actual registered tasks
celery.conf.beat_schedule = {
    'scrape-facebook-ads': {
        'task': 'app.tasks.facebook_ads_scraper_task.scrape_facebook_ads_task',
        'schedule': 3600.0,  # Run every hour
    },
}

if __name__ == "__main__":
    celery.start() 


================================================
File: app/celery_worker.py
================================================
from celery import Celery
from app.core.config import settings
import logging

# Configure logging
logging.basicConfig(level=getattr(logging, settings.LOG_LEVEL))
logger = logging.getLogger(__name__)

# Create Celery instance
celery_app = Celery(
    "ads_worker",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND,
    include=[
        "app.tasks.basic_tasks", 
        "app.tasks.ai_analysis_tasks", 
        "app.tasks.facebook_ads_scraper_task"
    ]
)

# Export the celery app for the CLI
app = celery_app

# Configure Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=settings.CELERY_TASK_TIME_LIMIT,
    task_soft_time_limit=settings.CELERY_TASK_SOFT_TIME_LIMIT,
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
    worker_log_format="[%(asctime)s: %(levelname)s/%(processName)s] %(message)s",
    worker_task_log_format="[%(asctime)s: %(levelname)s/%(processName)s][%(task_name)s(%(task_id)s)] %(message)s",
    broker_connection_retry_on_startup=True,
)

# Configure periodic tasks (Celery Beat) - using actual registered tasks
celery_app.conf.beat_schedule = {
    'scrape-facebook-ads': {
        'task': 'facebook_ads_scraper.scrape_ads',
        'schedule': 3600.0,  # Run every hour
    },
    'batch-ai-analysis': {
        'task': 'app.tasks.ai_analysis_tasks.batch_ai_analysis_task',
        'schedule': 21600.0,  # Run every 6 hours
        'kwargs': {'ad_ids': []}  # Empty list as default
    },
}

# Add Redis broker configuration
celery_app.conf.broker_transport_options = {
    'visibility_timeout': 3600,
    'fanout_prefix': True,
    'fanout_patterns': True
}

if __name__ == "__main__":
    logger.info("Starting Celery worker...")
    celery_app.start() 


================================================
File: app/database.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

# Database configuration
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@localhost:5432/ads_db")

# Create engine
engine = create_engine(DATABASE_URL)

# Create SessionLocal class
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create Base class
Base = declarative_base()

# Dependency to get DB session
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close() 


================================================
File: app/main.py
================================================
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import uvicorn
from pathlib import Path
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import configuration
from app.core.config import settings

# Import routers
from app.routers import health, ads, competitors
from app.api import internal_router

# Database imports
from app.database import engine, Base

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    Base.metadata.create_all(bind=engine)
    yield
    # Shutdown
    pass

# Create FastAPI app
app = FastAPI(
    title=settings.APP_NAME,
    description="A comprehensive API for managing Facebook Ads data with AI-powered analysis",
    version=settings.APP_VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(health.router, prefix=settings.API_V1_PREFIX, tags=["health"])
app.include_router(ads.router, prefix=settings.API_V1_PREFIX, tags=["ads"])
app.include_router(competitors.router, prefix=f"{settings.API_V1_PREFIX}/competitors", tags=["competitors"])
app.include_router(internal_router, prefix=settings.API_V1_PREFIX, tags=["internal"])

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": f"Welcome to {settings.APP_NAME}",
        "version": settings.APP_VERSION,
        "docs": "/docs",
        "redoc": "/redoc",
        "health": f"{settings.API_V1_PREFIX}/health"
    }

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"detail": f"Internal server error: {str(exc)}"}
    )

if __name__ == "__main__":
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG
    ) 



================================================
File: app/api/__init__.py
================================================
from .internal_api import router as internal_router

__all__ = ["internal_router"] 


================================================
File: app/api/internal_api.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Header, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.orm import Session
from typing import List, Optional
import logging

from app.database import get_db
from app.core.config import settings
from app.services.ingestion_service import DataIngestionService
from app.models.dto import AdCreate, AdIngestionResponse

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/internal", tags=["internal"])

# Security
security = HTTPBearer(auto_error=False)

# API Key for internal services (can be set via environment variable)
INTERNAL_API_KEY = settings.INTERNAL_API_KEY


async def verify_api_key(
    authorization: Optional[HTTPAuthorizationCredentials] = Depends(security),
    x_api_key: Optional[str] = Header(None)
) -> bool:
    """
    Verify API key for internal endpoints.
    Supports both Bearer token and X-API-Key header methods.
    """
    provided_key = None
    
    # Check Bearer token first
    if authorization:
        provided_key = authorization.credentials
    
    # Check X-API-Key header as fallback
    elif x_api_key:
        provided_key = x_api_key
    
    if not provided_key:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="API key required. Provide via Authorization header (Bearer token) or X-API-Key header"
        )
    
    if provided_key != INTERNAL_API_KEY:
        logger.warning(f"Invalid API key attempt: {provided_key[:10]}...")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API key"
        )
    
    return True


@router.post("/ingest", response_model=AdIngestionResponse)
async def ingest_ad(
    ad_data: AdCreate,
    db: Session = Depends(get_db),
    _: bool = Depends(verify_api_key)
) -> AdIngestionResponse:
    """
    Ingest a single ad from external scraper.
    
    This is the main endpoint that external scraping scripts should use
    to submit raw ad data to the system.
    
    **Security**: Requires API key authentication.
    
    **Process**:
    1. Validates incoming ad data
    2. Creates/updates competitor record
    3. Creates/updates ad record
    4. Triggers AI analysis task
    
    **Example Usage**:
    ```bash
    curl -X POST "http://localhost:8000/api/v1/internal/ingest" \
         -H "X-API-Key: your-api-key" \
         -H "Content-Type: application/json" \
         -d '{
           "ad_archive_id": "1234567890",
           "competitor": {
             "name": "Example Company",
             "page_id": "example123"
           },
           "ad_copy": "Amazing product!",
           "media_type": "image"
         }'
    ```
    """
    logger.info(f"Received ingestion request for ad: {ad_data.ad_archive_id}")
    
    try:
        # Create ingestion service instance
        ingestion_service = DataIngestionService(db)
        
        # Process the ad ingestion
        result = await ingestion_service.ingest_ad(ad_data)
        
        if result.success:
            logger.info(f"Successfully ingested ad: {ad_data.ad_archive_id} (ID: {result.ad_id})")
        else:
            logger.warning(f"Failed to ingest ad: {ad_data.ad_archive_id} - {result.message}")
        
        return result
        
    except Exception as e:
        logger.error(f"Unexpected error in ingest endpoint: {e}")
        return AdIngestionResponse(
            success=False,
            ad_id=None,
            competitor_id=None,
            analysis_task_id=None,
            message=f"Internal server error: {str(e)}"
        )


@router.post("/ingest/batch")
async def batch_ingest_ads(
    ads_data: List[AdCreate],
    db: Session = Depends(get_db),
    _: bool = Depends(verify_api_key)
) -> dict:
    """
    Batch ingest multiple ads from external scraper.
    
    This endpoint allows efficient ingestion of multiple ads at once,
    useful for bulk scraping operations.
    
    **Security**: Requires API key authentication.
    
    **Limits**: Maximum 100 ads per batch request.
    
    **Example Usage**:
    ```bash
    curl -X POST "http://localhost:8000/api/v1/internal/ingest/batch" \
         -H "X-API-Key: your-api-key" \
         -H "Content-Type: application/json" \
         -d '[
           {
             "ad_archive_id": "1234567890",
             "competitor": {"name": "Company A", "page_id": "compA123"},
             "ad_copy": "Product A"
           },
           {
             "ad_archive_id": "1234567891", 
             "competitor": {"name": "Company B", "page_id": "compB123"},
             "ad_copy": "Product B"
           }
         ]'
    ```
    """
    # Validate batch size
    if len(ads_data) > 100:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Batch size too large. Maximum 100 ads per request."
        )
    
    if len(ads_data) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Empty batch. At least one ad required."
        )
    
    logger.info(f"Received batch ingestion request for {len(ads_data)} ads")
    
    try:
        # Create ingestion service instance
        ingestion_service = DataIngestionService(db)
        
        # Process the batch ingestion
        result = await ingestion_service.batch_ingest_ads(ads_data)
        
        logger.info(f"Batch ingestion completed: {result['successful']}/{result['total_ads']} successful")
        
        return result
        
    except Exception as e:
        logger.error(f"Unexpected error in batch ingest endpoint: {e}")
        return {
            "success": False,
            "error": f"Internal server error: {str(e)}",
            "total_ads": len(ads_data),
            "successful": 0,
            "failed": len(ads_data)
        }


@router.get("/stats")
async def get_ingestion_stats(
    db: Session = Depends(get_db),
    _: bool = Depends(verify_api_key)
) -> dict:
    """
    Get ingestion statistics.
    
    **Security**: Requires API key authentication.
    
    Returns statistics about ingested ads, competitors, and recent activity.
    """
    logger.info("Fetching ingestion statistics")
    
    try:
        ingestion_service = DataIngestionService(db)
        stats = await ingestion_service.get_ingestion_stats()
        
        return stats
        
    except Exception as e:
        logger.error(f"Error fetching ingestion stats: {e}")
        return {
            "error": f"Failed to fetch statistics: {str(e)}"
        }


@router.get("/health")
async def internal_health_check(_: bool = Depends(verify_api_key)) -> dict:
    """
    Internal health check endpoint.
    
    **Security**: Requires API key authentication.
    
    Used by internal services to verify the ingestion API is available.
    """
    return {
        "status": "ok",
        "service": "internal-ingestion-api",
        "endpoints": [
            "POST /internal/ingest - Single ad ingestion",
            "POST /internal/ingest/batch - Batch ad ingestion", 
            "GET /internal/stats - Ingestion statistics",
            "GET /internal/health - This health check"
        ]
    }


@router.get("/api-key/test")
async def test_api_key(_: bool = Depends(verify_api_key)) -> dict:
    """
    Test API key authentication.
    
    **Security**: Requires API key authentication.
    
    Useful for testing if your API key is working correctly.
    """
    return {
        "status": "authenticated",
        "message": "API key is valid",
        "note": "You can now use the ingestion endpoints"
    }

# Debug endpoint removed for security 


================================================
File: app/core/config.py
================================================
import os
from typing import List


class Settings:
    """
    Application settings and configuration management.
    Uses simple environment variable access for now.
    """
    
    # Application Settings
    APP_NAME: str = "Ads Management API"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = os.getenv("DEBUG", "true").lower() == "true"
    
    # Security
    SECRET_KEY: str = os.getenv("SECRET_KEY", "your-secret-key-here-change-in-production")
    INTERNAL_API_KEY: str = os.getenv("INTERNAL_API_KEY", os.getenv("SECRET_KEY", "your-secret-key-here-change-in-production") + "-internal")
    
    # Database Configuration
    DATABASE_URL: str = os.getenv("DATABASE_URL", "postgresql://ads_user:ads_password@db:5432/ads_db")
    
    # Redis Configuration
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://redis:6379")
    
    # Celery Configuration
    CELERY_BROKER_URL: str = os.getenv("CELERY_BROKER_URL", "redis://redis:6379")
    CELERY_RESULT_BACKEND: str = os.getenv("CELERY_RESULT_BACKEND", "redis://redis:6379")
    
    # CORS Configuration
    CORS_ORIGINS: List[str] = os.getenv("CORS_ORIGINS", "http://localhost:3000").split(",")
    
    # API Configuration
    API_V1_PREFIX: str = "/api/v1"
    
    # Task Configuration
    CELERY_TASK_TIME_LIMIT: int = 30 * 60  # 30 minutes
    CELERY_TASK_SOFT_TIME_LIMIT: int = 25 * 60  # 25 minutes
    
    # AI Service Configuration
    GOOGLE_AI_API_KEY: str = os.getenv("GOOGLE_AI_API_KEY", "")
    GOOGLE_AI_MODEL: str = os.getenv("GOOGLE_AI_MODEL", "gemini-pro")
    AI_ANALYSIS_ENABLED: bool = os.getenv("AI_ANALYSIS_ENABLED", "true").lower() == "true"
    
    # Logging Configuration
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")


# Create settings instance
settings = Settings()


def get_settings() -> Settings:
    """
    Dependency to get settings instance.
    Can be used in FastAPI dependency injection.
    """
    return settings 



================================================
File: app/models/__init__.py
================================================
from .competitor import Competitor
from .ad import Ad
from .ad_analysis import AdAnalysis
from .task_status import TaskStatus
from .ad_set import AdSet

__all__ = ["Competitor", "Ad", "AdAnalysis", "TaskStatus", "AdSet"] 


================================================
File: app/models/ad.py
================================================
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, JSON, Boolean, func, Float, ARRAY
from sqlalchemy.orm import relationship
from app.database import Base


class Ad(Base):
    __tablename__ = "ads"

    # Core identification fields
    id = Column(Integer, primary_key=True, index=True)
    competitor_id = Column(Integer, ForeignKey("competitors.id"), nullable=False, index=True)
    ad_archive_id = Column(String, unique=True, nullable=False, index=True)
    
    # Basic tracking fields
    date_found = Column(DateTime(timezone=True), nullable=False, index=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)
    
    # Duration field - calculated during scraping
    duration_days = Column(Integer, nullable=True, index=True)  # Number of days the ad has been running
    
    # Raw data from initial scrape
    raw_data = Column(JSON, nullable=True)
    
    # Foreign key to ad set
    ad_set_id = Column(Integer, ForeignKey("ad_sets.id"), nullable=True, index=True)
    
    # Relationships
    competitor = relationship("Competitor", back_populates="ads")
    analysis = relationship("AdAnalysis", uselist=False, back_populates="ad", cascade="all, delete-orphan")
    ad_set = relationship("AdSet", back_populates="ads", foreign_keys=[ad_set_id])
    
    # New structured fields for enhanced extraction
    meta = Column(JSON, nullable=True)
    targeting = Column(JSON, nullable=True)
    lead_form = Column(JSON, nullable=True)
    creatives = Column(JSON, nullable=True)

    def __repr__(self):
        return f"<Ad(id={self.id}, ad_archive_id='{self.ad_archive_id}')>"
    
    def to_dict(self):
        """Convert Ad instance to dictionary for JSON serialization"""
        date_found_iso = None
        if hasattr(self, 'date_found') and self.date_found is not None:
            date_found_iso = self.date_found.isoformat()
            
        created_at_iso = None
        if hasattr(self, 'created_at') and self.created_at is not None:
            created_at_iso = self.created_at.isoformat()
            
        updated_at_iso = None
        if hasattr(self, 'updated_at') and self.updated_at is not None:
            updated_at_iso = self.updated_at.isoformat()
            
        return {
            "id": self.id,
            "competitor_id": self.competitor_id,
            "ad_archive_id": self.ad_archive_id,
            "ad_set_id": self.ad_set_id,
            "date_found": date_found_iso,
            "created_at": created_at_iso,
            "updated_at": updated_at_iso,
            "raw_data": self.raw_data,
            "meta": self.meta,
            "targeting": self.targeting,
            "lead_form": self.lead_form,
            "creatives": self.creatives
        }
    
    def to_enhanced_format(self):
        """Convert to the new enhanced frontend format"""
        return {
            "ad_archive_id": self.ad_archive_id,
            "meta": self.meta,
            "targeting": self.targeting,
            "lead_form": self.lead_form,
            "creatives": self.creatives
        } 


================================================
File: app/models/ad_analysis.py
================================================
from sqlalchemy import Column, Integer, String, Text, Float, DateTime, ForeignKey, JSON, func
from sqlalchemy.orm import relationship
from app.database import Base


class AdAnalysis(Base):
    __tablename__ = "ad_analyses"

    id = Column(Integer, primary_key=True, index=True)
    ad_id = Column(Integer, ForeignKey("ads.id"), unique=True, nullable=False)
    summary = Column(Text, nullable=True)
    hook_score = Column(Float, nullable=True)  # Score for how engaging the hook is
    overall_score = Column(Float, nullable=True)  # Overall ad effectiveness score
    
    # JSONB columns for flexible AI data storage
    ai_prompts = Column(JSON, nullable=True)  # Store the prompts sent to AI
    raw_ai_response = Column(JSON, nullable=True)  # Store complete AI response
    
    # Additional analysis fields
    target_audience = Column(String, nullable=True)
    ad_format_analysis = Column(JSON, nullable=True)  # Analysis of ad format effectiveness
    competitor_insights = Column(JSON, nullable=True)  # Insights about competitor strategy
    content_themes = Column(JSON, nullable=True)  # Identified themes in the ad content
    performance_predictions = Column(JSON, nullable=True)  # Predicted performance metrics
    
    # Metadata
    analysis_version = Column(String, nullable=True)  # Track AI model version used
    confidence_score = Column(Float, nullable=True)  # AI confidence in the analysis
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Relationships
    ad = relationship("Ad", back_populates="analysis")

    def __repr__(self):
        return f"<AdAnalysis(id={self.id}, ad_id={self.ad_id}, overall_score={self.overall_score})>" 


================================================
File: app/models/ad_set.py
================================================
from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, func, Index
from sqlalchemy.orm import relationship
from app.database import Base


class AdSet(Base):
    __tablename__ = "ad_sets"

    # Primary key
    id = Column(Integer, primary_key=True, index=True)
    
    # Content signature - perceptual hash of the representative ad's media (stable visual identifier)
    content_signature = Column(String, unique=True, nullable=False, index=True)
    
    # Count of ad variants in this set
    variant_count = Column(Integer, default=0, nullable=False)

    # Earliest and latest discovery dates among ads in this set
    first_seen_date = Column(DateTime(timezone=True), nullable=True, index=True)
    last_seen_date = Column(DateTime(timezone=True), nullable=True, index=True)
    
    # Reference to the best/representative ad in the set
    best_ad_id = Column(Integer, ForeignKey("ads.id"), nullable=True, index=True)
    
    # Standard timestamp fields
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)
    
    # Relationship to ads
    ads = relationship("Ad", back_populates="ad_set", foreign_keys="Ad.ad_set_id")
    
    # Relationship to the best ad
    best_ad = relationship("Ad", foreign_keys=[best_ad_id], post_update=True)
    
    def __repr__(self):
        return f"<AdSet(id={self.id}, content_signature='{self.content_signature}', variant_count={self.variant_count})>"
    
    def to_dict(self):
        """Convert AdSet instance to dictionary for JSON serialization"""
        created_at_iso = None
        if hasattr(self, 'created_at') and self.created_at is not None:
            created_at_iso = self.created_at.isoformat()
            
        updated_at_iso = None
        if hasattr(self, 'updated_at') and self.updated_at is not None:
            updated_at_iso = self.updated_at.isoformat()
            
        return {
            "id": self.id,
            "content_signature": self.content_signature,
            "variant_count": self.variant_count,
            "best_ad_id": self.best_ad_id,
            "created_at": created_at_iso,
            "updated_at": updated_at_iso
        } 


================================================
File: app/models/competitor.py
================================================
from sqlalchemy import Column, Integer, String, Boolean, DateTime, func
from sqlalchemy.orm import relationship
from app.database import Base


class Competitor(Base):
    __tablename__ = "competitors"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, nullable=False, index=True)
    page_id = Column(String, unique=True, nullable=False, index=True)
    page_url = Column(String, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Relationship to ads
    ads = relationship("Ad", back_populates="competitor")

    def __repr__(self):
        return f"<Competitor(id={self.id}, name='{self.name}', page_id='{self.page_id}')>" 


================================================
File: app/models/task_status.py
================================================
from sqlalchemy import Column, Integer, String, DateTime, JSON, func
from app.database import Base


class TaskStatus(Base):
    """Model for tracking task status and results"""
    __tablename__ = "task_status"

    id = Column(Integer, primary_key=True, index=True)
    task_id = Column(String, unique=True, index=True, nullable=False)
    status = Column(String, nullable=False)  # pending, running, completed, failed
    result = Column(JSON, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    def __repr__(self):
        return f"<TaskStatus(id={self.id}, task_id='{self.task_id}', status='{self.status}')>" 



================================================
File: app/models/dto/__init__.py
================================================
from .ad_dto import AdCreate, CompetitorCreateDTO, AdIngestionResponse
from .competitor_dto import (
    CompetitorCreateDTO,
    CompetitorUpdateDTO,
    CompetitorResponseDTO,
    CompetitorDetailResponseDTO,
    PaginatedCompetitorResponseDTO,
    CompetitorFilterParams,
    CompetitorStatsResponseDTO
)

__all__ = [
    "AdCreate", 
    "CompetitorCreateDTO", 
    "AdIngestionResponse",
    "CompetitorUpdateDTO",
    "CompetitorResponseDTO",
    "CompetitorDetailResponseDTO",
    "PaginatedCompetitorResponseDTO",
    "CompetitorFilterParams",
    "CompetitorStatsResponseDTO"
] 


================================================
File: app/models/dto/ad_dto.py
================================================
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime

from .competitor_dto import CompetitorResponseDTO


class ExtraTextItemDTO(BaseModel):
    """
    DTO for a single item in the extra_texts field.
    """
    text: str
    
    model_config = {"from_attributes": True}


class CompetitorCreateDTO(BaseModel):
    """
    DTO for creating or finding competitor records.
    Used when ingesting ad data to ensure competitor exists.
    """
    name: str = Field(..., description="Competitor/page name")
    page_id: str = Field(..., description="Facebook page ID")
    is_active: bool = Field(default=True, description="Whether competitor is active")
    
    model_config = {
        "json_encoders": {
            datetime: lambda v: v.isoformat()
        }
    }


class AdMeta(BaseModel):
    """
    DTO for ad meta information.
    """
    is_active: Optional[bool] = Field(None, description="Whether the ad is active")
    cta_type: Optional[str] = Field(None, description="Call to action type")
    display_format: Optional[str] = Field(None, description="Display format")
    start_date: Optional[str] = Field(None, description="Start date of the ad")
    end_date: Optional[str] = Field(None, description="End date of the ad")
    
    model_config = {"from_attributes": True}


class AgeRange(BaseModel):
    min: int
    max: int


class Location(BaseModel):
    name: str
    num_obfuscated: int = 0
    type: str
    excluded: bool = False


class GenderAgeBreakdown(BaseModel):
    age_range: str
    male: Optional[int] = None
    female: Optional[int] = None
    unknown: Optional[int] = None


class CountryReachBreakdown(BaseModel):
    country: str
    age_gender_breakdowns: List[GenderAgeBreakdown]


class AdTargeting(BaseModel):
    """
    DTO for ad targeting information.
    """
    locations: Optional[List[str]] = Field(None, description="Targeted locations")
    age_range: Optional[str] = Field(None, description="Targeted age range")
    gender: Optional[str] = Field(None, description="Targeted gender")
    reach_breakdown: Optional[Dict[str, Any]] = Field(None, description="Reach breakdown")
    total_reach: Optional[int] = Field(None, description="Total reach")
    
    model_config = {"from_attributes": True}


class LeadFormQuestion(BaseModel):
    """
    DTO for a lead form question.
    """
    question_id: str = Field(..., description="Question ID")
    question_text: str = Field(..., description="Question text")
    question_type: str = Field(..., description="Question type")
    options: Optional[List[str]] = Field(None, description="Question options")
    
    model_config = {"from_attributes": True}


class LeadForm(BaseModel):
    """
    DTO for lead form information.
    """
    questions: Optional[Dict[str, Any]] = Field(None, description="Lead form questions")
    standalone_fields: Optional[List[str]] = Field(None, description="Standalone fields")
    
    model_config = {"from_attributes": True}


class CreativeMedia(BaseModel):
    """
    DTO for creative media.
    """
    url: str = Field(..., description="Media URL")
    type: str = Field(..., description="Media type")
    
    model_config = {"from_attributes": True}


class CreativeCta(BaseModel):
    text: Optional[str] = None
    type: Optional[str] = None


class CreativeLink(BaseModel):
    url: Optional[str] = None
    caption: Optional[str] = None


class Creative(BaseModel):
    """
    DTO for ad creative.
    """
    id: str = Field(..., description="Creative ID")
    title: Optional[str] = Field(None, description="Creative title")
    body: Optional[str] = Field(None, description="Creative body")
    caption: Optional[str] = Field(None, description="Creative caption")
    link_url: Optional[str] = Field(None, description="Creative link URL")
    link_description: Optional[str] = Field(None, description="Creative link description")
    media: Optional[List[CreativeMedia]] = Field(None, description="Creative media")
    
    model_config = {"from_attributes": True}


class AdBase(BaseModel):
    ad_archive_id: str
    meta: Optional[AdMeta] = None
    targeting: Optional[AdTargeting] = None
    lead_form: Optional[LeadForm] = None
    creatives: List[Creative] = []


class AdCreate(AdBase):
    competitor_id: int


class AdUpdate(BaseModel):
    meta: Optional[AdMeta] = None
    targeting: Optional[AdTargeting] = None
    lead_form: Optional[LeadForm] = None
    creatives: Optional[List[Creative]] = None


class AdInDB(AdBase):
    id: int
    competitor_id: int
    date_found: datetime
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    model_config = {"from_attributes": True}


class AdResponse(AdBase):
    id: int
    competitor_id: int
    date_found: datetime
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    model_config = {"from_attributes": True}


class AdFilterParams(BaseModel):
    page: int = 1
    page_size: int = 20
    limit: int = 20  # Alias for page_size
    competitor_id: Optional[int] = None
    competitor_name: Optional[str] = None
    media_type: Optional[str] = None
    has_analysis: Optional[bool] = None
    min_hook_score: Optional[float] = None
    max_hook_score: Optional[float] = None
    min_overall_score: Optional[float] = None
    max_overall_score: Optional[float] = None
    min_duration_days: Optional[int] = None
    max_duration_days: Optional[int] = None
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None
    is_active: Optional[bool] = None
    search: Optional[str] = None
    sort_by: Optional[str] = "date_found"
    sort_order: Optional[str] = "desc"
    
    # Additional fields referenced in code
    campaign_id: Optional[int] = None
    has_lead_form: Optional[bool] = None
    platform: Optional[str] = None
    query: Optional[str] = None


class PaginatedAdResponse(BaseModel):
    items: List[AdResponse]
    total: int
    page: int
    limit: int
    total_pages: int


class AdvertiserInfo(BaseModel):
    page_id: str
    page_name: str
    page_url: Optional[str] = None
    page_likes: Optional[int] = None
    page_profile_picture: Optional[str] = None


class Campaign(BaseModel):
    campaign_id: str
    platforms: List[str] = []
    ads: List[AdResponse] = []


class AdsFullResponse(BaseModel):
    advertiser_info: AdvertiserInfo
    campaigns: List[Campaign] = []


class AdStats(BaseModel):
    total_ads: int = 0
    active_ads: int = 0
    with_lead_form: int = 0
    platforms: Dict[str, int] = {}
    media_types: Dict[str, int] = {}


class AdIngestionResponse(BaseModel):
    """
    Response model for ad ingestion operations.
    """
    success: bool = Field(..., description="Whether ingestion was successful")
    ad_id: Optional[int] = Field(None, description="ID of created/updated ad")
    competitor_id: Optional[int] = Field(None, description="ID of associated competitor")
    analysis_task_id: Optional[str] = Field(None, description="ID of triggered analysis task")
    message: str = Field(..., description="Human-readable result message")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "success": True,
                "ad_id": 123,
                "competitor_id": 456,
                "analysis_task_id": "some-task-id",
                "message": "Ad ingested successfully and analysis task started."
            }
        }
    }


class AdAnalysisResponseDTO(BaseModel):
    """
    DTO for AI analysis data in ad responses.
    """
    id: int = Field(..., description="Analysis ID")
    summary: Optional[str] = Field(None, description="AI-generated summary")
    hook_score: Optional[float] = Field(None, description="Hook effectiveness score (1-10)")
    overall_score: Optional[float] = Field(None, description="Overall ad effectiveness score (1-10)")
    confidence_score: Optional[float] = Field(None, description="AI confidence level (0-1)")
    target_audience: Optional[str] = Field(None, description="Target audience description")
    content_themes: Optional[List[str]] = Field(None, description="Identified content themes")
    analysis_version: Optional[str] = Field(None, description="Analysis version used")
    created_at: datetime = Field(..., description="Analysis creation timestamp")
    updated_at: datetime = Field(..., description="Analysis update timestamp")
    
    model_config = {"from_attributes": True}


class AdResponseDTO(BaseModel):
    """
    DTO for ad data in list responses.
    """
    id: int = Field(..., description="Ad ID")
    ad_archive_id: str = Field(..., description="Facebook Ad Library ID")
    competitor: CompetitorResponseDTO = Field(..., description="Competitor information")
    
    # Ad content
    ad_copy: Optional[str] = Field(None, description="Main ad text/copy")
    main_title: Optional[str] = Field(None, description="Main title")
    main_body_text: Optional[str] = Field(None, description="Main body text")
    main_caption: Optional[str] = Field(None, description="Main caption")
    
    # Media information
    media_type: Optional[str] = Field(None, description="Type of media")
    media_url: Optional[str] = Field(None, description="Primary media URL")
    main_image_urls: Optional[List[str]] = Field(None, description="Main image URLs")
    main_video_urls: Optional[List[str]] = Field(None, description="Main video URLs")
    
    # Page information
    page_name: Optional[str] = Field(None, description="Page name")
    page_id: Optional[str] = Field(None, description="Page ID")
    
    # Platform and targeting
    publisher_platform: Optional[List[str]] = Field(None, description="Publisher platforms")
    targeted_countries: Optional[List[str]] = Field(None, description="Targeted countries")
    
    # Performance indicators
    impressions_text: Optional[str] = Field(None, description="Impressions text")
    spend: Optional[str] = Field(None, description="Spend amount")
    
    # Call-to-action
    cta_text: Optional[str] = Field(None, description="CTA text")
    cta_type: Optional[str] = Field(None, description="CTA type")
    
    # Dates
    date_found: datetime = Field(..., description="When ad was discovered")
    start_date: Optional[str] = Field(None, description="Ad start date")
    end_date: Optional[str] = Field(None, description="Ad end date")
    is_active: Optional[bool] = Field(None, description="Whether ad is active")
    duration_days: Optional[int] = Field(None, description="Duration in days the ad has been running")
    
    # Timestamps
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Update timestamp")
    
    # AI Analysis (optional)
    analysis: Optional[AdAnalysisResponseDTO] = Field(None, description="AI analysis data")
    
    # New fields to match frontend_payload_final.json
    meta: Optional[AdMeta] = Field(None, description="Meta information about the ad")
    targeting: Optional[AdTargeting] = Field(None, description="Targeting information")
    lead_form: Optional[LeadForm] = Field(None, description="Lead form information")
    creatives: List[Creative] = Field(default_factory=list, description="Ad creatives")
    
    # AdSet fields
    ad_set_id: Optional[int] = Field(None, description="ID of the ad set this ad belongs to")
    variant_count: Optional[int] = Field(None, description="Number of variants in the ad set")
    ad_set_created_at: Optional[datetime] = Field(None, description="Timestamp when the ad set was first created")

    # Date range for the entire ad set
    ad_set_first_seen_date: Optional[datetime] = Field(None, description="The date the oldest ad in this set was found")
    ad_set_last_seen_date: Optional[datetime] = Field(None, description="The date the newest ad in this set was found")
    
    model_config = {"from_attributes": True}


class AdDetailResponseDTO(BaseModel):
    """
    DTO for detailed ad data in single ad responses.
    """
    id: int = Field(..., description="Ad ID")
    ad_archive_id: str = Field(..., description="Facebook Ad Library ID")
    competitor: CompetitorResponseDTO = Field(..., description="Competitor information")
    
    # All ad content
    ad_copy: Optional[str] = Field(None, description="Main ad text/copy")
    main_title: Optional[str] = Field(None, description="Main title")
    main_body_text: Optional[str] = Field(None, description="Main body text")
    main_caption: Optional[str] = Field(None, description="Main caption")
    main_link_url: Optional[str] = Field(None, description="Main link URL")
    main_link_description: Optional[str] = Field(None, description="Main link description")
    
    # Media information
    media_type: Optional[str] = Field(None, description="Type of media")
    media_url: Optional[str] = Field(None, description="Primary media URL")
    main_image_urls: Optional[List[str]] = Field(None, description="Main image URLs")
    main_video_urls: Optional[List[str]] = Field(None, description="Main video URLs")
    
    # Page information
    page_name: Optional[str] = Field(None, description="Page name")
    page_id: Optional[str] = Field(None, description="Page ID")
    page_like_count: Optional[int] = Field(None, description="Page like count")
    page_categories: Optional[List[str]] = Field(None, description="Page categories")
    page_profile_uri: Optional[str] = Field(None, description="Page profile URI")
    page_profile_picture_url: Optional[str] = Field(None, description="Page profile picture URL")
    
    # Platform and targeting
    publisher_platform: Optional[List[str]] = Field(None, description="Publisher platforms")
    targeted_countries: Optional[List[str]] = Field(None, description="Targeted countries")
    display_format: Optional[str] = Field(None, description="Display format")
    
    # Performance indicators
    impressions_text: Optional[str] = Field(None, description="Impressions text")
    impressions_index: Optional[int] = Field(None, description="Impressions index")
    spend: Optional[str] = Field(None, description="Spend amount")
    currency: Optional[str] = Field(None, description="Currency")
    
    # Call-to-action
    cta_text: Optional[str] = Field(None, description="CTA text")
    cta_type: Optional[str] = Field(None, description="CTA type")
    
    # Additional content
    extra_texts: Optional[List[ExtraTextItemDTO]] = Field(None, description="Extra text content")
    extra_links: Optional[List[str]] = Field(None, description="Extra links")
    
    # Content flags
    contains_sensitive_content: Optional[bool] = Field(None, description="Contains sensitive content")
    contains_digital_created_media: Optional[bool] = Field(None, description="Contains digital created media")
    
    # Dates
    date_found: datetime = Field(..., description="When ad was discovered")
    start_date: Optional[str] = Field(None, description="Ad start date")
    end_date: Optional[str] = Field(None, description="Ad end date")
    is_active: Optional[bool] = Field(None, description="Whether ad is active")
    
    # Timestamps
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Update timestamp")
    
    # AI Analysis (optional)
    analysis: Optional[AdAnalysisResponseDTO] = Field(None, description="AI analysis data")
    
    # Raw data (optional)
    raw_data: Optional[Dict[str, Any]] = Field(None, description="Raw API response data")
    
    # New fields to match frontend_payload_final.json
    meta: Optional[AdMeta] = Field(None, description="Meta information about the ad")
    targeting: Optional[AdTargeting] = Field(None, description="Targeting information")
    lead_form: Optional[LeadForm] = Field(None, description="Lead form information")
    creatives: List[Creative] = Field(default_factory=list, description="Ad creatives")
    
    model_config = {"from_attributes": True}


class PaginationMetadata(BaseModel):
    """
    DTO for pagination metadata.
    """
    page: int = Field(..., description="Current page number")
    page_size: int = Field(..., description="Number of items per page")
    total_items: int = Field(..., description="Total number of items")
    total_pages: int = Field(..., description="Total number of pages")
    has_next: bool = Field(..., description="Whether there's a next page")
    has_previous: bool = Field(..., description="Whether there's a previous page")


class PaginatedAdResponseDTO(BaseModel):
    """
    DTO for paginated ad responses.
    """
    data: List[AdResponseDTO] = Field(..., description="List of ads")
    pagination: PaginationMetadata = Field(..., description="Pagination metadata")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "data": [
                    {
                        "id": 1,
                        "ad_archive_id": "1557310628577134",
                        "competitor": {
                            "page_id": "1591077094491398",
                            "page_name": "Binghatti"
                        },
                        "ad_copy": "Join Binghatti Properties at Cityscape Global Dubai and explore our latest real estate developments.",
                        "main_title": "Cityscape Global Dubai",
                        "date_found": "2023-07-19T12:00:00Z",
                        "start_date": "2023-07-01",
                        "end_date": None,
                        "is_active": True,
                        "created_at": "2023-07-19T12:00:00Z",
                        "updated_at": "2023-07-19T12:00:00Z"
                    }
                ],
                "pagination": {
                    "page": 1,
                    "page_size": 20,
                    "total_items": 150,
                    "total_pages": 8,
                    "has_next": True,
                    "has_previous": False
                }
            }
        }
    }


class AdStatsResponseDTO(BaseModel):
    """
    DTO for ad statistics response.
    """
    total_ads: int = Field(..., description="Total number of ads")
    active_ads: int = Field(..., description="Number of active ads")
    total_competitors: int = Field(..., description="Total number of competitors")
    active_competitors: int = Field(..., description="Number of active competitors")
    analyzed_ads: int = Field(..., description="Number of ads with AI analysis")
    analysis_coverage: float = Field(..., description="Percentage of ads with analysis")
    
    # Media type breakdown
    media_type_breakdown: Dict[str, int] = Field(..., description="Breakdown by media type")
    
    # Platform breakdown
    platform_breakdown: Dict[str, int] = Field(..., description="Breakdown by platform")
    
    # Recent activity
    recent_ads_7_days: int = Field(..., description="Ads added in last 7 days")
    recent_ads_30_days: int = Field(..., description="Ads added in last 30 days")
    
    # Analysis scores
    avg_hook_score: Optional[float] = Field(None, description="Average hook score")
    avg_overall_score: Optional[float] = Field(None, description="Average overall score")
    
    # Last update
    last_updated: datetime = Field(..., description="When stats were last updated")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "total_ads": 1250,
                "active_ads": 890,
                "total_competitors": 45,
                "active_competitors": 38,
                "analyzed_ads": 1100,
                "analysis_coverage": 88.0,
                "media_type_breakdown": {
                    "image": 650,
                    "video": 400,
                    "carousel": 200
                },
                "platform_breakdown": {
                    "facebook": 800,
                    "instagram": 450
                },
                "recent_ads_7_days": 75,
                "recent_ads_30_days": 320,
                "avg_hook_score": 7.2,
                "avg_overall_score": 6.8,
                "last_updated": "2023-09-01T10:15:00Z"
            }
        }
    } 


================================================
File: app/models/dto/competitor_dto.py
================================================
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime


class CompetitorCreateDTO(BaseModel):
    """DTO for creating a new competitor"""
    name: str = Field(..., description="Competitor name", min_length=1, max_length=255)
    page_id: str = Field(..., description="Facebook page ID", min_length=1, max_length=100)
    is_active: bool = Field(True, description="Whether the competitor is active")


class CompetitorUpdateDTO(BaseModel):
    """DTO for updating an existing competitor"""
    name: Optional[str] = Field(None, description="Competitor name", min_length=1, max_length=255)
    page_id: Optional[str] = Field(None, description="Facebook page ID", min_length=1, max_length=100)
    is_active: Optional[bool] = Field(None, description="Whether the competitor is active")


class CompetitorResponseDTO(BaseModel):
    """DTO for competitor response data"""
    id: int
    name: str
    page_id: str
    page_url: Optional[str] = None
    is_active: bool
    ads_count: Optional[int] = Field(0, description="Number of ads for this competitor")
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    model_config = {"from_attributes": True}


class CompetitorDetailResponseDTO(BaseModel):
    """DTO for detailed competitor response with additional stats"""
    id: int
    name: str
    page_id: str
    is_active: bool
    ads_count: int = Field(0, description="Total number of ads")
    active_ads_count: int = Field(0, description="Number of active ads")
    analyzed_ads_count: int = Field(0, description="Number of analyzed ads")
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    model_config = {"from_attributes": True}


class PaginatedCompetitorResponseDTO(BaseModel):
    """DTO for paginated competitor response"""
    data: List[CompetitorResponseDTO]
    total: int
    page: int
    page_size: int
    total_pages: int
    has_next: bool
    has_previous: bool


class CompetitorFilterParams(BaseModel):
    """DTO for competitor filtering parameters"""
    page: int = Field(1, ge=1, description="Page number")
    page_size: int = Field(20, ge=1, le=100, description="Number of items per page")
    is_active: Optional[bool] = Field(None, description="Filter by active status")
    search: Optional[str] = Field(None, description="Search in competitor names")
    sort_by: Optional[str] = Field("created_at", description="Sort by field")
    sort_order: Optional[str] = Field("desc", description="Sort order (asc/desc)")


class CompetitorStatsResponseDTO(BaseModel):
    """DTO for competitor statistics"""
    total_competitors: int
    active_competitors: int
    inactive_competitors: int
    competitors_with_ads: int
    total_ads_across_competitors: int
    avg_ads_per_competitor: float 



================================================
File: app/routers/__init__.py
================================================
# Empty file to make this directory a Python package 


================================================
File: app/routers/ads.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List, Optional, TYPE_CHECKING, Dict
from datetime import datetime
import logging
from pydantic import BaseModel, Field
import uuid
import json

from app.database import get_db
from app.models.dto.ad_dto import (
    PaginatedAdResponseDTO,
    AdDetailResponseDTO,
    AdFilterParams,
    AdStatsResponseDTO,
    AdResponseDTO
)
from app.models import Ad, TaskStatus, Competitor
from app.services.facebook_ads_scraper import FacebookAdsScraperService, FacebookAdsScraperConfig
from app.services.ingestion_service import DataIngestionService
from app.services.enhanced_ad_extraction import EnhancedAdExtractionService

# Import Celery tasks
from app.tasks.basic_tasks import add_together, test_task, long_running_task
from app.tasks.facebook_ads_scraper_task import scrape_facebook_ads_task, scrape_competitor_ads_task
from app.tasks.ai_analysis_tasks import ai_analysis_task, batch_ai_analysis_task
from app.celery_worker import celery_app

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    from app.services.ad_service import AdService

router = APIRouter()
logger = logging.getLogger(__name__)

# ========================================
# Request/Response Models
# ========================================

class BulkDeleteRequest(BaseModel):
    ad_ids: List[int]

class TaskResponse(BaseModel):
    task_id: str
    status: str
    message: str

class FacebookAdsScraperRequest(BaseModel):
    """
    Request model for scraping Facebook ads.
    """
    view_all_page_id: Optional[str] = "1591077094491398"
    countries: Optional[List[str]] = ["AE"]
    max_pages: Optional[int] = 10
    delay_between_requests: Optional[int] = 1
    active_status: Optional[str] = "active"
    ad_type: Optional[str] = "ALL"
    media_type: Optional[str] = "all"
    search_type: Optional[str] = "page"
    query_string: Optional[str] = ""
    save_json: Optional[bool] = False

class CompetitorAdsScraperRequest(BaseModel):
    """
    Request model for scraping ads from a specific competitor page.
    """
    competitor_page_id: str
    countries: Optional[List[str]] = ["AE"]
    max_pages: Optional[int] = 5
    delay_between_requests: Optional[int] = 1
    active_status: Optional[str] = "active"
    ad_type: Optional[str] = "ALL"
    media_type: Optional[str] = "all"
    date_from: Optional[str] = None  # YYYY-MM-DD
    date_to: Optional[str] = None
    save_json: Optional[bool] = False

class AddNumbersRequest(BaseModel):
    x: int
    y: int

class TestTaskRequest(BaseModel):
    message: Optional[str] = "Hello from API!"

class ReprocessAdsRequest(BaseModel):
    """
    Request model for reprocessing ads
    
    ad_ids: Optional list of ad IDs to reprocess. If None, all ads with raw data will be reprocessed.
    """
    ad_ids: Optional[List[str]] = None

class DeleteAllAdsResponse(BaseModel):
    message: str
    deleted_count: int

# Dependency factory to avoid circular imports
def get_ad_service_dependency(db: Session = Depends(get_db)) -> "AdService":
    """Factory function to create AdService instance for dependency injection."""
    from app.services.ad_service import AdService
    return AdService(db)

# ========================================
# Main Dashboard API Endpoints
# ========================================

@router.get("/ads", response_model=PaginatedAdResponseDTO)
async def get_ads(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    competitor_id: Optional[int] = Query(None, description="Filter by competitor ID"),
    competitor_name: Optional[str] = Query(None, description="Filter by competitor name"),
    media_type: Optional[str] = Query(None, description="Filter by media type"),
    has_analysis: Optional[bool] = Query(None, description="Filter by analysis availability"),
    min_hook_score: Optional[float] = Query(None, ge=0, le=10, description="Minimum hook score"),
    max_hook_score: Optional[float] = Query(None, ge=0, le=10, description="Maximum hook score"),
    min_overall_score: Optional[float] = Query(None, ge=0, le=10, description="Minimum overall score"),
    max_overall_score: Optional[float] = Query(None, ge=0, le=10, description="Maximum overall score"),
    min_duration_days: Optional[int] = Query(None, ge=1, description="Minimum duration in days"),
    max_duration_days: Optional[int] = Query(None, ge=1, description="Maximum duration in days"),
    date_from: Optional[datetime] = Query(None, description="Filter ads from this date"),
    date_to: Optional[datetime] = Query(None, description="Filter ads to this date"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    search: Optional[str] = Query(None, description="Search in ad copy and titles"),
    sort_by: Optional[str] = Query("created_at", description="Sort by field"),
    sort_order: Optional[str] = Query("desc", description="Sort order (asc/desc)"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
) -> PaginatedAdResponseDTO:
    """
    Get paginated and filtered ads with AI analysis data.
    
    This is the main endpoint for the dashboard to fetch ads with comprehensive
    filtering, pagination, and sorting capabilities.
    """
    try:
        # Create filter parameters
        filters = AdFilterParams(
            page=page,
            page_size=page_size,
            competitor_id=competitor_id,
            competitor_name=competitor_name,
            media_type=media_type,
            has_analysis=has_analysis,
            min_hook_score=min_hook_score,
            max_hook_score=max_hook_score,
            min_overall_score=min_overall_score,
            max_overall_score=max_overall_score,
            min_duration_days=min_duration_days,
            max_duration_days=max_duration_days,
            date_from=date_from,
            date_to=date_to,
            is_active=is_active,
            search=search,
            sort_by=sort_by,
            sort_order=sort_order
        )
        
        # Get ads using service
        result = ad_service.get_ads(filters)
        
        logger.info(f"Retrieved {len(result.data)} ads for page {page}")
        return result
        
    except Exception as e:
        logger.error(f"Error fetching ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ads: {str(e)}")

@router.get("/ads/{ad_id}", response_model=AdDetailResponseDTO)
async def get_ad(
    ad_id: int, 
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get detailed information for a specific ad including AI analysis.
    """
    try:
        ad = ad_service.get_ad_by_id(ad_id)
        
        if not ad:
            raise HTTPException(status_code=404, detail="Ad not found")
        
        return ad
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching ad {ad_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ad: {str(e)}")

@router.get("/ads/stats/overview", response_model=AdStatsResponseDTO)
async def get_ads_stats(
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get comprehensive statistics about ads and analysis for the dashboard.
    """
    try:
        stats = ad_service.get_ad_stats()
        return stats
        
    except Exception as e:
        logger.error(f"Error fetching ad stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ad stats: {str(e)}")

@router.get("/ads/search", response_model=List[AdResponseDTO])
async def search_ads(
    q: str = Query(..., description="Search query"),
    limit: int = Query(50, ge=1, le=100, description="Maximum number of results"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Search ads by text content in ad copy, titles, and page names.
    """
    try:
        results = ad_service.search_ads(q, limit)
        return results
        
    except Exception as e:
        logger.error(f"Error searching ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error searching ads: {str(e)}")

@router.get("/ads/top-performing", response_model=List[AdResponseDTO])
async def get_top_performing_ads(
    limit: int = Query(10, ge=1, le=50, description="Number of top ads to return"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get top performing ads based on AI analysis scores.
    """
    try:
        top_ads = ad_service.get_top_performing_ads(limit)
        return top_ads
        
    except Exception as e:
        logger.error(f"Error fetching top performing ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching top performing ads: {str(e)}")

@router.get("/ads/competitor/{competitor_id}", response_model=List[AdResponseDTO])
async def get_competitor_ads(
    competitor_id: int,
    limit: int = Query(50, ge=1, le=100, description="Maximum number of ads to return"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get ads for a specific competitor.
    """
    try:
        ads = ad_service.get_competitor_ads(competitor_id, limit)
        return ads
        
    except Exception as e:
        logger.error(f"Error fetching competitor ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor ads: {str(e)}")

@router.delete("/ads/bulk")
async def bulk_delete_ads(
    request: BulkDeleteRequest,
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Delete multiple ads by IDs.
    """
    try:
        if not request.ad_ids:
            raise HTTPException(status_code=400, detail="No ad IDs provided")
        
        deleted_count = ad_service.bulk_delete_ads(request.ad_ids)
        
        return {
            "message": f"Successfully deleted {deleted_count} ads",
            "deleted_count": deleted_count,
            "requested_count": len(request.ad_ids)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error bulk deleting ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error bulk deleting ads: {str(e)}")

@router.delete("/ads/{ad_id}")
async def delete_ad(
    ad_id: int,
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Delete an ad by ID.
    """
    try:
        result = ad_service.delete_ad(ad_id)
        logger.info(f"Deleted ad ID {ad_id}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting ad {ad_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting ad: {str(e)}")

@router.delete("/ads/all", response_model=DeleteAllAdsResponse)
async def delete_all_ads(
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Delete all ads from the database.
    
    DANGER: This endpoint is for development purposes only!
    It will delete all ads and their analyses from the database.
    """
    try:
        count = ad_service.delete_all_ads()
        return DeleteAllAdsResponse(
            message=f"Successfully deleted all {count} ads",
            deleted_count=count
        )
    except Exception as e:
        logger.error(f"Error deleting all ads: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting all ads: {str(e)}")

# ========================================
# AI Analysis Endpoints
# ========================================

@router.post("/ads/{ad_id}/analyze")
async def trigger_ad_analysis(
    ad_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Manually trigger AI analysis for a specific ad.
    """
    try:
        # Check if ad exists
        ad = db.query(Ad).filter(Ad.id == ad_id).first()
        if not ad:
            raise HTTPException(status_code=404, detail="Ad not found")
        
        # Trigger analysis task using celery_app to avoid type issues
        task = celery_app.send_task('app.tasks.ai_analysis_tasks.ai_analysis_task', args=[ad_id])
        
        return {
            "message": f"AI analysis triggered for ad {ad_id}",
            "task_id": task.id,
            "ad_id": ad_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error triggering analysis for ad {ad_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error triggering analysis: {str(e)}")

@router.post("/ads/analyze/batch")
async def trigger_batch_analysis(
    ad_ids: List[int],
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Trigger AI analysis for multiple ads.
    """
    try:
        # Validate ad IDs exist
        existing_ads = db.query(Ad.id).filter(Ad.id.in_(ad_ids)).all()
        existing_ad_ids = [ad.id for ad in existing_ads]
        
        if not existing_ad_ids:
            raise HTTPException(status_code=404, detail="No valid ads found")
        
        # Trigger batch analysis using celery_app
        task = celery_app.send_task('app.tasks.ai_analysis_tasks.batch_ai_analysis_task', args=[existing_ad_ids])
        
        return {
            "message": f"Batch AI analysis triggered for {len(existing_ad_ids)} ads",
            "task_id": task.id,
            "ad_ids": existing_ad_ids,
            "invalid_ids": list(set(ad_ids) - set(existing_ad_ids))
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error triggering batch analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error triggering batch analysis: {str(e)}")

@router.post("/ads/reprocess", response_model=Dict)
def reprocess_ads_data(
    request: ReprocessAdsRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Reprocess existing ads data with enhanced extraction
    
    This endpoint starts a background task to reprocess ads data that was previously scraped,
    applying the enhanced extraction to structure the data better.
    """
    logger.info(f"Received request to reprocess ads with enhanced extraction")
    
    try:
        # Start background task
        background_tasks.add_task(
            _run_enhanced_reprocessing,
            db=db,
            ad_ids=request.ad_ids
        )
        
        return {
            "success": True,
            "message": "Ads reprocessing task started",
            "ad_count": len(request.ad_ids) if request.ad_ids else "all"
        }
    except Exception as e:
        logger.error(f"Error starting ads reprocessing task: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting reprocessing task: {str(e)}")

async def _run_enhanced_reprocessing(db: Session, ad_ids: Optional[List[str]] = None):
    """
    Background task to reprocess ads with enhanced extraction
    
    Args:
        db: Database session
        ad_ids: Optional list of ad IDs to reprocess. If None, all ads with raw data will be reprocessed.
    """
    logger.info(f"Starting background task for ads reprocessing")
    
    try:
        # Create enhanced extraction service
        enhanced_extraction_service = EnhancedAdExtractionService(db)
        
        # Query ads to reprocess
        query = db.query(Ad)
        
        # Filter by ad_ids if provided
        if ad_ids:
            logger.info(f"Reprocessing specific ads: {ad_ids}")
            query = query.filter(Ad.ad_id.in_(ad_ids))
        else:
            logger.info("Reprocessing all ads with raw data")
            query = query.filter(Ad.raw_data.isnot(None))
        
        # Execute query
        ads = query.all()
        logger.info(f"Found {len(ads)} ads to reprocess")
        
        if not ads:
            logger.warning("No ads found to reprocess")
            return
        
        # Group ads by campaign_id for batch processing
        campaigns = {}
        for ad in ads:
            campaign_id = ad.campaign_id or "unknown"
            if campaign_id not in campaigns:
                campaigns[campaign_id] = []
            campaigns[campaign_id].append(ad)
        
        logger.info(f"Grouped ads into {len(campaigns)} campaigns")
        
        # Process each campaign
        processed_campaigns = 0
        total_processed = 0
        total_updated = 0
        total_errors = 0
        
        for campaign_id, campaign_ads in campaigns.items():
            try:
                # Prepare raw data for enhanced extraction
                raw_responses = []
                for ad in campaign_ads:
                    if ad.raw_data:
                        # Create a mock response structure that the enhanced extractor can process
                        mock_response = {
                            "data": {
                                "ad_library_main": {
                                    "search_results_connection": {
                                        "edges": [
                                            {
                                                "node": {
                                                    "collated_results": [ad.raw_data]
                                                }
                                            }
                                        ]
                                    }
                                }
                            }
                        }
                        raw_responses.append(mock_response)
                    else:
                        logger.warning(f"Ad {ad.ad_id} has no raw data, skipping")
                    
                if not raw_responses:
                    logger.warning(f"No raw data found for campaign {campaign_id}, skipping")
                    continue
                    
                # Process with enhanced extraction
                enhanced_data, stats = enhanced_extraction_service.process_raw_responses(raw_responses)
                
                # Update stats
                processed_campaigns += 1
                total_processed += len(campaign_ads)
                total_updated += stats.get("total_updated", 0)
                total_errors += stats.get("total_errors", 0)
                
                logger.info(f"Processed campaign {campaign_id}: {stats}")
                    
            except Exception as e:
                logger.error(f"Error processing campaign {campaign_id}: {str(e)}")
                total_errors += 1
        
        # Log final stats
        logger.info(f"Reprocessing complete!")
        logger.info(f"Campaigns processed: {processed_campaigns}")
        logger.info(f"Total ads processed: {total_processed}")
        logger.info(f"Total ads updated: {total_updated}")
        logger.info(f"Total errors: {total_errors}")
        
    except Exception as e:
        logger.error(f"Error in reprocessing: {str(e)}")
    finally:
        # Close database session
        db.close()

# ========================================
# Scraping Endpoints (Backward Compatibility)
# ========================================

@router.post("/ads/scrape", response_model=Dict)
def scrape_facebook_ads(
    request: FacebookAdsScraperRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Endpoint to scrape Facebook Ads Library data
    
    This endpoint will start a background task to scrape Facebook Ads Library data.
    """
    logger.info(f"Received request to scrape Facebook Ads for page: {request.view_all_page_id}")
    
    # Create task ID
    task_id = str(uuid.uuid4())
    
    # Create config from request
    config = {
        "view_all_page_id": request.view_all_page_id,
        "countries": request.countries,
        "max_pages": request.max_pages,
        "delay_between_requests": request.delay_between_requests,
        "active_status": request.active_status,
        "ad_type": request.ad_type,
        "media_type": request.media_type,
        "search_type": request.search_type,
        "query_string": request.query_string,
        "save_json": request.save_json
    }
    
    # Start background task
    background_tasks.add_task(
        _run_facebook_ads_scraper_task,
        task_id=task_id,
        config=config,
        db=db
    )
    
    return {
        "message": "Facebook Ads scraping task started",
        "task_id": task_id,
        "status": "pending"
    }

@router.post("/ads/scrape/competitor", response_model=Dict)
def scrape_competitor_ads(
    request: CompetitorAdsScraperRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Endpoint to scrape ads from a specific competitor page
    
    This endpoint will start a background task to scrape ads from a specific competitor page.
    """
    logger.info(f"Received request to scrape competitor ads for {request.competitor_page_id}")
    
    # Create task ID
    task_id = str(uuid.uuid4())
    
    # Start background task
    background_tasks.add_task(
        _run_competitor_ads_scraper_task,
        task_id=task_id,
            competitor_page_id=request.competitor_page_id,
            countries=request.countries,
            max_pages=request.max_pages,
        delay_between_requests=request.delay_between_requests,
        active_status=request.active_status,
        ad_type=request.ad_type,
        media_type=request.media_type,
        date_from=request.date_from,
        date_to=request.date_to,
        save_json=request.save_json,
        db=db
    )
    
    return {
        "message": f"Competitor ads scraping task started for {request.competitor_page_id}",
        "task_id": task_id,
        "status": "pending"
    }

@router.get("/ads/scrape/status/{task_id}", response_model=Dict)
def get_scraping_task_status(task_id: str):
    """
    Get the status of a scraping task
    
    This endpoint returns the current status of a Facebook Ads scraping task,
    including progress information and results if the task is complete.
    """
    try:
        # Get task result from Celery
        from celery.result import AsyncResult
        from app.celery_worker import celery_app
        
        task = AsyncResult(task_id, app=celery_app)
        
        # Check if task exists
        if not task:
            logger.error(f"Task {task_id} not found")
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
        
        # Get task state
        state = task.state
        
        # Prepare response
        response = {
            "task_id": task_id,
            "state": state
        }
        
        # Add info based on state
        if state == "PENDING":
            response["status"] = "Task is pending execution"
            
        elif state == "STARTED":
            response["status"] = "Task has started"
            
        elif state == "PROGRESS":
            # Task is in progress, get meta info
            meta = task.info
            response["status"] = meta.get("status", "Task is in progress")
            response["progress"] = {
                "current": meta.get("current", 0),
                "total": meta.get("total", 100)
            }
            
        elif state == "SUCCESS":
            # Task completed successfully
            result = task.result
            
            # Add task result to response
            response["status"] = "Task completed successfully"
            response["result"] = {
                "success": result.get("success", True),
                "total_ads_scraped": result.get("total_ads_scraped", 0),
                "database_stats": result.get("database_stats", {}),
                "completion_time": result.get("completion_time")
            }
            
            # Add scraper config if available
            if "scraper_config" in result:
                response["result"]["scraper_config"] = result["scraper_config"]
            
            # Add competitor info if available
            if "competitor_page_id" in result:
                response["result"]["competitor_page_id"] = result["competitor_page_id"]
            
        elif state == "FAILURE":
            # Task failed
            response["status"] = "Task failed"
            
            # Add error info if available
            if task.info:
                if isinstance(task.info, dict):
                    response["error"] = task.info.get("status", str(task.info))
                else:
                    response["error"] = str(task.info)
        
        return response
        
    except Exception as e:
        logger.error(f"Error getting task status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting task status: {str(e)}")

# ========================================
# Competitors endpoints have been moved to dedicated competitors router
# ========================================

# ========================================
# Testing Endpoints (Backward Compatibility)
# ========================================

@router.post("/test/add", response_model=TaskResponse)
async def test_add_numbers(request: AddNumbersRequest):
    """Test endpoint for adding numbers via Celery task"""
    try:
        task = celery_app.send_task('app.tasks.basic_tasks.add_together', args=[request.x, request.y])
        return TaskResponse(
            task_id=task.id,
            status="started",
            message=f"Addition task started: {request.x} + {request.y}"
        )
    except Exception as e:
        logger.error(f"Error starting addition task: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting addition task: {str(e)}")

@router.post("/test/task", response_model=TaskResponse)
async def test_celery_task(request: TestTaskRequest):
    """Test endpoint for basic Celery task"""
    try:
        task = celery_app.send_task('app.tasks.basic_tasks.test_task', args=[request.message])
        return TaskResponse(
            task_id=task.id,
            status="started",
            message="Test task started successfully"
        )
    except Exception as e:
        logger.error(f"Error starting test task: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting test task: {str(e)}")

@router.get("/test/task/{task_id}", response_model=None)
async def get_task_status(task_id: str):
    """Get the status of any Celery task"""
    try:
        from celery.result import AsyncResult
        from app.celery_worker import celery_app
        
        task_result = AsyncResult(task_id, app=celery_app)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'status': 'Task is pending...'
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'result': task_result.result,
                'status': 'Task completed successfully'
            }
        elif task_result.state == 'FAILURE':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'error': str(task_result.info),
                'status': 'Task failed'
            }
        else:
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'info': task_result.info,
                'status': f'Task is {task_result.state.lower()}'
            }
        
        return response
        
    except Exception as e:
        logger.error(f"Error fetching task status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching task status: {str(e)}") 

def _run_facebook_ads_scraper_task(
    task_id: str,
    config: Dict,
    db: Session
):
    """Run Facebook Ads scraper task in the background"""
    try:
        # Create scraper service
        scraper = FacebookAdsScraperService(db)
        
        # Create configuration
        scraper_config = FacebookAdsScraperConfig(**config)
        
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper.scrape_ads(scraper_config)
        
        # Save to database if requested
        if scraper_config.save_json and all_json_responses:
            # Save raw JSON responses to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_ads_{scraper_config.view_all_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")
        
        # Update task status
        task_status = TaskStatus(
            task_id=task_id,
            status="completed",
            result={
                "stats": stats,
                "enhanced_data_summary": {
                    "advertiser_info": enhanced_data.get("advertiser_info", {}),
                    "campaigns_count": len(enhanced_data.get("campaigns", [])),
                    "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
                }
            }
        )
        db.add(task_status)
        db.commit()
        
    except Exception as e:
        logger.error(f"Error in Facebook Ads scraping task: {str(e)}")
        # Update task status with error
        task_status = TaskStatus(
            task_id=task_id,
            status="failed",
            result={"error": str(e)}
        )
        db.add(task_status)
        db.commit()

def _run_competitor_ads_scraper_task(
    task_id: str,
    competitor_page_id: str,
    countries: List[str],
    max_pages: int,
    delay_between_requests: int,
    active_status: str,
    ad_type: str,
    media_type: str,
    date_from: Optional[str],
    date_to: Optional[str],
    save_json: bool,
    db: Session
):
    """Run competitor ads scraper task in the background"""
    try:
        # Create scraper service
        scraper = FacebookAdsScraperService(db)
        
        # Get competitor
        competitor = db.query(Competitor).filter_by(page_id=competitor_page_id).first()
        if not competitor:
            # Don't create new competitor, exit early
            logger.warning(f"Competitor with page_id {competitor_page_id} not found, skipping scraping")
            # Update task status with warning
            task_status = TaskStatus(
                task_id=task_id,
                status="completed",
                result={"warning": f"Competitor with page_id {competitor_page_id} not found, skipping scraping"}
            )
            db.add(task_status)
            db.commit()
            return
        
        # Create configuration
        scraper_config = FacebookAdsScraperConfig(
            view_all_page_id=competitor_page_id,
            countries=countries,
            max_pages=max_pages,
            delay_between_requests=delay_between_requests,
            active_status=active_status,
            ad_type=ad_type,
            media_type=media_type,
            start_date=date_from,
            end_date=date_to,
            save_json=save_json
        )
        
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper.scrape_ads(scraper_config)
        
        # Save to database if requested
        if scraper_config.save_json and all_json_responses:
            # Save raw JSON responses to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"competitor_ads_{competitor_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")
        
        # Update task status
        task_status = TaskStatus(
            task_id=task_id,
            status="completed",
            result={
                "competitor_id": competitor.id,
                "competitor_page_id": competitor_page_id,
                "stats": stats,
                "enhanced_data_summary": {
                    "advertiser_info": enhanced_data.get("advertiser_info", {}),
                    "campaigns_count": len(enhanced_data.get("campaigns", [])),
                    "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
                }
            }
        )
        db.add(task_status)
        db.commit()
        
    except Exception as e:
        logger.error(f"Error in competitor ads scraping task: {str(e)}")
        # Update task status with error
        task_status = TaskStatus(
            task_id=task_id,
            status="failed",
            result={"error": str(e)}
        )
        db.add(task_status)
        db.commit() 

@router.get("/ad-sets", response_model=PaginatedAdResponseDTO)
async def get_ad_sets(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    sort_by: Optional[str] = Query("created_at", description="Sort by field"),
    sort_order: Optional[str] = Query("desc", description="Sort order (asc/desc)"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get all ad sets with pagination.
    Each ad set is represented by its best/representative ad.
    """
    try:
        result = ad_service.get_ad_sets(page, page_size, sort_by, sort_order)
        
        if not result:
            return PaginatedAdResponseDTO(
                data=[], 
                pagination=PaginationMetadata(
                    page=page, 
                    page_size=page_size,
                    total_items=0,
                    total_pages=0,
                    has_next=False,
                    has_previous=False
                )
            )
        
        return result
        
    except Exception as e:
        logger.error(f"Error fetching ad sets: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ad sets: {str(e)}") 

@router.get("/ad-sets/{ad_set_id}", response_model=PaginatedAdResponseDTO)
async def get_ads_in_set(
    ad_set_id: int,
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    ad_service: "AdService" = Depends(get_ad_service_dependency)
):
    """
    Get all ad variants belonging to a specific AdSet with pagination.
    This endpoint is used to show all variations of ads within an ad set.
    """
    try:
        result = ad_service.get_ads_in_set(ad_set_id, page, page_size)
        
        if not result:
            raise HTTPException(status_code=404, detail=f"AdSet with ID {ad_set_id} not found")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching ads in set {ad_set_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching ads in set: {str(e)}") 


================================================
File: app/routers/competitors.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List, Optional, TYPE_CHECKING
import logging
from pydantic import BaseModel

from app.database import get_db
from app.models.dto.competitor_dto import (
    CompetitorCreateDTO,
    CompetitorUpdateDTO,
    CompetitorResponseDTO,
    CompetitorDetailResponseDTO,
    PaginatedCompetitorResponseDTO,
    CompetitorFilterParams,
    CompetitorStatsResponseDTO
)

# Import tasks for scraping
from app.tasks.facebook_ads_scraper_task import scrape_competitor_ads_task

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    from app.services.competitor_service import CompetitorService

router = APIRouter()
logger = logging.getLogger(__name__)

# Dependency factory to avoid circular imports
def get_competitor_service_dependency(db: Session = Depends(get_db)) -> "CompetitorService":
    """Factory function to create CompetitorService instance for dependency injection."""
    from app.services.competitor_service import CompetitorService
    return CompetitorService(db)

# ========================================
# Main Competitors CRUD Endpoints
# ========================================

@router.get("/", response_model=PaginatedCompetitorResponseDTO)
async def get_competitors(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    search: Optional[str] = Query(None, description="Search in competitor names and page IDs"),
    sort_by: Optional[str] = Query("created_at", description="Sort by field"),
    sort_order: Optional[str] = Query("desc", description="Sort order (asc/desc)"),
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> PaginatedCompetitorResponseDTO:
    """
    Get paginated and filtered list of competitors.
    
    This endpoint provides comprehensive competitor listing with:
    - Pagination support
    - Search functionality
    - Active/inactive filtering
    - Sorting options
    - Ads count for each competitor
    """
    try:
        # Create filter parameters
        filters = CompetitorFilterParams(
            page=page,
            page_size=page_size,
            is_active=is_active,
            search=search,
            sort_by=sort_by,
            sort_order=sort_order
        )
        
        # Get competitors using service
        result = competitor_service.get_competitors(filters)
        
        logger.info(f"Retrieved {len(result.data)} competitors for page {page}")
        return result
        
    except Exception as e:
        logger.error(f"Error fetching competitors: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitors: {str(e)}")

@router.get("/{competitor_id}", response_model=CompetitorDetailResponseDTO)
async def get_competitor(
    competitor_id: int,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorDetailResponseDTO:
    """
    Get detailed information for a specific competitor.
    
    Returns comprehensive competitor data including:
    - Basic competitor information
    - Total ads count
    - Active ads count
    - Analyzed ads count
    """
    try:
        competitor = competitor_service.get_competitor_by_id(competitor_id)
        return competitor
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor: {str(e)}")

@router.post("/", response_model=CompetitorResponseDTO)
async def create_competitor(
    competitor_data: CompetitorCreateDTO,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorResponseDTO:
    """
    Create a new competitor.
    
    Required fields:
    - name: Competitor name
    - page_id: Facebook page ID (must be unique)
    - is_active: Whether the competitor is active (default: True)
    """
    try:
        competitor = competitor_service.create_competitor(competitor_data)
        logger.info(f"Created new competitor: {competitor.name} (ID: {competitor.id})")
        return competitor
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating competitor: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error creating competitor: {str(e)}")

@router.put("/{competitor_id}", response_model=CompetitorDetailResponseDTO)
async def update_competitor(
    competitor_id: int,
    competitor_data: CompetitorUpdateDTO,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorDetailResponseDTO:
    """
    Update an existing competitor.
    
    All fields are optional:
    - name: Competitor name
    - page_id: Facebook page ID (must be unique if changed)
    - is_active: Whether the competitor is active
    """
    try:
        competitor = competitor_service.update_competitor(competitor_id, competitor_data)
        logger.info(f"Updated competitor: {competitor.name} (ID: {competitor.id})")
        return competitor
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating competitor: {str(e)}")

class BulkDeleteRequest(BaseModel):
    competitor_ids: List[int]

@router.delete("/")
async def bulk_delete_competitors(
    request: BulkDeleteRequest,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> dict:
    """
    Bulk delete competitors.
    - Soft deletes competitors with ads.
    - Hard deletes competitors without ads.
    """
    try:
        result = competitor_service.bulk_delete_competitors(request.competitor_ids)
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during bulk competitor deletion: {str(e)}")
        raise HTTPException(status_code=500, detail=f"An error occurred during bulk deletion: {str(e)}")

@router.delete("/{competitor_id}")
async def delete_competitor(
    competitor_id: int,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> dict:
    """
    Delete a competitor.
    
    Behavior:
    - If competitor has ads: Soft delete (sets is_active to False)
    - If competitor has no ads: Hard delete (permanent removal)
    """
    try:
        result = competitor_service.delete_competitor(competitor_id)
        logger.info(f"Deleted competitor ID {competitor_id}: {result['message']}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting competitor: {str(e)}")

# ========================================
# Competitors Statistics & Search
# ========================================

@router.get("/stats/overview", response_model=CompetitorStatsResponseDTO)
async def get_competitor_stats(
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> CompetitorStatsResponseDTO:
    """
    Get comprehensive statistics about competitors.
    
    Returns:
    - Total competitors count
    - Active/inactive competitors count
    - Competitors with ads count
    - Total ads across all competitors
    - Average ads per competitor
    """
    try:
        stats = competitor_service.get_competitor_stats()
        return stats
        
    except Exception as e:
        logger.error(f"Error fetching competitor stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor stats: {str(e)}")

@router.get("/search/query", response_model=List[CompetitorResponseDTO])
async def search_competitors(
    q: str = Query(..., description="Search query"),
    limit: int = Query(50, ge=1, le=100, description="Maximum number of results"),
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> List[CompetitorResponseDTO]:
    """
    Search competitors by name or page ID.
    
    Searches in:
    - Competitor names (case-insensitive)
    - Facebook page IDs
    """
    try:
        results = competitor_service.search_competitors(q, limit)
        return results
        
    except Exception as e:
        logger.error(f"Error searching competitors: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error searching competitors: {str(e)}")

# ========================================
# Competitor Ads Scraping
# ========================================

class CompetitorScrapeRequest(BaseModel):
    """Request model for competitor ads scraping"""
    countries: Optional[List[str]] = ["AE"]
    max_pages: Optional[int] = 5
    delay_between_requests: Optional[int] = 1
    active_status: Optional[str] = "active"

class TaskResponse(BaseModel):
    """Response model for task operations"""
    task_id: str
    status: str
    message: str

@router.post("/{competitor_id}/scrape", response_model=TaskResponse)
async def scrape_competitor_ads(
    competitor_id: int,
    scrape_request: CompetitorScrapeRequest,
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
) -> TaskResponse:
    """
    Trigger ads scraping for a specific competitor.
    
    This will:
    1. Validate the competitor exists
    2. Start a background task to scrape ads from their Facebook page
    3. Return task ID for monitoring progress
    """
    try:
        # Validate competitor exists and is active
        competitor = competitor_service.get_competitor_by_id(competitor_id)
        
        if not competitor.is_active:
            raise HTTPException(
                status_code=400,
                detail=f"Competitor '{competitor.name}' is not active. Please activate it first."
            )
        
        # Start scraping task
        task = scrape_competitor_ads_task.delay(
            competitor_page_id=competitor.page_id,
            countries=scrape_request.countries,
            max_pages=scrape_request.max_pages,
            delay_between_requests=scrape_request.delay_between_requests,
            active_status=scrape_request.active_status
        )
        
        logger.info(f"Started scraping task for competitor {competitor.name} (ID: {competitor_id})")
        
        return TaskResponse(
            task_id=task.id,
            status="started",
            message=f"Ads scraping started for competitor '{competitor.name}'"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting scraping for competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error starting scraping: {str(e)}")

@router.get("/scrape/status/{task_id}")
async def get_scraping_status(task_id: str) -> dict:
    """
    Get the status of a competitor scraping task.
    
    Returns task status and progress information.
    """
    try:
        from celery.result import AsyncResult
        from app.celery_worker import celery_app
        
        task_result = AsyncResult(task_id, app=celery_app)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'status': 'Task is pending...'
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'result': task_result.result,
                'status': 'Task completed successfully'
            }
        elif task_result.state == 'FAILURE':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'error': str(task_result.info),
                'status': 'Task failed'
            }
        else:
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'info': task_result.info,
                'status': f'Task is {task_result.state.lower()}'
            }
        
        return response
        
    except Exception as e:
        logger.error(f"Error fetching scraping status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching scraping status: {str(e)}")

# ========================================
# Competitor Ads Management
# ========================================

@router.get("/{competitor_id}/ads")
async def get_competitor_ads(
    competitor_id: int,
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Number of items per page"),
    is_active: Optional[bool] = Query(None, description="Filter by active status"),
    has_analysis: Optional[bool] = Query(None, description="Filter by analysis availability"),
    competitor_service: "CompetitorService" = Depends(get_competitor_service_dependency)
):
    """
    Get ads for a specific competitor with filtering and pagination.
    
    This endpoint redirects to the main ads endpoint with competitor filter.
    """
    try:
        # Validate competitor exists
        competitor = competitor_service.get_competitor_by_id(competitor_id)
        
        # Import here to avoid circular imports
        from app.services.ad_service import AdService
        from app.models.dto.ad_dto import AdFilterParams
        
        # Create ad service
        db = next(get_db())
        ad_service = AdService(db)
        
        # Create filter parameters
        filters = AdFilterParams(
            page=page,
            page_size=page_size,
            competitor_id=competitor_id,
            is_active=is_active,
            has_analysis=has_analysis,
            sort_by="created_at",
            sort_order="desc"
        )
        
        # Get ads using ad service
        result = ad_service.get_ads(filters)
        
        return {
            "competitor": {
                "id": competitor.id,
                "name": competitor.name,
                "page_id": competitor.page_id
            },
            "ads": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching ads for competitor {competitor_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching competitor ads: {str(e)}")

# Import required models 


================================================
File: app/routers/health.py
================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.database import get_db
from app.core.config import settings
import redis
import os
from datetime import datetime

router = APIRouter()

@router.get("/health")
async def health_check():
    """Basic health check endpoint - returns the exact format requested in the prompt"""
    return {"status": "ok"}

@router.get("/health/detailed")
async def detailed_health_check(db: Session = Depends(get_db)):
    """Detailed health check including database and Redis connectivity"""
    health_status = {
        "status": "ok",
        "timestamp": datetime.utcnow().isoformat(),
        "service": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "checks": {}
    }
    
    # Check database connectivity
    try:
        db.execute(text("SELECT 1"))
        health_status["checks"]["database"] = {
            "status": "ok", 
            "message": "Database connection successful"
        }
    except Exception as e:
        health_status["checks"]["database"] = {
            "status": "error", 
            "message": f"Database connection failed: {str(e)}"
        }
        health_status["status"] = "error"
    
    # Check Redis connectivity
    try:
        redis_client = redis.from_url(settings.REDIS_URL)
        redis_client.ping()
        health_status["checks"]["redis"] = {
            "status": "ok", 
            "message": "Redis connection successful"
        }
    except Exception as e:
        health_status["checks"]["redis"] = {
            "status": "error", 
            "message": f"Redis connection failed: {str(e)}"
        }
        health_status["status"] = "error"
    
    # Check Celery worker connectivity
    try:
        from app.celery_worker import celery_app
        inspect = celery_app.control.inspect()
        stats = inspect.stats()
        if stats:
            health_status["checks"]["celery"] = {
                "status": "ok", 
                "message": "Celery workers are active",
                "active_workers": len(stats)
            }
        else:
            health_status["checks"]["celery"] = {
                "status": "warning", 
                "message": "No active Celery workers found"
            }
    except Exception as e:
        health_status["checks"]["celery"] = {
            "status": "error", 
            "message": f"Celery check failed: {str(e)}"
        }
    
    if health_status["status"] == "error":
        raise HTTPException(status_code=503, detail=health_status)
    
    return health_status 


================================================
File: app/services/__init__.py
================================================
from .ingestion_service import DataIngestionService
from .ai_service import AIService, get_ai_service
from .ad_service import AdService, get_ad_service
from .creative_comparison_service import CreativeComparisonService

__all__ = [
    "DataIngestionService", 
    "AIService", 
    "get_ai_service", 
    "AdService", 
    "get_ad_service",
    "CreativeComparisonService"
] 


================================================
File: app/services/ad_service.py
================================================
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import func, and_, or_, desc, asc, cast, String as SQLAString, literal, case
from typing import List, Optional, Dict, Any, Tuple, Union, Sequence
from datetime import datetime, timedelta
import logging

from app.models import Ad, Competitor, AdAnalysis, AdSet
from app.models.dto.ad_dto import (
    AdFilterParams, 
    AdResponseDTO, 
    AdDetailResponseDTO, 
    PaginatedAdResponseDTO, 
    PaginationMetadata,
    AdStatsResponseDTO,
    AdStats,
    Campaign,
    AdvertiserInfo,
    PaginatedAdResponse,
    AdResponse,
    AdAnalysisResponseDTO
)
from app.models.dto.competitor_dto import CompetitorResponseDTO

logger = logging.getLogger(__name__)


class AdService:
    """
    Service class for handling all ad-related database operations.
    Provides methods for querying, filtering, and paginating ad data.
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    def get_ads(self, filters: AdFilterParams) -> PaginatedAdResponseDTO:
        """
        Get paginated and filtered ads grouped by AdSets.
        Returns the "best" ad from each set with the variant count.
        
        Args:
            filters: AdFilterParams containing pagination and filter parameters
            
        Returns:
            PaginatedAdResponseDTO with the best ad from each set and pagination metadata
        """
        try:
            # Build base query on AdSet model
            query = self.db.query(AdSet).options(
                joinedload(AdSet.best_ad).joinedload(Ad.competitor),
                joinedload(AdSet.best_ad).joinedload(Ad.analysis)
            )
            
            # Apply filters to the AdSets based on their best ads
            query = self._apply_adset_filters(query, filters)
            
            # Apply sorting
            sort_by = filters.sort_by or "created_at"
            sort_order = filters.sort_order or "desc"
            query = self._apply_adset_sorting(query, sort_by, sort_order)
            
            # Get total count before pagination
            total_items = query.count()
            
            # Apply pagination
            offset = (filters.page - 1) * filters.page_size
            ad_sets = query.offset(offset).limit(filters.page_size).all()
            
            # Convert to DTOs with AdSet information
            ad_dtos = []
            for ad_set in ad_sets:
                if ad_set.best_ad:
                    # Convert the best ad to DTO and add AdSet information
                    ad_dto = self._convert_to_dto(ad_set.best_ad)
                    # Augment the Ad DTO with metadata from its parent AdSet
                    ad_dto.ad_set_id = ad_set.id
                    ad_dto.variant_count = ad_set.variant_count
                    ad_dto.ad_set_created_at = ad_set.created_at
                    ad_dto.ad_set_first_seen_date = ad_set.first_seen_date
                    ad_dto.ad_set_last_seen_date = ad_set.last_seen_date
                    ad_dtos.append(ad_dto)
                else:
                    # If there's no best_ad, fetch the first ad in the set
                    first_ad = self.db.query(Ad).filter(Ad.ad_set_id == ad_set.id).first()
                    if first_ad:
                        ad_dto = self._convert_to_dto(first_ad)
                        ad_dto.ad_set_id = ad_set.id
                        ad_dto.variant_count = ad_set.variant_count
                        ad_dto.ad_set_created_at = ad_set.created_at
                        ad_dto.ad_set_first_seen_date = ad_set.first_seen_date
                        ad_dto.ad_set_last_seen_date = ad_set.last_seen_date
                        ad_dtos.append(ad_dto)
            
            # Calculate pagination metadata
            total_pages = (total_items + filters.page_size - 1) // filters.page_size
            pagination = PaginationMetadata(
                page=filters.page,
                page_size=filters.page_size,
                total_items=total_items,
                total_pages=total_pages,
                has_next=filters.page < total_pages,
                has_previous=filters.page > 1
            )
            
            return PaginatedAdResponseDTO(
                data=ad_dtos,
                pagination=pagination
            )
            
        except Exception as e:
            logger.error(f"Error fetching ad sets: {str(e)}")
            raise
    
    def _apply_adset_filters(self, query, filters: AdFilterParams):
        """
        Apply filters to AdSet query based on filter parameters.
        Filters are applied based on the properties of the best ad in each set.
        """
        try:
            # Filter by competitor
            if filters.competitor_id:
                query = query.join(AdSet.best_ad).filter(Ad.competitor_id == filters.competitor_id)
            
            if filters.competitor_name:
                query = query.join(AdSet.best_ad).join(Ad.competitor).filter(
                    Competitor.name.ilike(f"%{filters.competitor_name}%")
                )
            
            # Filter by has_analysis
            if filters.has_analysis is not None:
                if filters.has_analysis:
                    query = query.join(AdSet.best_ad).join(Ad.analysis, isouter=True).filter(AdAnalysis.id.isnot(None))
                else:
                    query = query.join(AdSet.best_ad).outerjoin(Ad.analysis).filter(AdAnalysis.id.is_(None))
            
            # Filter by hook_score
            if filters.min_hook_score is not None:
                query = query.join(AdSet.best_ad).join(Ad.analysis).filter(
                    AdAnalysis.hook_score >= filters.min_hook_score
                )
                
            if filters.max_hook_score is not None:
                query = query.join(AdSet.best_ad).join(Ad.analysis).filter(
                    AdAnalysis.hook_score <= filters.max_hook_score
                )
            
            # Filter by overall_score
            if filters.min_overall_score is not None:
                query = query.join(AdSet.best_ad).join(Ad.analysis).filter(
                    AdAnalysis.overall_score >= filters.min_overall_score
                )
                
            if filters.max_overall_score is not None:
                query = query.join(AdSet.best_ad).join(Ad.analysis).filter(
                    AdAnalysis.overall_score <= filters.max_overall_score
                )
            
            # Filter by duration
            if filters.min_duration_days is not None:
                query = query.join(AdSet.best_ad).filter(Ad.duration_days >= filters.min_duration_days)
                
            if filters.max_duration_days is not None:
                query = query.join(AdSet.best_ad).filter(Ad.duration_days <= filters.max_duration_days)
            
            # Filter by date range on the AdSet's lifetime
            if filters.date_from:
                query = query.filter(AdSet.last_seen_date >= filters.date_from)
                
            if filters.date_to:
                query = query.filter(AdSet.first_seen_date <= filters.date_to)
            
            # Filter by active status
            if filters.is_active is not None:
                # Check the meta JSON field for is_active
                query = query.join(AdSet.best_ad).filter(
                    Ad.meta['is_active'].as_string() == str(filters.is_active).lower()
                )
            
            # Search in ad content
            if filters.search:
                search_term = f"%{filters.search}%"
                query = query.join(AdSet.best_ad).filter(
                    or_(
                        # Search in ad text fields within the creatives JSON
                        cast(Ad.creatives, SQLAString).ilike(search_term),
                        # Check for page name within the meta JSON
                        cast(Ad.meta, SQLAString).ilike(search_term)
                    )
                )
                
            # Filter by media type
            if filters.media_type:
                query = query.join(AdSet.best_ad).filter(
                    cast(Ad.creatives, SQLAString).like(f'%"type": "{filters.media_type}"%')
                )
                
            return query
        except Exception as e:
            logger.error(f"Error applying filters to AdSet query: {str(e)}")
            raise
    
    def _apply_adset_sorting(self, query, sort_by: str, sort_order: str):
        """
        Apply sorting to AdSet query.
        Sorting is based on the properties of the best ad in each set.
        """
        try:
            # Determine sort direction
            direction = desc if sort_order.lower() == "desc" else asc
            
            # Apply sorting based on different fields
            if sort_by == "created_at" or sort_by == "date_found" or sort_by == "updated_at":
                # Sort by date fields from the Ad model
                query = query.join(AdSet.best_ad).order_by(direction(getattr(Ad, sort_by)))
            elif sort_by == "variant_count":
                # Sort directly by AdSet's variant_count
                query = query.order_by(direction(AdSet.variant_count))
            elif sort_by == "hook_score" or sort_by == "overall_score":
                # Sort by analysis scores
                query = query.join(AdSet.best_ad).join(Ad.analysis, isouter=True).order_by(
                    direction(getattr(AdAnalysis, sort_by, None)),
                    desc(Ad.date_found)  # Secondary sort by date found
                )
            else:
                # Default to sorting by AdSet creation date
                query = query.order_by(direction(AdSet.created_at))
                
            return query
        except Exception as e:
            logger.error(f"Error applying sorting to AdSet query: {str(e)}")
            raise
    
    def get_ad_by_id(self, ad_id: int) -> Optional[AdDetailResponseDTO]:
        """Get a single ad by ID."""
        ad = self.db.query(Ad).options(
            joinedload(Ad.competitor),
            joinedload(Ad.analysis)
        ).filter(Ad.id == ad_id).first()
        
        if not ad:
            return None
            
        return self._convert_to_detail_dto(ad)
    
    def get_ad_by_archive_id(self, ad_archive_id: str) -> Optional[Ad]:
        """Get ad by Facebook archive ID"""
        return self.db.query(Ad).filter(Ad.ad_archive_id == ad_archive_id).first()
    
    def get_ads_paginated(self, filters: AdFilterParams) -> PaginatedAdResponse:
        """Get paginated ads with filtering"""
        query = self.db.query(Ad)
        
        # Apply filters
        if filters.campaign_id:
            query = query.filter(Ad.campaign_id == filters.campaign_id)
        
        if filters.is_active is not None:
            query = query.filter(Ad.meta_is_active == filters.is_active)
            
        if filters.has_lead_form is not None:
            if filters.has_lead_form:
                # Has non-empty lead form fields
                query = query.filter(
                    or_(
                        cast(Ad.lead_form_standalone_fields, SQLAString) != '[]',
                        cast(Ad.lead_form_questions, SQLAString) != '{}'
                    )
                )
            else:
                # Empty lead form fields
                query = query.filter(
                    and_(
                        cast(Ad.lead_form_standalone_fields, SQLAString) == '[]',
                        cast(Ad.lead_form_questions, SQLAString) == '{}'
                    )
                )
        
        if filters.platform:
            query = query.filter(Ad.platforms.contains([filters.platform]))
        
        if filters.media_type:
            # Check in creatives array for media type
            query = query.filter(
                cast(Ad.creatives, SQLAString).like(f'%"type": "{filters.media_type}"%')
            )
        
        if filters.query:
            # Search in various text fields
            search_term = f"%{filters.query}%"
            query = query.filter(
                or_(
                    cast(Ad.creatives, SQLAString).ilike(search_term),
                    # Check for page name within the meta JSON
                    Ad.meta.op('->>')('page_name').ilike(search_term)
                )
            )
        
        # Count total results for pagination
        total = query.count()
        
        # Apply pagination
        query = query.order_by(desc(Ad.date_found))
        page_size = getattr(filters, 'limit', filters.page_size)
        query = query.offset((filters.page - 1) * page_size).limit(page_size)
        
        # Get results
        items = query.all()
        
        # Convert items to AdResponse objects
        response_items = [AdResponse.model_validate(item) for item in items]
        
        # Calculate total pages
        total_pages = (total + page_size - 1) // page_size
        
        return PaginatedAdResponse(
            items=response_items,
            total=total,
            page=filters.page,
            limit=page_size,
            total_pages=total_pages
        )
    
    def get_campaigns(self) -> List[Campaign]:
        """Get all campaigns with their ads"""
        # Get unique campaign IDs
        campaign_ids = self.db.query(Ad.campaign_id).distinct().all()
        campaign_ids = [c[0] for c in campaign_ids if c[0]]
        
        campaigns = []
        for campaign_id in campaign_ids:
            # Get ads for this campaign
            ads = self.db.query(Ad).filter(Ad.campaign_id == campaign_id).all()
            if ads:
                # Extract platforms from the first ad
                platforms = []
                if hasattr(ads[0], 'platforms') and ads[0].platforms is not None:
                    platforms = ads[0].platforms
                
                # Convert ads to AdResponse objects
                response_ads = [AdResponse.model_validate(ad) for ad in ads]
                
                campaigns.append(Campaign(
                    campaign_id=campaign_id,
                    platforms=platforms,
                    ads=response_ads
                ))
        
        return campaigns
    
    def get_ad_stats(self) -> AdStats:
        """Get ad statistics"""
        total_ads = self.db.query(func.count(Ad.id)).scalar() or 0
        active_ads = self.db.query(func.count(Ad.id)).filter(Ad.meta_is_active == True).scalar() or 0
        
        # Count ads with lead forms
        with_lead_form = self.db.query(func.count(Ad.id)).filter(
            or_(
                cast(Ad.lead_form_standalone_fields, SQLAString) != '[]',
                cast(Ad.lead_form_questions, SQLAString) != '{}'
            )
        ).scalar() or 0
        
        # Get platform stats
        platforms_stats = {}
        for ad in self.db.query(Ad.platforms).all():
            platforms = []
            if hasattr(ad, 'platforms') and ad.platforms is not None:
                platforms = ad.platforms
                
            for platform in platforms:
                    platforms_stats[platform] = platforms_stats.get(platform, 0) + 1
        
        # Get media type stats
        # This is more complex with JSON array, using a simplified approach
        media_types = {
            "Image": self.db.query(func.count(Ad.id)).filter(cast(Ad.creatives, SQLAString).like('%"type": "Image"%')).scalar() or 0,
            "Video": self.db.query(func.count(Ad.id)).filter(cast(Ad.creatives, SQLAString).like('%"type": "Video"%')).scalar() or 0,
            "Carousel": self.db.query(func.count(Ad.id)).filter(cast(Ad.creatives, SQLAString).like('%"type": "Carousel"%')).scalar() or 0,
            "Other": self.db.query(func.count(Ad.id)).filter(
                and_(
                    ~cast(Ad.creatives, SQLAString).like('%"type": "Image"%'),
                    ~cast(Ad.creatives, SQLAString).like('%"type": "Video"%'),
                    ~cast(Ad.creatives, SQLAString).like('%"type": "Carousel"%')
                )
            ).scalar() or 0
        }
        
        return AdStats(
            total_ads=total_ads,
            active_ads=active_ads,
            with_lead_form=with_lead_form,
            platforms=platforms_stats,
            media_types=media_types
        )
    
    def search_ads(self, query: str, limit: int = 50) -> List[AdResponseDTO]:
        """Search ads by text content."""
        try:
            search_filter = or_(
                cast(Ad.creatives, SQLAString).ilike(f'%{query}%'),
                cast(Ad.meta, SQLAString).ilike(f'%{query}%')
            )
            
            ads = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).join(Ad.competitor).filter(search_filter).limit(limit).all()
            
            return [self._convert_to_dto(ad) for ad in ads]
            
        except Exception as e:
            logger.error(f"Error searching ads: {str(e)}")
            raise
    
    def get_top_performing_ads(self, limit: int = 10) -> List[AdResponseDTO]:
        """
        Get top performing ads based on AI analysis scores.
        
        Args:
            limit: Number of top ads to return
            
        Returns:
            List of top performing ads
        """
        try:
            ads = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).join(AdAnalysis).order_by(
                desc(AdAnalysis.overall_score)
            ).limit(limit).all()
            
            return [self._convert_to_dto(ad) for ad in ads]
            
        except Exception as e:
            logger.error(f"Error fetching top performing ads: {str(e)}")
            raise
    
    def get_competitor_ads(self, competitor_id: int, limit: int = 50) -> List[AdResponseDTO]:
        """
        Get ads for a specific competitor.
        
        Args:
            competitor_id: ID of the competitor
            limit: Maximum number of ads to return
            
        Returns:
            List of competitor ads
        """
        try:
            ads = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).filter(Ad.competitor_id == competitor_id).limit(limit).all()
            
            return [self._convert_to_dto(ad) for ad in ads]
            
        except Exception as e:
            logger.error(f"Error fetching competitor ads: {str(e)}")
            raise
    
    def delete_ad(self, ad_id: int) -> bool:
        """
        Delete a specific ad by ID.
        
        Args:
            ad_id: ID of the ad to delete
            
        Returns:
            True if the ad was deleted, False if not found
        """
        try:
            ad = self.db.query(Ad).filter(Ad.id == ad_id).first()
            
            if not ad:
                return False
            
            # Delete associated analysis first (if any)
            if ad.analysis:
                self.db.delete(ad.analysis)
            
            # Delete the ad
            self.db.delete(ad)
            self.db.commit()
            
            logger.info(f"Successfully deleted ad {ad_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting ad {ad_id}: {str(e)}")
            self.db.rollback()
            raise
    
    def bulk_delete_ads(self, ad_ids: List[int]) -> int:
        """
        Delete multiple ads by IDs.
        
        Args:
            ad_ids: List of ad IDs to delete
            
        Returns:
            Number of ads successfully deleted
        """
        try:
            # Unset best_ad_id in AdSets where it's one of the ads being deleted
            self.db.query(AdSet).filter(
                AdSet.best_ad_id.in_(ad_ids)
            ).update({"best_ad_id": None}, synchronize_session=False)

            # First, delete all analyses associated with the ads
            analyses_deleted = self.db.query(AdAnalysis).filter(
                AdAnalysis.ad_id.in_(ad_ids)
            ).delete(synchronize_session=False)
            
            # Then delete the ads
            ads_deleted = self.db.query(Ad).filter(
                Ad.id.in_(ad_ids)
            ).delete(synchronize_session=False)
            
            self.db.commit()
            
            logger.info(f"Successfully deleted {ads_deleted} ads and {analyses_deleted} analyses")
            return ads_deleted
            
        except Exception as e:
            logger.error(f"Error bulk deleting ads: {str(e)}")
            self.db.rollback()
            raise
    
    def _apply_filters(self, query, filters: AdFilterParams):
        """
        Apply filters to query based on filter parameters.
        """
        
        if filters.competitor_id:
            query = query.filter(Ad.competitor_id == filters.competitor_id)
        
        if filters.competitor_name:
            query = query.join(Ad.competitor).filter(Competitor.name.ilike(f"%{filters.competitor_name}%"))
        
        if filters.media_type and filters.media_type != 'all':
            query = query.filter(cast(Ad.creatives, SQLAString).ilike(f'%"type": "{filters.media_type}"%'))
        
        if filters.is_active is not None:
            query = query.filter(cast(Ad.meta['is_active'], SQLAString) == str(filters.is_active).lower())

        if filters.has_analysis is not None:
            if filters.has_analysis:
                query = query.join(Ad.analysis).filter(AdAnalysis.id.isnot(None))
        
        if filters.min_overall_score is not None:
            query = query.join(Ad.analysis).filter(AdAnalysis.overall_score >= filters.min_overall_score)
            
        # Duration filters
        if filters.min_duration_days is not None:
            query = query.filter(Ad.duration_days >= filters.min_duration_days)
        if filters.max_duration_days is not None:
            query = query.filter(Ad.duration_days <= filters.max_duration_days)
            
        if filters.date_from:
            query = query.filter(Ad.date_found >= filters.date_from)
        if filters.date_to:
            query = query.filter(Ad.date_found <= filters.date_to)
        
        if filters.search:
            search_query = f"%{filters.search}%"
            query = query.join(Ad.competitor).filter(
                or_(
                    cast(Ad.creatives, SQLAString).ilike(search_query),
                    Competitor.name.ilike(search_query)
                )
            )
        
        return query
    
    def _apply_sorting(self, query, sort_by: str, sort_order: str):
        """
        Apply sorting to query based on sort_by and sort_order.
        """
        sort_mapping = {
            'created_at': Ad.created_at,
            'date_found': Ad.date_found,
            'duration_days': Ad.duration_days,
            'overall_score': AdAnalysis.overall_score,
            'hook_score': AdAnalysis.hook_score
        }
        
        column = sort_mapping.get(sort_by, Ad.created_at)
        
        if sort_by in ['overall_score', 'hook_score']:
            query = query.outerjoin(Ad.analysis)
        
        order_func = asc if sort_order.lower() == 'asc' else desc
        query = query.order_by(order_func(column))
        
        return query
    
    def _convert_to_dto(self, ad: Ad) -> AdResponseDTO:
        """
        Convert Ad entity to AdResponseDTO.
        """
        if not ad:
            return None  # type: ignore
            
        try:
            import json
            
            competitor_dto = None
            if ad.competitor:
                competitor_dto = CompetitorResponseDTO(
                    id=ad.competitor.id,
                    name=ad.competitor.name,
                    page_id=ad.competitor.page_id,
                    is_active=ad.competitor.is_active,
                    ads_count=0  # Will be calculated later if needed
                )
            
            # Extract media information
            media_type = None
            media_url = None
            main_image_urls = []
            main_video_urls = []
            
            # Initialize text content variables
            ad_copy = None
            main_title = None
            main_body_text = None
            main_caption = None
            
            # Process creatives if they exist
            creatives_data = []
            if ad.creatives:
                # Handle the case when creatives is a string (JSON string)
                if isinstance(ad.creatives, str):
                    try:
                        creatives_data = json.loads(ad.creatives)
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse creatives JSON for ad {ad.id}")
                        creatives_data = []
                else:
                    # Creatives is already a list or dict
                    creatives_data = ad.creatives
                
                # If creatives_data is a dict, convert to list for consistency
                if isinstance(creatives_data, dict):
                    creatives_data = [creatives_data]
                
                # Process each creative
                if isinstance(creatives_data, list):
                    for creative in creatives_data:
                        # Extract media information
                        if creative.get('media'):
                            for media in creative.get('media'):
                                if media.get('type') == 'Video':
                                    main_video_urls.append(media.get('url'))
                                    if not media_type:
                                        media_type = 'Video'
                                        media_url = media.get('url')
                                elif media.get('type') == 'Image':
                                    main_image_urls.append(media.get('url'))
                                    if not media_type and not main_video_urls:
                                        media_type = 'Image'
                                        media_url = media.get('url')
                        
                        # Extract text content
                        if creative.get('headline') and not main_title:
                            main_title = creative.get('headline')
                        if creative.get('body') and not main_body_text:
                            main_body_text = creative.get('body')
                        if creative.get('link', {}).get('caption') and not main_caption:
                            main_caption = creative.get('link', {}).get('caption')
            
            # Combine text fields for ad_copy
            all_text = [main_title, main_body_text, main_caption]
            ad_copy = " ".join([t for t in all_text if t])
            
            # Extract page info
            page_name = None
            page_id = None
            
            if ad.competitor:
                page_name = ad.competitor.name
                page_id = ad.competitor.page_id
                
            # Extract dates and status
            start_date = None
            end_date = None
            is_active = False
            
            # Parse meta data
            meta_data = {}
            if ad.meta:
                # Handle the case when meta is a string (JSON string)
                if isinstance(ad.meta, str):
                    try:
                        meta_data = json.loads(ad.meta)
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse meta JSON for ad {ad.id}")
                        meta_data = {}
                else:
                    meta_data = ad.meta
                
                if isinstance(meta_data, dict):
                    if 'start_date' in meta_data:
                        start_date = meta_data.get('start_date')
                    if 'end_date' in meta_data:
                        end_date = meta_data.get('end_date')
                    if 'is_active' in meta_data:
                        is_active = meta_data.get('is_active', False)
            
            # Extract CTA info
            cta_text = None
            cta_type = None
            
            if isinstance(creatives_data, list):
                for creative in creatives_data:
                    if creative.get('cta', {}).get('text') and not cta_text:
                        cta_text = creative.get('cta', {}).get('text')
                    if creative.get('cta', {}).get('type') and not cta_type:
                        cta_type = creative.get('cta', {}).get('type')
            
            # Parse targeting data
            targeting_data = {}
            targeted_countries = []
            if ad.targeting:
                # Handle the case when targeting is a string (JSON string)
                if isinstance(ad.targeting, str):
                    try:
                        targeting_data = json.loads(ad.targeting)
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse targeting JSON for ad {ad.id}")
                        targeting_data = {}
                else:
                    targeting_data = ad.targeting
                
                if isinstance(targeting_data, dict) and 'locations' in targeting_data:
                    targeted_countries = targeting_data.get('locations', [])
            
            # Parse lead form data
            lead_form_data = {}
            if ad.lead_form:
                # Handle the case when lead_form is a string (JSON string)
                if isinstance(ad.lead_form, str):
                    try:
                        lead_form_data = json.loads(ad.lead_form)
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse lead_form JSON for ad {ad.id}")
                        lead_form_data = {}
                else:
                    lead_form_data = ad.lead_form
            
            # Extract publisher platforms - using empty list as default since we don't have this data
            publisher_platform = []
            
            # Create the analysis DTO if analysis exists
            analysis_dto = None
            if ad.analysis:
                analysis_dto = AdAnalysisResponseDTO(
                    id=ad.analysis.id,
                    summary=ad.analysis.summary,
                    hook_score=ad.analysis.hook_score,
                    overall_score=ad.analysis.overall_score,
                    confidence_score=ad.analysis.confidence_score,
                    target_audience=ad.analysis.target_audience,
                    content_themes=ad.analysis.content_themes,
                    analysis_version=ad.analysis.analysis_version,
                    created_at=ad.analysis.created_at,
                    updated_at=ad.analysis.updated_at
                )
            
            # Create the ad response DTO
            return AdResponseDTO(
                id=ad.id,
                ad_archive_id=ad.ad_archive_id,
                competitor=competitor_dto,
                
                # Content fields
                ad_copy=ad_copy,
                main_title=main_title,
                main_body_text=main_body_text,
                main_caption=main_caption,
                
                # Media fields
                media_type=media_type,
                media_url=media_url,
                main_image_urls=main_image_urls,
                main_video_urls=main_video_urls,
                
                # Page info
                page_name=page_name,
                page_id=page_id,
                
                # Platform and targeting
                publisher_platform=publisher_platform,
                targeted_countries=targeted_countries,
                
                # Performance indicators - these are placeholder values
                impressions_text=None,
                spend=None,
                
                # CTA fields
                cta_text=cta_text,
                cta_type=cta_type,
                
                # Dates and status
                date_found=ad.date_found,
                start_date=start_date,
                end_date=end_date,
                is_active=is_active,
                duration_days=ad.duration_days,
                
                # Timestamps
                created_at=ad.created_at,
                updated_at=ad.updated_at,
                
                # Analysis data
                analysis=analysis_dto,
                
                # Raw data fields - use the parsed dictionaries
                meta=meta_data,
                targeting=targeting_data,
                lead_form=lead_form_data,
                creatives=creatives_data,
                
                # AdSet fields - will be populated later when needed
                ad_set_id=ad.ad_set_id,
                variant_count=None,  # This will be set separately when needed
                ad_set_created_at=None,
                ad_set_first_seen_date=None,
                ad_set_last_seen_date=None
            )
            
        except Exception as e:
            logger.error(f"Error converting ad to DTO: {str(e)}")
            raise
    
    def _convert_to_detail_dto(self, ad: Ad) -> AdDetailResponseDTO:
        """Helper to convert Ad model to AdDetailResponseDTO"""
        if not ad:
            return None  # type: ignore
            
        analysis_dto = None
        if hasattr(ad, 'analysis') and ad.analysis is not None:
            analysis_dto = AdAnalysisResponseDTO.model_validate(ad.analysis)
        
        competitor_dto = None
        if hasattr(ad, 'competitor') and ad.competitor is not None:
            competitor_dto = CompetitorResponseDTO.model_validate(ad.competitor)

        creatives = []
        if hasattr(ad, 'creatives') and ad.creatives is not None:
            creatives = ad.creatives

        first_creative = creatives[0] if creatives else {}
        main_media = first_creative.get('media', [])[0] if first_creative.get('media') else {}

        # Prepare meta data
        meta_is_active = None
        meta_start_date = None
        meta_end_date = None
        
        if hasattr(ad, 'meta') and ad.meta is not None:
            meta = ad.meta
            meta_is_active = meta.get('is_active')
            meta_start_date = meta.get('start_date')
            meta_end_date = meta.get('end_date')

        # The AdDetailResponseDTO has more fields, so we map them explicitly
        return AdDetailResponseDTO(
            id=ad.id,
            ad_archive_id=ad.ad_archive_id,
            competitor=competitor_dto,
            analysis=analysis_dto,
            meta=ad.meta,
            targeting=ad.targeting,
            lead_form=ad.lead_form,
            creatives=creatives,
            date_found=ad.date_found,
            created_at=ad.created_at,
            updated_at=ad.updated_at,
            raw_data=ad.raw_data,
            
            # Populated from nested data
            ad_copy=first_creative.get('body'),
            main_title=first_creative.get('headline'),
            cta_text=first_creative.get('cta', {}).get('text'),
            cta_type=first_creative.get('cta', {}).get('type'),
            media_type=main_media.get('type'),
            media_url=main_media.get('url'),
            is_active=meta_is_active,
            start_date=meta_start_date,
            end_date=meta_end_date,
            
            # Required fields with default values
            main_body_text=None,
            main_caption=None,
            main_link_url=None,
            main_link_description=None,
            main_image_urls=[],
            main_video_urls=[],
            page_name=None,
            page_id=None,
            page_like_count=None,
            page_categories=None,
            page_profile_uri=None,
            page_profile_picture_url=None,
            publisher_platform=None,
            targeted_countries=None,
            display_format=None,
            impressions_text=None,
            impressions_index=None,
            spend=None,
            currency=None,
            extra_texts=None,
            extra_links=None,
            contains_sensitive_content=None,
            contains_digital_created_media=None
        )

    def delete_all_ads(self) -> int:
        """
        Delete all ads and their analyses from the database.
        
        Returns:
            Number of ads deleted
        """
        try:
            # First delete all analyses
            analyses_deleted = self.db.query(AdAnalysis).delete(synchronize_session=False)
            
            # Then delete all ads
            ads_deleted = self.db.query(Ad).delete(synchronize_session=False)
            
            self.db.commit()
            
            logger.info(f"Successfully deleted ALL {ads_deleted} ads and {analyses_deleted} analyses")
            return ads_deleted
            
        except Exception as e:
            logger.error(f"Error deleting all ads: {str(e)}")
            self.db.rollback()
            raise

    def get_ads_in_set(self, ad_set_id: int, page: int = 1, page_size: int = 20) -> Optional[PaginatedAdResponseDTO]:
        """
        Get all ads within a specific ad set with pagination.
        
        Args:
            ad_set_id: The ID of the ad set
            page: Page number (1-indexed)
            page_size: Number of items per page
            
        Returns:
            PaginatedAdResponseDTO with all ads in the set and pagination metadata
        """
        try:
            # Check if the ad set exists
            ad_set = self.db.query(AdSet).filter(AdSet.id == ad_set_id).first()
            if not ad_set:
                return None
                
            # Build query for ads in this set
            query = self.db.query(Ad).options(
                joinedload(Ad.competitor),
                joinedload(Ad.analysis)
            ).filter(Ad.ad_set_id == ad_set_id)
            
            # Get total count before pagination
            total_items = query.count()
            
            # Apply pagination
            offset = (page - 1) * page_size
            ads = query.offset(offset).limit(page_size).all()
            
            # Convert to DTOs
            ad_dtos = []
            for ad in ads:
                try:
                    ad_dto = self._convert_to_dto(ad)
                    if ad_dto:
                        ad_dtos.append(ad_dto)
                except Exception as e:
                    logger.error(f"Error converting ad {ad.id} to DTO: {str(e)}")
            
            # Calculate pagination metadata
            total_pages = (total_items + page_size - 1) // page_size
            pagination = PaginationMetadata(
                page=page,
                page_size=page_size,
                total_items=total_items,
                total_pages=total_pages,
                has_next=page < total_pages,
                has_previous=page > 1
            )
            
            return PaginatedAdResponseDTO(
                data=ad_dtos,
                pagination=pagination
            )
            
        except Exception as e:
            logger.error(f"Error fetching ads in set {ad_set_id}: {str(e)}")
            raise

    def get_ad_sets(self, page: int = 1, page_size: int = 20, sort_by: str = "created_at", sort_order: str = "desc") -> Optional[PaginatedAdResponseDTO]:
        """
        Get all ad sets with pagination.
        
        Args:
            page: Page number (1-indexed)
            page_size: Number of items per page
            sort_by: Field to sort by (created_at, variant_count, etc.)
            sort_order: Sort direction (asc, desc)
            
        Returns:
            PaginatedAdResponseDTO with the best ad from each set and pagination metadata
        """
        try:
            # Build base query on AdSet model
            query = self.db.query(AdSet).options(
                joinedload(AdSet.best_ad).joinedload(Ad.competitor),
                joinedload(AdSet.best_ad).joinedload(Ad.analysis)
            )
            
            # Apply sorting
            direction = desc if sort_order.lower() == "desc" else asc
            
            if sort_by == "created_at":
                query = query.order_by(direction(AdSet.created_at))
            elif sort_by == "variant_count":
                query = query.order_by(direction(AdSet.variant_count))
            elif sort_by == "first_seen_date":
                query = query.order_by(direction(AdSet.first_seen_date))
            elif sort_by == "last_seen_date":
                query = query.order_by(direction(AdSet.last_seen_date))
            else:
                query = query.order_by(direction(AdSet.created_at))
            
            # Get total count before pagination
            total_items = query.count()
            
            # Apply pagination
            offset = (page - 1) * page_size
            ad_sets = query.offset(offset).limit(page_size).all()
            
            # Convert to DTOs with AdSet information
            ad_dtos = []
            for ad_set in ad_sets:
                if ad_set.best_ad:
                    # Convert the best ad to DTO and add AdSet information
                    ad_dto = self._convert_to_dto(ad_set.best_ad)
                    # Augment the Ad DTO with metadata from its parent AdSet
                    ad_dto.ad_set_id = ad_set.id
                    ad_dto.variant_count = ad_set.variant_count
                    ad_dto.ad_set_created_at = ad_set.created_at
                    ad_dto.ad_set_first_seen_date = ad_set.first_seen_date
                    ad_dto.ad_set_last_seen_date = ad_set.last_seen_date
                    ad_dtos.append(ad_dto)
                else:
                    # If there's no best_ad, fetch the first ad in the set
                    first_ad = self.db.query(Ad).filter(Ad.ad_set_id == ad_set.id).first()
                    if first_ad:
                        ad_dto = self._convert_to_dto(first_ad)
                        ad_dto.ad_set_id = ad_set.id
                        ad_dto.variant_count = ad_set.variant_count
                        ad_dto.ad_set_created_at = ad_set.created_at
                        ad_dto.ad_set_first_seen_date = ad_set.first_seen_date
                        ad_dto.ad_set_last_seen_date = ad_set.last_seen_date
                        ad_dtos.append(ad_dto)
            
            # Calculate pagination metadata
            total_pages = (total_items + page_size - 1) // page_size
            pagination = PaginationMetadata(
                page=page,
                page_size=page_size,
                total_items=total_items,
                total_pages=total_pages,
                has_next=page < total_pages,
                has_previous=page > 1
            )
            
            return PaginatedAdResponseDTO(
                data=ad_dtos,
                pagination=pagination
            )
            
        except Exception as e:
            logger.error(f"Error fetching ad sets: {str(e)}")
            raise


def get_ad_service(db: Session) -> AdService:
    """
    Factory function to get AdService instance.
    
    Args:
        db: Database session
        
    Returns:
        AdService instance
    """
    return AdService(db) 


================================================
File: app/services/ai_service.py
================================================
import json
import logging
from typing import Dict, Any, Optional
import google.generativeai as genai
from app.core.config import settings

logger = logging.getLogger(__name__)

class AIService:
    """
    Service class for interacting with Google Gemini AI API
    to analyze ad content and generate insights.
    """
    
    def __init__(self):
        """Initialize the AI service with Google AI configuration."""
        if not settings.GOOGLE_AI_API_KEY:
            raise ValueError("GOOGLE_AI_API_KEY is not configured")
        
        # Configure the Google AI API
        genai.configure(api_key=settings.GOOGLE_AI_API_KEY)
        self.model = genai.GenerativeModel(settings.GOOGLE_AI_MODEL)
        
    def analyze_ad_content(self, ad_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze ad content using Google Gemini AI API.
        
        Args:
            ad_data: Dictionary containing ad information including:
                - ad_copy: Main ad text
                - main_title: Ad title
                - main_body_text: Ad body text
                - main_caption: Ad caption
                - cta_text: Call-to-action text
                - media_type: Type of media (video, image, etc.)
                - page_name: Advertiser page name
                - targeted_countries: List of targeted countries
                - card_titles: List of card titles (for carousel ads)
                - card_bodies: List of card body texts (for carousel ads)
                
        Returns:
            Dict containing AI analysis results
        """
        try:
            # Construct the analysis prompt
            prompt = self._build_analysis_prompt(ad_data)
            
            logger.info(f"Sending analysis request to Google AI for ad {ad_data.get('ad_archive_id', 'unknown')}")
            
            # Generate response from AI
            response = self.model.generate_content(prompt)
            
            # Parse the response
            analysis_result = self._parse_ai_response(response.text)
            
            # Add metadata
            analysis_result['ai_prompts'] = {
                'analysis_prompt': prompt,
                'model_used': settings.GOOGLE_AI_MODEL
            }
            analysis_result['raw_ai_response'] = {
                'full_response': response.text,
                'model': settings.GOOGLE_AI_MODEL
            }
            analysis_result['analysis_version'] = 'v1.0-gemini'
            
            logger.info(f"Successfully analyzed ad {ad_data.get('ad_archive_id', 'unknown')}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Error analyzing ad content: {str(e)}")
            raise
    
    def _build_analysis_prompt(self, ad_data: Dict[str, Any]) -> str:
        """
        Build a comprehensive analysis prompt for the AI model.
        
        Args:
            ad_data: Dictionary containing ad information
            
        Returns:
            Formatted prompt string
        """
        # Extract relevant ad content
        ad_copy = ad_data.get('ad_copy', '')
        main_title = ad_data.get('main_title', '')
        main_body_text = ad_data.get('main_body_text', '')
        main_caption = ad_data.get('main_caption', '')
        cta_text = ad_data.get('cta_text', '')
        media_type = ad_data.get('media_type', '')
        page_name = ad_data.get('page_name', '')
        targeted_countries = ad_data.get('targeted_countries', [])
        card_titles = ad_data.get('card_titles', [])
        card_bodies = ad_data.get('card_bodies', [])
        
        # Build the comprehensive prompt
        prompt = f"""
You are an expert digital marketing analyst specializing in Facebook/Meta advertising. 
Analyze the following ad content and provide a comprehensive analysis in JSON format.

AD CONTENT TO ANALYZE:
======================
Advertiser: {page_name}
Main Title: {main_title}
Main Body Text: {main_body_text}
Ad Copy: {ad_copy}
Caption: {main_caption}
Call-to-Action: {cta_text}
Media Type: {media_type}
Target Countries: {', '.join(targeted_countries) if targeted_countries else 'Not specified'}
"""

        # Add carousel content if present
        if card_titles or card_bodies:
            prompt += f"\nCarousel Cards:\n"
            for i, (title, body) in enumerate(zip(card_titles or [], card_bodies or [])):
                prompt += f"Card {i+1} - Title: {title}, Body: {body}\n"

        # Add analysis instructions
        prompt += f"""

ANALYSIS REQUIREMENTS:
======================
Provide your analysis as a valid JSON object with the following structure:
{{
    "summary": "A comprehensive 2-3 sentence summary of the ad's main message and approach",
    "hook_score": 8.5,
    "overall_score": 7.2,
    "target_audience": "Specific target audience description",
    "content_themes": ["theme1", "theme2", "theme3"],
    "effectiveness_analysis": {{
        "strengths": ["strength1", "strength2"],
        "weaknesses": ["weakness1", "weakness2"],
        "recommendations": ["recommendation1", "recommendation2"]
    }},
    "ad_format_analysis": {{
        "format_effectiveness": "Analysis of the ad format choice",
        "media_type_appropriateness": "Assessment of media type choice",
        "cta_effectiveness": "Analysis of call-to-action effectiveness"
    }},
    "competitor_insights": {{
        "positioning": "How the brand positions itself",
        "unique_selling_points": ["USP1", "USP2"],
        "competitive_advantage": "Main competitive advantage highlighted"
    }},
    "performance_predictions": {{
        "predicted_engagement_rate": 2.5,
        "predicted_click_through_rate": 1.8,
        "audience_fit_score": 8.0,
        "conversion_potential": "high/medium/low"
    }},
    "confidence_score": 0.85
}}

SCORING CRITERIA:
=================
- hook_score (1-10): Rate how engaging and attention-grabbing the opening/hook is
- overall_score (1-10): Rate the overall effectiveness of the ad
- confidence_score (0-1): Your confidence level in this analysis

ANALYSIS FOCUS:
===============
- Identify the primary value proposition
- Assess the emotional appeal and psychological triggers
- Evaluate the clarity and persuasiveness of the message
- Consider the target market fit
- Analyze the call-to-action effectiveness
- Assess visual and textual harmony (if applicable)

Return ONLY the JSON object, no additional text or formatting.
"""
        
        return prompt
    
    def _parse_ai_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parse the AI response and extract structured data.
        
        Args:
            response_text: Raw response text from AI
            
        Returns:
            Parsed analysis results
        """
        try:
            # Try to find JSON in the response
            response_text = response_text.strip()
            
            # Remove any markdown formatting
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            # Parse JSON
            analysis_result = json.loads(response_text)
            
            # Validate required fields and set defaults
            analysis_result.setdefault('summary', 'AI analysis completed')
            analysis_result.setdefault('hook_score', 5.0)
            analysis_result.setdefault('overall_score', 5.0)
            analysis_result.setdefault('confidence_score', 0.7)
            analysis_result.setdefault('target_audience', 'General audience')
            analysis_result.setdefault('content_themes', [])
            
            # Ensure scores are within valid range
            analysis_result['hook_score'] = max(0, min(10, float(analysis_result['hook_score'])))
            analysis_result['overall_score'] = max(0, min(10, float(analysis_result['overall_score'])))
            analysis_result['confidence_score'] = max(0, min(1, float(analysis_result['confidence_score'])))
            
            return analysis_result
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response as JSON: {e}")
            logger.error(f"Response text: {response_text}")
            
            # Return fallback analysis
            return {
                'summary': 'AI analysis completed with parsing error',
                'hook_score': 5.0,
                'overall_score': 5.0,
                'confidence_score': 0.5,
                'target_audience': 'General audience',
                'content_themes': [],
                'parse_error': str(e),
                'raw_response': response_text
            }
        except Exception as e:
            logger.error(f"Error parsing AI response: {e}")
            raise


# Create a singleton instance
try:
    ai_service = AIService() if settings.AI_ANALYSIS_ENABLED and settings.GOOGLE_AI_API_KEY else None
except Exception as e:
    # If there's an error creating the service (e.g., missing dependencies), set to None
    logger.warning(f"Failed to create AI service: {e}")
    ai_service = None


def get_ai_service() -> Optional[AIService]:
    """
    Get the AI service instance.
    
    Returns:
        AIService instance or None if not configured
    """
    return ai_service 


================================================
File: app/services/competitor_service.py
================================================
from typing import List, Optional, Tuple
from sqlalchemy.orm import Session
from sqlalchemy import func, and_, or_
from fastapi import HTTPException
import logging
from datetime import datetime
import math

from app.models.competitor import Competitor
from app.models.ad import Ad
from app.models.ad_analysis import AdAnalysis
from app.models.dto.competitor_dto import (
    CompetitorCreateDTO,
    CompetitorUpdateDTO,
    CompetitorResponseDTO,
    CompetitorDetailResponseDTO,
    PaginatedCompetitorResponseDTO,
    CompetitorFilterParams,
    CompetitorStatsResponseDTO
)

logger = logging.getLogger(__name__)


class CompetitorService:
    """Service for managing competitor operations"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def create_competitor(self, competitor_data: CompetitorCreateDTO) -> CompetitorResponseDTO:
        """Create a new competitor"""
        try:
            # Check if competitor with same page_id already exists
            existing = self.db.query(Competitor).filter(
                Competitor.page_id == competitor_data.page_id
            ).first()
            
            if existing:
                raise HTTPException(
                    status_code=400,
                    detail=f"Competitor with page_id '{competitor_data.page_id}' already exists"
                )
            
            # Create new competitor
            competitor = Competitor(
                name=competitor_data.name,
                page_id=competitor_data.page_id,
                is_active=competitor_data.is_active
            )
            
            self.db.add(competitor)
            self.db.commit()
            self.db.refresh(competitor)
            
            logger.info(f"Created new competitor: {competitor.name} (ID: {competitor.id})")
            
            # Create DTO and set values explicitly
            dto = CompetitorResponseDTO.model_validate(competitor)
            dto.ads_count = 0
            return dto
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error creating competitor: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error creating competitor: {str(e)}")
    
    def get_competitors(self, filters: CompetitorFilterParams) -> PaginatedCompetitorResponseDTO:
        """Get paginated list of competitors with filtering"""
        try:
            # Build base query
            query = self.db.query(Competitor)
            
            # Apply filters
            if filters.is_active is not None:
                query = query.filter(Competitor.is_active == filters.is_active)
            
            if filters.search:
                search_term = f"%{filters.search}%"
                query = query.filter(
                    or_(
                        Competitor.name.ilike(search_term),
                        Competitor.page_id.ilike(search_term)
                    )
                )
            
            # Get total count before pagination
            total = query.count()
            
            # Apply sorting
            if filters.sort_by == "name":
                order_column = Competitor.name
            elif filters.sort_by == "page_id":
                order_column = Competitor.page_id
            elif filters.sort_by == "is_active":
                order_column = Competitor.is_active
            elif filters.sort_by == "updated_at":
                order_column = Competitor.updated_at
            else:
                order_column = Competitor.created_at
            
            if filters.sort_order == "asc":
                query = query.order_by(order_column.asc())
            else:
                query = query.order_by(order_column.desc())
            
            # Apply pagination
            offset = (filters.page - 1) * filters.page_size
            competitors = query.offset(offset).limit(filters.page_size).all()
            
            # Get ads count for each competitor
            competitor_ids = [comp.id for comp in competitors]
            ads_counts = {}
            
            if competitor_ids:
                ads_count_query = self.db.query(
                    Ad.competitor_id,
                    func.count(Ad.id).label('ads_count')
                ).filter(
                    Ad.competitor_id.in_(competitor_ids)
                ).group_by(Ad.competitor_id).all()
                
                ads_counts = {comp_id: count for comp_id, count in ads_count_query}
            
            # Build response
            competitor_data = []
            for competitor in competitors:
                # Create DTO from ORM and set ads_count explicitly
                dto = CompetitorResponseDTO.model_validate(competitor)
                dto.ads_count = ads_counts.get(competitor.id, 0)
                competitor_data.append(dto)
            
            # Calculate pagination info
            total_pages = math.ceil(total / filters.page_size)
            has_next = filters.page < total_pages
            has_previous = filters.page > 1
            
            return PaginatedCompetitorResponseDTO(
                data=competitor_data,
                total=total,
                page=filters.page,
                page_size=filters.page_size,
                total_pages=total_pages,
                has_next=has_next,
                has_previous=has_previous
            )
            
        except Exception as e:
            logger.error(f"Error fetching competitors: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error fetching competitors: {str(e)}")
    
    def get_competitor_by_id(self, competitor_id: int) -> CompetitorDetailResponseDTO:
        """Get detailed information about a specific competitor"""
        try:
            competitor = self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).first()
            
            if not competitor:
                raise HTTPException(status_code=404, detail="Competitor not found")
            
            # Get ads statistics
            total_ads = self.db.query(Ad).filter(Ad.competitor_id == competitor_id).count()
            active_ads = self.db.query(Ad).filter(
                Ad.competitor_id == competitor_id,
                Ad.meta['is_active'].as_boolean() == True
            ).count()
            
            # Get analyzed ads count
            analyzed_ads = self.db.query(AdAnalysis).join(
                Ad, AdAnalysis.ad_id == Ad.id
            ).filter(
                Ad.competitor_id == competitor_id
            ).count()
            
            # Create DTO from ORM and set fields explicitly
            dto = CompetitorDetailResponseDTO.model_validate(competitor)
            dto.ads_count = total_ads
            dto.active_ads_count = active_ads
            dto.analyzed_ads_count = analyzed_ads
            return dto
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Error fetching competitor {competitor_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error fetching competitor: {str(e)}")
    
    def bulk_delete_competitors(self, competitor_ids: List[int]) -> dict:
        """
        Bulk delete competitors.
        - Soft delete competitors with ads.
        - Hard delete competitors without ads.
        """
        if not competitor_ids:
            raise HTTPException(status_code=400, detail="No competitor IDs provided")

        soft_deleted_count = 0
        hard_deleted_count = 0
        not_found_count = 0
        
        try:
            competitors_to_process = self.db.query(Competitor).filter(
                Competitor.id.in_(competitor_ids)
            ).all()
            
            found_ids = {c.id for c in competitors_to_process}
            not_found_count = len(competitor_ids) - len(found_ids)

            ids_to_check_ads = [comp.id for comp in competitors_to_process]
            
            ads_counts_query = self.db.query(
                Ad.competitor_id,
                func.count(Ad.id).label('ads_count')
            ).filter(
                Ad.competitor_id.in_(ids_to_check_ads)
            ).group_by(Ad.competitor_id).all()

            ads_counts = {comp_id: count for comp_id, count in ads_counts_query}
            
            ids_for_soft_delete = []
            ids_for_hard_delete = []

            for comp_id in ids_to_check_ads:
                if ads_counts.get(comp_id, 0) > 0:
                    ids_for_soft_delete.append(comp_id)
                else:
                    ids_for_hard_delete.append(comp_id)
            
            # Perform soft deletes
            if ids_for_soft_delete:
                self.db.query(Competitor).filter(
                    Competitor.id.in_(ids_for_soft_delete)
                ).update(
                    {'is_active': False, 'updated_at': datetime.utcnow()},
                    synchronize_session=False
                )
                soft_deleted_count = len(ids_for_soft_delete)
            
            # Perform hard deletes
            if ids_for_hard_delete:
                self.db.query(Competitor).filter(
                    Competitor.id.in_(ids_for_hard_delete)
                ).delete(synchronize_session=False)
                hard_deleted_count = len(ids_for_hard_delete)
            
            self.db.commit()
            
            logger.info(
                f"Bulk delete completed. Soft deleted: {soft_deleted_count}, "
                f"Hard deleted: {hard_deleted_count}, Not found: {not_found_count}"
            )
            
            return {
                "message": "Bulk delete operation completed.",
                "soft_deleted_count": soft_deleted_count,
                "hard_deleted_count": hard_deleted_count,
                "not_found_count": not_found_count
            }

        except Exception as e:
            self.db.rollback()
            logger.error(f"Error during bulk competitor deletion: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"An error occurred during bulk deletion: {str(e)}"
            )
    
    def update_competitor(
        self, 
        competitor_id: int, 
        competitor_data: CompetitorUpdateDTO
    ) -> CompetitorDetailResponseDTO:
        """Update an existing competitor"""
        try:
            competitor = self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).first()
            
            if not competitor:
                raise HTTPException(status_code=404, detail="Competitor not found")
            
            # Check if page_id is being updated and if it conflicts with existing competitor
            if competitor_data.page_id and competitor_data.page_id != competitor.page_id:
                existing = self.db.query(Competitor).filter(
                    and_(
                        Competitor.page_id == competitor_data.page_id,
                        Competitor.id != competitor_id
                    )
                ).first()
                
                if existing:
                    raise HTTPException(
                        status_code=400,
                        detail=f"Another competitor with page_id '{competitor_data.page_id}' already exists"
                    )
            
            # Update fields using SQLAlchemy's update method
            update_data = {}
            if competitor_data.name is not None:
                update_data['name'] = competitor_data.name
            if competitor_data.page_id is not None:
                update_data['page_id'] = competitor_data.page_id
            if competitor_data.is_active is not None:
                update_data['is_active'] = competitor_data.is_active
            
            update_data['updated_at'] = datetime.utcnow()
            
            self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).update(
                update_data,
                synchronize_session=False
            )
            
            self.db.commit()
            self.db.refresh(competitor)
            
            logger.info(f"Updated competitor: {competitor.name} (ID: {competitor.id})")
            
            # Return detailed response
            return self.get_competitor_by_id(competitor_id)
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error updating competitor {competitor_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error updating competitor: {str(e)}")
    
    def delete_competitor(self, competitor_id: int) -> dict:
        """Delete a competitor (soft delete by setting is_active to False)"""
        try:
            competitor = self.db.query(Competitor).filter(
                Competitor.id == competitor_id
            ).first()
            
            if not competitor:
                raise HTTPException(status_code=404, detail="Competitor not found")
            
            # Check if competitor has ads
            ads_count = self.db.query(Ad).filter(Ad.competitor_id == competitor_id).count()
            
            if ads_count > 0:
                # Soft delete - using update instead of direct assignment
                self.db.query(Competitor).filter(
                    Competitor.id == competitor_id
                ).update(
                    {'is_active': False, 'updated_at': datetime.utcnow()},
                    synchronize_session=False
                )
                self.db.commit()
                
                logger.info(f"Soft deleted competitor: {competitor.name} (ID: {competitor.id}) - {ads_count} ads retained")
                
                return {
                    "message": f"Competitor '{competitor.name}' has been deactivated. {ads_count} ads are retained.",
                    "competitor_id": competitor_id,
                    "ads_count": ads_count,
                    "soft_delete": True
                }
            else:
                # Hard delete - no ads associated
                competitor_name = competitor.name
                self.db.delete(competitor)
                self.db.commit()
                
                logger.info(f"Hard deleted competitor: {competitor_name} (ID: {competitor_id}) - no ads")
                
                return {
                    "message": f"Competitor '{competitor_name}' has been permanently deleted.",
                    "competitor_id": competitor_id,
                    "ads_count": 0,
                    "soft_delete": False
                }
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error deleting competitor {competitor_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error deleting competitor: {str(e)}")
    
    def get_competitor_stats(self) -> CompetitorStatsResponseDTO:
        """Get comprehensive statistics about competitors"""
        try:
            # Basic counts
            total_competitors = self.db.query(Competitor).count()
            active_competitors = self.db.query(Competitor).filter(Competitor.is_active == True).count()
            inactive_competitors = total_competitors - active_competitors
            
            # Competitors with ads
            competitors_with_ads = self.db.query(Competitor.id).distinct().join(Ad).count()
            
            # Total ads across all competitors
            total_ads = self.db.query(Ad).count()
            
            # Average ads per competitor
            avg_ads_per_competitor = total_ads / total_competitors if total_competitors > 0 else 0
            
            return CompetitorStatsResponseDTO(
                total_competitors=total_competitors,
                active_competitors=active_competitors,
                inactive_competitors=inactive_competitors,
                competitors_with_ads=competitors_with_ads,
                total_ads_across_competitors=total_ads,
                avg_ads_per_competitor=round(avg_ads_per_competitor, 2)
            )
            
        except Exception as e:
            logger.error(f"Error fetching competitor stats: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error fetching competitor stats: {str(e)}")
    
    def search_competitors(self, search_term: str, limit: int = 50) -> List[CompetitorResponseDTO]:
        """Search competitors by name or page_id"""
        try:
            search_pattern = f"%{search_term}%"
            
            competitors = self.db.query(Competitor).filter(
                or_(
                    Competitor.name.ilike(search_pattern),
                    Competitor.page_id.ilike(search_pattern)
                )
            ).limit(limit).all()
            
            # Get ads count for each competitor
            competitor_ids = [comp.id for comp in competitors]
            ads_counts = {}
            
            if competitor_ids:
                ads_count_query = self.db.query(
                    Ad.competitor_id,
                    func.count(Ad.id).label('ads_count')
                ).filter(
                    Ad.competitor_id.in_(competitor_ids)
                ).group_by(Ad.competitor_id).all()
                
                ads_counts = {comp_id: count for comp_id, count in ads_count_query}
            
            # Create DTOs from ORM objects and set ads_count manually
            result = []
            for competitor in competitors:
                dto = CompetitorResponseDTO.model_validate(competitor)
                dto.ads_count = ads_counts.get(competitor.id, 0)
                result.append(dto)
            return result
            
        except Exception as e:
            logger.error(f"Error searching competitors: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error searching competitors: {str(e)}") 


================================================
File: app/services/creative_comparison_service.py
================================================
import logging
import concurrent.futures
import requests
from typing import Dict, List, Tuple, Any, Optional
import hashlib
from urllib.parse import urlparse

import av
from PIL import Image
import imagehash
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)

class CreativeComparisonService:
    """
    Service for comparing ad creatives (images and videos) to determine if they should be grouped
    into the same ad set. Uses perceptual hashing techniques for both images and videos.
    """
    
    def __init__(self, db: Session):
        self.db = db
        self.logger = logging.getLogger(__name__)
        
    # ===============================================================
    # Image Comparison Methods (from image_comparator_streamed.py)
    # ===============================================================
    
    def _download_and_hash_image(self, url: str) -> Optional[imagehash.ImageHash]:
        """Download an image via streaming and return its perceptual hash."""
        try:
            self.logger.debug(f"Downloading image: {url[:60]}...")
            resp = requests.get(url, stream=True, timeout=10)
            resp.raise_for_status()
            # Read raw bytes in chunks then hash
            img = Image.open(resp.raw)
            return imagehash.average_hash(img)  # average_hash is faster than phash
        except Exception as e:
            self.logger.error(f"Failed to process image {url[:60]}... â€“ {e}")
            return None
    
    def compare_images(self, image_url1: str, image_url2: str, cutoff: int = 5) -> Tuple[bool, Optional[int]]:
        """
        Return True/False if two remote images are similar, with optional hash difference.
        
        Args:
            image_url1: URL of the first image
            image_url2: URL of the second image
            cutoff: Maximum hash difference to consider images similar (default: 5)
            
        Returns:
            Tuple of (is_similar, hash_diff) where hash_diff may be None on error
        """
        # Skip comparison if URLs are identical
        if image_url1 == image_url2:
            return True, 0
        
        # Run downloads and hash computation in parallel to minimize wall time
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            fut1 = executor.submit(self._download_and_hash_image, image_url1)
            fut2 = executor.submit(self._download_and_hash_image, image_url2)
            
            try:
                hash1 = fut1.result()
                hash2 = fut2.result()
            except Exception as e:
                self.logger.error(f"Error comparing images: {e}")
                return False, None
            
            # Check for None results (errors in downloading or processing)
            if hash1 is None or hash2 is None:
                return False, None
            
            # Calculate hash difference
            diff = hash1 - hash2
            self.logger.debug(f"Image hash difference: {diff} (cutoff: {cutoff})")
            return diff <= cutoff, diff

    # ===============================================================
    # Video Comparison Methods (from video_comparator_fast.py)
    # ===============================================================
    
    def _duration_seconds(self, stream: av.video.stream.VideoStream, container: av.container.input.InputContainer) -> Optional[float]:
        """Return duration of the video stream in seconds (best effort)."""
        if stream.duration and stream.time_base:  # Prefer per-stream duration
            return float(stream.duration * stream.time_base)
        if container.duration:
            # container.duration is in micro-seconds
            return float(container.duration / 1_000_000)
        return None

    def _sample_video_hashes(self, url: str, samples: int = 6, resize: int = 8) -> List[imagehash.ImageHash]:
        """
        Return a list of perceptual hashes sampled from the remote video.
        
        It seeks to *samples* evenly spaced timestamps to avoid decoding the entire
        video. Falls back to sequential reading if seeking isn't supported.
        """
        self.logger.debug(f"Sampling video hashes from: {url[:60]}...")
        hashes = []
        
        try:
            container = av.open(url, timeout=15)
            video_stream = next(s for s in container.streams if s.type == "video")
            dur_s = self._duration_seconds(video_stream, container)

            def _hash_frame(frame):
                pil_img = frame.to_image()
                hashes.append(imagehash.average_hash(pil_img, hash_size=resize))

            if dur_s and dur_s > 0:
                # Seek method
                step = dur_s / (samples + 1)
                for i in range(samples):
                    ts = (i + 1) * step
                    # Convert seconds to stream timestamp units (time_base)
                    tb = float(video_stream.time_base) if video_stream.time_base else 1.0
                    pts = int(ts / tb)
                    try:
                        container.seek(pts, any_frame=False, backward=True, stream=video_stream)
                        frame = next(container.decode(video_stream))
                        _hash_frame(frame)
                    except (StopIteration, av.AVError):
                        # Fallback: cannot seek/decode
                        break
            else:
                # Unknown duration â€“ sequentially iterate until we collect enough hashes
                for frame in container.decode(video_stream):
                    _hash_frame(frame)
                    if len(hashes) >= samples:
                        break

            container.close()
            return hashes
            
        except Exception as e:
            self.logger.error(f"Error sampling video hashes: {e}")
            return []

    def compare_videos(
        self, 
        url1: str, 
        url2: str, 
        samples: int = 6, 
        hash_cutoff: int = 6, 
        similarity_threshold: float = 0.8
    ) -> Tuple[bool, float]:
        """
        Compare two online videos quickly.
        
        Args:
            url1, url2: Remote video URLs (HTTP/S)
            samples: Number of frames to compare (higher = more robust, slower)
            hash_cutoff: Maximum Hamming distance for two frame hashes to match
            similarity_threshold: Fraction of matching samples (0-1) to call videos similar
            
        Returns:
            (is_similar, similarity_score) â€“ similarity_score is 0-1
        """
        # Skip comparison if URLs are identical
        if url1 == url2:
            self.logger.info("Videos considered identical due to exact URL match")
            return True, 1.0

        # Quick metadata check via parallel HEAD requests
        def _head(url):
            try:
                r = requests.head(url, timeout=5, allow_redirects=True)
                return r.headers.get("ETag"), r.headers.get("Content-Length")
            except requests.RequestException:
                return None, None

        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as pool:
            etag1, length1 = pool.submit(_head, url1).result()
            etag2, length2 = pool.submit(_head, url2).result()

        # If both ETags present and equal, videos are identical
        if etag1 and etag2 and etag1 == etag2:
            self.logger.info("Videos considered identical via ETag match")
            return True, 1.0

        # If Content-Length identical and >0, assume high likelihood of same
        if length1 and length2 and length1 == length2 and length1 != '0':
            self.logger.info("Content-Length values match â€“ performing quick frame check")
            # Still need small verification; reduce samples to 2 for speed
            samples = min(samples, 2)

        # Collect hashes in parallel to overlap network I/O
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                fut1 = executor.submit(self._sample_video_hashes, url1, samples)
                fut2 = executor.submit(self._sample_video_hashes, url2, samples)
                hashes1 = fut1.result()
                hashes2 = fut2.result()
                
                common = min(len(hashes1), len(hashes2))
                if common == 0:
                    return False, 0.0
                
                matches = sum(1 for h1, h2 in zip(hashes1[:common], hashes2[:common]) 
                               if (h1 - h2) <= hash_cutoff)
                score = matches / common
                
                self.logger.info(f"Video comparison: {matches}/{common} frames matched (score: {score:.2f})")
                
                return score >= similarity_threshold, score
                
        except Exception as e:
            self.logger.error(f"Error comparing videos: {e}")
            return False, 0.0

    def compare_ad_videos(
        self,
        creative1: Dict,
        creative2: Dict,
        samples: int = 6,
        hash_cutoff: int = 6,
        similarity_threshold: float = 0.9
    ) -> Tuple[bool, float, str]:
        """
        Multi-factor video comparison using various video sources from ad creatives.
        Implements a fast-path system similar to group_ads.py:
        1. HQ URL match -> 2. Thumbnail comparison -> 3. LQ Stream comparison
        
        Args:
            creative1: First creative object with media data
            creative2: Second creative object with media data
            samples: Number of frames to compare if sampling is needed
            hash_cutoff: Maximum hash difference for frame comparison
            similarity_threshold: Minimum similarity score to consider videos similar
            
        Returns:
            Tuple of (is_similar, similarity_score, match_type)
            - is_similar: True if videos are considered similar
            - similarity_score: 0-1 score indicating similarity
            - match_type: What matched ('url', 'thumbnail', 'frames', 'none')
        """
        self.logger.info("Performing multi-factor video comparison")
        
        # 1. High-quality video URL match (fastest check)
        hq_url1 = self._extract_hq_video_url(creative1)
        hq_url2 = self._extract_hq_video_url(creative2)
        
        if hq_url1 and hq_url2 and hq_url1 == hq_url2:
            self.logger.info("Videos match via exact high-quality URL match")
            return True, 1.0, "url"
        
        # 2. Thumbnail comparison (second fastest)
        thumbnail_url1 = self._extract_thumbnail_url(creative1)
        thumbnail_url2 = self._extract_thumbnail_url(creative2)
        
        if thumbnail_url1 and thumbnail_url2:
            self.logger.info(f"Comparing video thumbnails: {thumbnail_url1[:50]}... vs {thumbnail_url2[:50]}...")
            similar, diff = self.compare_images(thumbnail_url1, thumbnail_url2)
            if similar:
                similarity = 1.0 - (diff / 10.0) if diff is not None else 0.8
                self.logger.info(f"Videos match via thumbnail comparison (diff: {diff}, similarity: {similarity:.2f})")
                return True, similarity, "thumbnail"
            else:
                self.logger.info(f"Thumbnails are different (diff: {diff})")
        else:
            self.logger.info("Cannot compare thumbnails: one or both are missing")
        
        # 3. Low-quality video stream comparison (most expensive, but most thorough)
        lq_url1 = self._extract_lq_video_url(creative1)
        lq_url2 = self._extract_lq_video_url(creative2)
        
        if lq_url1 and lq_url2:
            self.logger.info(f"Comparing video streams: {lq_url1[:50]}... vs {lq_url2[:50]}...")
            similar, score = self.compare_videos(lq_url1, lq_url2, samples, hash_cutoff, similarity_threshold)
            if similar:
                self.logger.info(f"Videos match via frame sampling (score: {score:.2f})")
                return True, score, "frames"
            else:
                self.logger.info(f"Video frames are different (score: {score:.2f})")
        else:
            self.logger.info("Cannot compare video streams: one or both are missing")
        
        # No match found through any method
        return False, 0.0, "none"
    
    def _extract_hq_video_url(self, creative: Dict) -> Optional[str]:
        """Extract high-quality video URL from creative"""
        # Check for video_hd_url in creative
        if creative.get("video_hd_url"):
            return creative["video_hd_url"]
            
        # Check in media list for high-quality video
        for media in creative.get("media", []):
            if media.get("type", "").lower() == "video" and media.get("url"):
                # If there's a "quality" indicator, check for HD
                if media.get("quality", "").lower() in ["hd", "high"]:
                    return media["url"]
        
        # If no explicit HQ URL, return the first video URL found
        for media in creative.get("media", []):
            if media.get("type", "").lower() == "video" and media.get("url"):
                return media["url"]
        
        return None
    
    def _extract_thumbnail_url(self, creative: Dict) -> Optional[str]:
        """Extract video thumbnail URL from creative"""
        # Check for explicit thumbnail
        if creative.get("video_preview_image_url"):
            return creative["video_preview_image_url"]
            
        # Check in main_image_urls if available
        if creative.get("main_image_urls") and len(creative["main_image_urls"]) > 0:
            return creative["main_image_urls"][0]
            
        # Check in media list for thumbnail or preview image
        for media in creative.get("media", []):
            if media.get("type", "").lower() == "image" and media.get("url"):
                if media.get("role", "").lower() in ["thumbnail", "preview"]:
                    return media["url"]
        
        # If no explicit thumbnail, return the first image URL found
        for media in creative.get("media", []):
            if media.get("type", "").lower() == "image" and media.get("url"):
                return media["url"]
        
        return None
    
    def _extract_lq_video_url(self, creative: Dict) -> Optional[str]:
        """Extract low-quality video URL from creative for frame sampling"""
        # Check for video_sd_url in creative
        if creative.get("video_sd_url"):
            return creative["video_sd_url"]
            
        # Check in main_video_urls if available
        if creative.get("main_video_urls") and len(creative["main_video_urls"]) > 1:
            # Often the second URL is the low-quality stream in Facebook ads
            return creative["main_video_urls"][1]
        elif creative.get("main_video_urls") and len(creative["main_video_urls"]) > 0:
            return creative["main_video_urls"][0]
            
        # Check in media list for low-quality video
        for media in creative.get("media", []):
            if media.get("type", "").lower() == "video" and media.get("url"):
                # If there's a "quality" indicator, check for SD/low
                if media.get("quality", "").lower() in ["sd", "low"]:
                    return media["url"]
        
        # If no specific LQ URL, use the HQ URL as fallback
        return self._extract_hq_video_url(creative)
    
    # ===============================================================
    # Creative Comparison Integration
    # ===============================================================
    
    def is_media_url(self, url: str) -> bool:
        """
        Check if a URL points to an image or video file based on extension.
        """
        if not url:
            return False
            
        try:
            path = urlparse(url).path.lower()
            media_extensions = [
                # Images
                '.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp',
                # Videos
                '.mp4', '.webm', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.m4v'
            ]
            return any(path.endswith(ext) for ext in media_extensions)
        except Exception:
            return False
    
    def get_media_type(self, url: str) -> str:
        """
        Determine if a URL is for an image, video, or unknown type.
        """
        if not url:
            return "unknown"
            
        try:
            path = urlparse(url).path.lower()
            
            # Check for video extensions
            video_extensions = ['.mp4', '.webm', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.m4v']
            if any(path.endswith(ext) for ext in video_extensions):
                return "video"
                
            # Check for image extensions
            image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
            if any(path.endswith(ext) for ext in image_extensions):
                return "image"
                
            # For Facebook URLs without extensions, use contextual clues
            if 'video.f' in url.lower():  # Common pattern in Facebook video URLs
                return "video"
            elif 'scontent.f' in url.lower():  # Common pattern in Facebook image URLs
                return "image"
                
            return "unknown"
            
        except Exception:
            return "unknown"
    
    def extract_media_from_creative(self, creative: Dict) -> List[Dict]:
        """
        Extract media items from a creative with their URLs and types.
        """
        media_items = []
        
        if not creative or not isinstance(creative, dict):
            return media_items
            
        # Extract media from the creative
        for media_item in creative.get("media", []):
            if not isinstance(media_item, dict):
                continue
                
            url = media_item.get("url")
            if not url:
                continue
                
            # Get media type from data or infer from URL
            media_type = media_item.get("type")
            if not media_type:
                media_type = self.get_media_type(url)
                
            media_items.append({
                "url": url,
                "type": media_type
            })
            
        return media_items
    
    def compare_ad_creatives(self, creative1: Dict, creative2: Dict) -> Tuple[bool, float, str]:
        """
        Compare two ad creatives to determine if they are similar enough to be grouped.
        
        Args:
            creative1: First creative object with media items
            creative2: Second creative object with media items
            
        Returns:
            Tuple of (is_similar, similarity_score, comparison_type)
            - is_similar: True if creatives are similar enough to group
            - similarity_score: 0-1 score of similarity (1 = identical)
            - comparison_type: What was compared ('text', 'image', 'video', 'mixed', 'none')
        """
        creative1_id = creative1.get("id", "unknown")
        creative2_id = creative2.get("id", "unknown")
        self.logger.info(f"Comparing creatives: {creative1_id} and {creative2_id}")
        
        # Extract ad text
        text1 = (creative1.get("body") or "").strip() if creative1 else ""
        text2 = (creative2.get("body") or "").strip() if creative2 else ""
        
        # If both creatives have significant text, compare the text
        if text1 and text2 and len(text1) > 20 and len(text2) > 20:
            self.logger.info(f"Both creatives have significant text. Comparing text content.")
            self.logger.debug(f"Text 1: {text1[:100]}...")
            self.logger.debug(f"Text 2: {text2[:100]}...")
            
            # Simple text comparison for now
            if text1 == text2:
                self.logger.info(f"Text content is identical. Marking as similar (text).")
                return True, 1.0, "text"
            
            # Calculate similarity using Jaccard similarity of words
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            if not words1 or not words2:
                text_similarity = 0.0
            else:
                intersection = len(words1.intersection(words2))
                union = len(words1.union(words2))
                text_similarity = intersection / union if union > 0 else 0.0
                
            self.logger.info(f"Text similarity score: {text_similarity:.2f} (threshold: 0.8)")
            if text_similarity > 0.8:  # High text similarity threshold
                self.logger.info(f"Text similarity exceeds threshold. Marking as similar (text).")
                return True, text_similarity, "text"
            else:
                self.logger.info(f"Text similarity below threshold. Checking media content.")
        else:
            self.logger.info(f"Insufficient text content. Checking media content.")
        
        # Extract media from both creatives
        media1 = self.extract_media_from_creative(creative1)
        media2 = self.extract_media_from_creative(creative2)
        
        # Log media information
        self.logger.info(f"Creative 1 has {len(media1)} media items")
        self.logger.info(f"Creative 2 has {len(media2)} media items")
        
        # No media to compare
        if not media1 or not media2:
            self.logger.info(f"No comparable media content found. Marking as not similar (none).")
            return False, 0.0, "none"
            
        # Compare media items
        best_similarity = 0.0
        comparison_type = "none"
        is_similar = False
        
        # Check for video media first (using the new multi-factor approach)
        has_video1 = any(item.get("type", "").lower() == "video" for item in media1)
        has_video2 = any(item.get("type", "").lower() == "video" for item in media2)
        
        if has_video1 and has_video2:
            self.logger.info("Both creatives have video content. Using enhanced video comparison.")
            similar, score, match_type = self.compare_ad_videos(creative1, creative2)
            if similar:
                self.logger.info(f"Video comparison successful: {match_type} match with score {score:.2f}")
                return True, score, "video"
            else:
                self.logger.info("Video comparison unsuccessful. Continuing with other media types.")
        
        # For each media item in creative1, find best match in creative2
        for item1 in media1:
            url1 = item1.get("url")
            type1 = item1.get("type", "").lower()
            
            if not url1 or not self.is_media_url(url1):
                continue
                
            for item2 in media2:
                url2 = item2.get("url")
                type2 = item2.get("type", "").lower()
                
                if not url2 or not self.is_media_url(url2):
                    continue
                
                self.logger.info(f"Comparing {type1} URL: {url1[:60]}... with {type2} URL: {url2[:60]}...")
                
                # Exact URL match = definite match
                if url1 == url2:
                    self.logger.info(f"Exact URL match found. Marking as similar ({type1}).")
                    return True, 1.0, type1
                
                # Compare images
                if type1 == "image" and type2 == "image":
                    self.logger.info(f"Performing image comparison")
                    similar, diff = self.compare_images(url1, url2)
                    similarity = 1.0 - (diff / 10.0) if diff is not None else 0.0
                    self.logger.info(f"Image comparison result: similar={similar}, hash_diff={diff}, similarity_score={similarity:.2f}")
                    
                    if similar and similarity > best_similarity:
                        best_similarity = similarity
                        is_similar = True
                        comparison_type = "image"
                        self.logger.info(f"Found better image match (score: {similarity:.2f})")
                
                # Individual video URL comparison (legacy approach, prefer compare_ad_videos above)
                elif type1 == "video" and type2 == "video" and not has_video1 and not has_video2:
                    self.logger.info(f"Performing legacy video comparison")
                    similar, score = self.compare_videos(url1, url2)
                    self.logger.info(f"Video comparison result: similar={similar}, similarity_score={score:.2f}")
                    
                    if similar and score > best_similarity:
                        best_similarity = score
                        is_similar = True
                        comparison_type = "video"
                        self.logger.info(f"Found better video match (score: {score:.2f})")
        
        self.logger.info(f"Final comparison result: similar={is_similar}, best_score={best_similarity:.2f}, type={comparison_type}")
        return is_similar, best_similarity, comparison_type
        
    def should_group_ads(self, ad_data1: Dict, ad_data2: Dict, text_weight: float = 0.3, media_weight: float = 0.7) -> bool:
        """
        Determine if two ads should be grouped into the same ad set based on their content.
        
        Args:
            ad_data1: First ad data with creatives
            ad_data2: Second ad data with creatives
            text_weight: Weight to give to text similarity (0-1)
            media_weight: Weight to give to media similarity (0-1)
            
        Returns:
            True if ads should be grouped, False otherwise
        """
        # Extract ad identifiers
        ad1_id = ad_data1.get("ad_archive_id", ad_data1.get("id", "unknown"))
        ad2_id = ad_data2.get("ad_archive_id", ad_data2.get("id", "unknown"))
        self.logger.info(f"Checking if ads should be grouped: Ad {ad1_id} with Ad {ad2_id}")
        
        # Check for identical ad IDs
        if ad1_id != "unknown" and ad1_id == ad2_id:
            self.logger.info(f"Same ad ID - identical ads")
            return True
        
        # Quick check for media type compatibility
        media_type1 = ad_data1.get("media_type", "").lower()
        media_type2 = ad_data2.get("media_type", "").lower()
        
        if media_type1 and media_type2 and media_type1 != media_type2:
            # Different media types (e.g., Image vs Video) - not similar
            self.logger.info(f"Different media types: {media_type1} vs {media_type2}")
            if not (("image" in media_type1 and "carousel" in media_type2) or 
                   ("carousel" in media_type1 and "image" in media_type2)):
                # Exception: Image and Carousel can be considered compatible
                return False
        
        # Extract creatives
        creatives1 = ad_data1.get("creatives", [])
        creatives2 = ad_data2.get("creatives", [])
        
        if not creatives1 or not creatives2:
            self.logger.info(f"One or both ads have no creatives. Not grouping.")
            return False
        
        self.logger.info(f"Ad {ad1_id} has {len(creatives1)} creative(s)")
        self.logger.info(f"Ad {ad2_id} has {len(creatives2)} creative(s)")
        
        # Helper function to normalize URL for comparison
        def normalize_url(url):
            if not url:
                return ""
            # Parse URL and remove query parameters for more robust comparison
            parsed = urlparse(url)
            # Return just the path part of the URL for comparison
            return f"{parsed.netloc}{parsed.path}".lower()
        
        # Perform efficient checks first - check for direct URL matches
        if media_type1 == "image" and media_type2 == "image":
            # For images, check if primary media URL matches (after normalization)
            url1 = normalize_url(ad_data1.get("media_url"))
            url2 = normalize_url(ad_data2.get("media_url"))
            if url1 and url2 and url1 == url2:
                self.logger.info(f"Exact media URL match for images. Grouping ads.")
                return True
                
            # Also check image URLs arrays
            image_urls1 = [normalize_url(url) for url in ad_data1.get("main_image_urls", []) if url]
            image_urls2 = [normalize_url(url) for url in ad_data2.get("main_image_urls", []) if url]
            
            # Check if any normalized image URLs match
            if image_urls1 and image_urls2:
                common_urls = set(image_urls1).intersection(set(image_urls2))
                if common_urls:
                    self.logger.info(f"Matching image URLs found in main_image_urls. Grouping ads.")
                    return True
        
        elif media_type1 == "video" and media_type2 == "video":
            # For videos, check for match in any video URLs (after normalization)
            video_urls1 = [normalize_url(url) for url in ad_data1.get("main_video_urls", []) if url]
            video_urls2 = [normalize_url(url) for url in ad_data2.get("main_video_urls", []) if url]
            
            # Check if any normalized video URLs match
            if video_urls1 and video_urls2:
                common_urls = set(video_urls1).intersection(set(video_urls2))
                if common_urls:
                    self.logger.info(f"Matching video URLs found. Grouping ads.")
                    return True
        
        # Compare the first creative from each ad (primary creative)
        # This could be expanded to compare all creatives in the future
        creative1 = creatives1[0] if len(creatives1) > 0 else None
        creative2 = creatives2[0] if len(creatives2) > 0 else None
        
        if not creative1 or not creative2:
            self.logger.info(f"Could not extract creatives for comparison. Not grouping.")
            return False
        
        # Use the enhanced comparison logic
        is_similar, similarity, comparison_type = self.compare_ad_creatives(creative1, creative2)
        
        self.logger.info(f"Ad comparison: similar={is_similar}, score={similarity:.2f}, type={comparison_type}")
        self.logger.info(f"Grouping decision: {'Group together' if is_similar else 'Keep separate'}")
        
        return is_similar 


================================================
File: app/services/enhanced_ad_extraction.py
================================================
import json
import hashlib
from datetime import datetime, date, timezone
from typing import Dict, List, Optional, Any, Tuple, cast
import logging
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import text
from sqlalchemy.exc import IntegrityError

from app.models import Ad, Competitor, AdSet
from app.database import get_db
from app.services.creative_comparison_service import CreativeComparisonService

logger = logging.getLogger(__name__)


class EnhancedAdExtractionService:
    """Service for enhanced ad data extraction matching frontend_payload_final.json format"""
    
    EXTRACTION_VERSION = "1.0.0"
    
    def __init__(self, db: Session):
        self.db = db
        self.logger = logging.getLogger(__name__)
        self.creative_comparison_service = CreativeComparisonService(db)
    
    def convert_timestamp_to_date(self, ts: Any) -> Optional[str]:
        """Converts a UNIX timestamp to a 'YYYY-MM-DD' formatted string."""
        if not ts: 
            return None
        try:
            return datetime.fromtimestamp(ts).strftime('%Y-%m-%d')
        except (ValueError, TypeError):
            return None
    
    def calculate_duration_days(self, start_date_str: Optional[str], end_date_str: Optional[str], is_active: bool = False) -> Optional[int]:
        """Calculate duration in days between start_date and end_date (or current date if active)"""
        if not start_date_str:
            return None
        
        try:
            start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
            
            if is_active or not end_date_str:
                # If ad is active or no end date, calculate up to today
                end_date = date.today()
            else:
                end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
            
            duration = (end_date - start_date).days
            return max(duration, 1)  # Ensure minimum of 1 day
            
        except (ValueError, TypeError) as e:
            logger.warning(f"Error calculating duration: start={start_date_str}, end={end_date_str}, error={e}")
            return None
    
    def _generate_content_signature(self, ad_data: Dict) -> str:
        """
        Generate a content signature for an AdSet based on the perceptual hash of its representative ad's media.
        This creates a stable, visual identifier that persists across ingestion processes.
        
        Args:
            ad_data: Ad data object containing media information
            
        Returns:
            Content signature string (perceptual hash or fallback)
        """
        ad_id = ad_data.get("ad_archive_id", ad_data.get("id", "unknown"))
        self.logger.info(f"Generating perceptual hash content signature for ad {ad_id}")
        
        # Try to generate perceptual hash based on media
        perceptual_hash = self._calculate_perceptual_hash_for_ad(ad_data)
        if perceptual_hash:
            self.logger.info(f"Generated perceptual hash signature for ad {ad_id}: {perceptual_hash}")
            return perceptual_hash
            
        # Fallback: use ad_archive_id as signature if no media hash available
        fallback = str(ad_id)
        self.logger.warning(f"No perceptual hash available for ad {ad_id}, using ad ID as fallback: {fallback}")
        return fallback
    
    def _calculate_perceptual_hash_for_ad(self, ad_data: Dict) -> Optional[str]:
        """
        Calculate a perceptual hash for an ad's primary media.
        This creates a stable visual identifier for the ad set based on the representative ad.
        
        Args:
            ad_data: Ad data object containing media information
            
        Returns:
            Perceptual hash string or None if no media found
        """
        try:
            ad_id = ad_data.get("ad_archive_id", "unknown")
            self.logger.info(f"Calculating perceptual hash for ad {ad_id}")
            
            # Try to find primary media URL
            media_url = ad_data.get("media_url")
            if not media_url:
                # Fallback to main image URLs
                if main_images := ad_data.get("main_image_urls", []):
                    media_url = main_images[0]
                elif main_videos := ad_data.get("main_video_urls", []):
                    media_url = main_videos[0]
                else:
                    # Check creatives for media
                    if creatives := ad_data.get("creatives", []):
                        for creative in creatives:
                            if media_list := creative.get("media", []):
                                for media_item in media_list:
                                    if media_item.get("url"):
                                        media_url = media_item["url"]
                                        break
                                if media_url:
                                    break
            
            if not media_url:
                self.logger.warning(f"No media URL found for ad {ad_id}, cannot calculate perceptual hash")
                return None
            
            # Determine media type and calculate appropriate hash
            media_type = self.creative_comparison_service.get_media_type(media_url)
            
            if media_type == "image":
                # Use image hashing
                image_hash = self.creative_comparison_service._download_and_hash_image(media_url)
                if image_hash:
                    hash_str = str(image_hash)
                    self.logger.info(f"Generated image perceptual hash for ad {ad_id}: {hash_str}")
                    return hash_str
                    
            elif media_type == "video":
                # Use video hashing - take the first frame hash as representative
                video_hashes = self.creative_comparison_service._sample_video_hashes(media_url, samples=1)
                if video_hashes and len(video_hashes) > 0:
                    hash_str = str(video_hashes[0])
                    self.logger.info(f"Generated video perceptual hash for ad {ad_id}: {hash_str}")
                    return hash_str
            
            self.logger.warning(f"Could not generate perceptual hash for ad {ad_id}, media type: {media_type}")
            return None
            
        except Exception as e:
            self.logger.error(f"Error calculating perceptual hash for ad: {e}")
            return None
    
    def _update_ad_set_metadata(self, ad_set_id: int) -> None:
        """
        Update metadata for an AdSet including:
        - variant_count: Count of ads in the set
        - best_ad_id: ID of the ad with highest overall_score
        - content_signature: Perceptual hash of the best ad's media
        
        Args:
            ad_set_id: ID of the AdSet to update
        """
        try:
            # Get the AdSet
            ad_set = self.db.query(AdSet).filter(AdSet.id == ad_set_id).first()
            if not ad_set:
                self.logger.warning(f"AdSet not found for updating metadata: {ad_set_id}")
                return
            
            # Count variants (ads in this set)
            variant_count = self.db.query(Ad).filter(Ad.ad_set_id == ad_set_id).count()
            ad_set.variant_count = variant_count
            
            # Find best ad (currently using first ad in set as we don't have overall_score yet)
            # TODO: Update this logic when overall_score is implemented
            best_ad = self.db.query(Ad).filter(Ad.ad_set_id == ad_set_id).first()
            previous_best_ad_id = ad_set.best_ad_id
            
            if best_ad:
                ad_set.best_ad_id = best_ad.id
            
                # Update content_signature if best_ad changed or if it's not set
                if (previous_best_ad_id != best_ad.id or not ad_set.content_signature):
                    self.logger.info(f"Best ad changed or content_signature missing for AdSet {ad_set_id}, updating content_signature")
                    
                    # Convert the best ad back to dict format for hash calculation
                    try:
                        best_ad_data = best_ad.to_enhanced_format()
                        if best_ad_data:
                            new_signature = self._generate_content_signature(best_ad_data)
                            if new_signature:
                                ad_set.content_signature = new_signature
                                self.logger.info(f"Updated content_signature for AdSet {ad_set_id}: {new_signature}")
                            else:
                                self.logger.warning(f"Could not generate content_signature for AdSet {ad_set_id}")
                        else:
                            self.logger.warning(f"Could not convert best ad to dict format for AdSet {ad_set_id}")
                    except Exception as e:
                        self.logger.error(f"Error updating content_signature for AdSet {ad_set_id}: {e}")
            
            self.logger.info(f"Updated AdSet metadata: id={ad_set_id}, variants={variant_count}, best_ad_id={ad_set.best_ad_id}")
            
        except Exception as e:
            self.logger.error(f"Error updating AdSet metadata: {e}")
    
    def parse_dynamic_lead_form(self, extra_texts: List[Dict]) -> Dict:
        """
        Dynamically parses lead form questions and options from extra_texts.
        This is the key function from test_ad_extraction.py
        """
        if not extra_texts:
            return {}
        
        form_details = {"questions": {}, "standalone_fields": []}
        standalone_field_keywords = {"full name", "email", "phone number", "country"}
        current_question = None
        
        for item in extra_texts:
            text = (item.get("text") or "").strip()
            if not text:
                continue
            
            text_lower = text.lower()
            
            # Check if this is a standalone field
            if any(keyword in text_lower for keyword in standalone_field_keywords) and len(text.split()) < 4:
                form_details["standalone_fields"].append(text)
                current_question = None
                continue
            
            # Check if this is a question (ends with ':')
            if text.endswith(':'):
                current_question = text
                form_details["questions"][current_question] = []
            elif current_question and len(text.split()) < 6 and "\n" not in text:
                # This is likely an option for the current question
                form_details["questions"][current_question].append(text)
        
        # Clean up questions with no options
        form_details["questions"] = {q: opts for q, opts in form_details["questions"].items() if opts}
        
        # Remove duplicates from standalone fields and sort
        form_details["standalone_fields"] = sorted(list(set(form_details["standalone_fields"])))
        
        return form_details
    
    def build_detailed_creative_object(self, creative_data: Dict) -> Optional[Dict]:
        """
        Builds a highly detailed object for a single creative (card).
        This matches the logic from test_ad_extraction.py
        """
        if not isinstance(creative_data, dict):
            return None
        
        # Safely extract and strip text fields
        headline = (creative_data.get("title") or "").strip()
        
        # Handle body data more carefully with additional type checking
        body_data = creative_data.get("body")
        body = ""
        if body_data is None:
            body = ""
        elif isinstance(body_data, dict):
            body = (body_data.get("text") or "").strip()
        elif isinstance(body_data, str):
            body = body_data.strip()
        else:
            self.logger.warning(f"Unexpected body_data type: {type(body_data)}")
            body = str(body_data) if body_data is not None else ""
            
        caption = (creative_data.get("caption") or "").strip()

        # Transform media URLs into a list of media objects
        media_list = []
        if url := creative_data.get("video_hd_url"):
            media_list.append({"type": "Video", "url": url})
        if url := creative_data.get("video_sd_url"):
            media_list.append({"type": "Video", "url": url})
        if url := creative_data.get("original_image_url"):
            media_list.append({"type": "Image", "url": url})
        if url := creative_data.get("resized_image_url"):
            media_list.append({"type": "Image", "url": url})
        if url := creative_data.get("video_preview_image_url"):
             media_list.append({"type": "Image", "url": url})

        creative_object = {
            "headline": headline or None,
            "body": body or None,
            "cta": {
                "text": creative_data.get("cta_text"), 
                "type": creative_data.get("cta_type")
            },
            "media": media_list,
            "link": {
                "url": creative_data.get("link_url"), 
                "caption": caption or None
            }
        }
        
        # Clean up empty values in dictionaries
        for key in ["cta", "link"]:
            if isinstance(creative_object[key], dict):
                creative_object[key] = {k: v for k, v in creative_object[key].items() if v}
        
        return creative_object
    
    def extract_targeting_data(self, ad_data: Dict) -> Dict:
        """
        Extracts detailed targeting and reach data, filtering for countries with non-zero reach.
        This is the enhanced logic from test_ad_extraction.py
        """
        transparency_data = (ad_data.get("enrichment_response", {})
                            .get("data", {})
                            .get("ad_library_main", {})
                            .get("ad_details", {})
                            .get("transparency_by_location", {}))
        
        targeting_info = {
            "locations": [], 
            "age_range": None, 
            "gender": None, 
            "reach_breakdown": [], 
            "total_reach": None
        }
        
        # Check both UK and EU transparency data
        for region in ["uk_transparency", "eu_transparency"]:
            if region_data := transparency_data.get(region):
                targeting_info["locations"] = region_data.get("location_audience", [])
                targeting_info["age_range"] = region_data.get("age_audience")
                targeting_info["gender"] = region_data.get("gender_audience")
                targeting_info["total_reach"] = (region_data.get("total_reach") or 
                                               region_data.get("eu_total_reach"))
                
                # Smart Filtering Logic - only include countries with non-zero reach
                filtered_reach_breakdown = []
                for country_breakdown in region_data.get("age_country_gender_reach_breakdown", []):
                    total_country_reach = 0
                    for age_gender in country_breakdown.get("age_gender_breakdowns", []):
                        total_country_reach += age_gender.get("male") or 0
                        total_country_reach += age_gender.get("female") or 0
                        total_country_reach += age_gender.get("unknown") or 0
                    
                    if total_country_reach > 0:
                        filtered_reach_breakdown.append(country_breakdown)
                
                targeting_info["reach_breakdown"] = filtered_reach_breakdown
                break
        
        # Return only non-empty values
        return {k: v for k, v in targeting_info.items() if v}
    
    def build_clean_ad_object(self, ad_data: Dict) -> Optional[Dict]:
        """
        Transforms a single raw ad variation into a clean, structured object.
        This is the main transformation logic from test_ad_extraction.py
        """
        snapshot = ad_data.get("snapshot", {})
        if not snapshot:
            return None
        
        # Extract page information directly from ad_data and snapshot
        page_id = ad_data.get("page_id")
        page_name = ad_data.get("page_name") or snapshot.get("page_name")
        page_url = snapshot.get("page_profile_uri")
        
        ad_object = {
            "ad_archive_id": ad_data.get("ad_archive_id"),
            "meta": {
                "is_active": ad_data.get("is_active", False),
                "cta_type": snapshot.get("cta_type"),
                "display_format": snapshot.get("display_format"),
                "page_id": page_id,
                "page_name": page_name,
                "page_url": page_url
            },
            "targeting": self.extract_targeting_data(ad_data),
            "lead_form": self.parse_dynamic_lead_form(snapshot.get("extra_texts", [])),
            "creatives": []
        }
        
        # Process cards/creatives
        creatives_source = []
        if snapshot.get("cards"):
            creatives_source.extend(snapshot["cards"])
        
        if not creatives_source:
            if snapshot.get("videos"):
                creatives_source.extend(snapshot["videos"])
            if snapshot.get("images"):
                 creatives_source.extend(snapshot["images"])

        if not creatives_source and (snapshot.get('title') or snapshot.get('body')):
             creatives_source = [snapshot]

        for i, creative_data in enumerate(creatives_source):
            detailed_creative = self.build_detailed_creative_object(creative_data)
            if detailed_creative:
                detailed_creative['id'] = f"{ad_object['ad_archive_id']}-{i}"
                ad_object['creatives'].append(detailed_creative)
        
        return ad_object
    
    def _group_ads_into_sets(self, ads_data: List[Dict]) -> Dict[str, List[Dict]]:
        """
        Group similar ads into sets based on creative content comparison
        
        Args:
            ads_data: List of ads to group
            
        Returns:
            Dictionary mapping content_signature to list of ad data
        """
        self.logger.info(f"Starting ad grouping process for {len(ads_data)} ads")
        
        ad_groups = {}  # Signature -> list of ads with that signature
        
        # First pass: group identical ads (exact same content signature)
        for ad_data in ads_data:
            # Extract ad identifier
            ad_id = ad_data.get("ad_archive_id", ad_data.get("id", "unknown"))
            
            # Generate the content signature based on ad copy or media
            content_signature = self._generate_content_signature(ad_data)
            self.logger.info(f"Generated content signature for ad {ad_id}: {content_signature[:8]}...")
            
            if content_signature not in ad_groups:
                self.logger.info(f"Creating new ad group with signature {content_signature[:8]}...")
                ad_groups[content_signature] = []
                
            ad_groups[content_signature].append(ad_data)
            self.logger.info(f"Added ad {ad_id} to group {content_signature[:8]}...")
        
        self.logger.info(f"First-pass grouping complete: {len(ad_groups)} groups created")
        for sig, ads in ad_groups.items():
            self.logger.info(f"Group {sig[:8]}: {len(ads)} ads, first ad: {ads[0].get('ad_archive_id', ads[0].get('id', 'unknown'))}")
        
        # Second pass: try to merge groups with similar content
        # This uses our creative comparison service for more sophisticated matching
        signatures = list(ad_groups.keys())
        merged_signatures = set()
        
        self.logger.info(f"Starting second-pass group merging for {len(signatures)} groups")
        
        # For each group
        for i, sig1 in enumerate(signatures):
            if sig1 in merged_signatures:
                self.logger.debug(f"Skipping already merged group {sig1[:8]}...")
                continue
            
            group1_id = f"{sig1[:8]}({len(ad_groups[sig1])} ads)"
            self.logger.info(f"Processing group {i+1}/{len(signatures)}: {group1_id}")
                
            # Compare with every other group
            for j in range(i + 1, len(signatures)):
                sig2 = signatures[j]
                if sig2 in merged_signatures:
                    self.logger.debug(f"Skipping already merged group {sig2[:8]}...")
                    continue
                
                group2_id = f"{sig2[:8]}({len(ad_groups[sig2])} ads)"
                self.logger.info(f"Comparing group {group1_id} with group {group2_id}")
                
                # Check if representative ads from each group are similar
                if self._are_ad_groups_similar(ad_groups[sig1], ad_groups[sig2]):
                    # Merge groups
                    self.logger.info(f"Merging groups: {group1_id} + {group2_id}")
                    ad_groups[sig1].extend(ad_groups[sig2])
                    merged_signatures.add(sig2)
                    self.logger.info(f"Group {sig1[:8]} now has {len(ad_groups[sig1])} ads after merging")
                else:
                    self.logger.info(f"Groups {group1_id} and {group2_id} are not similar enough to merge")
        
        # Remove merged groups
        for sig in merged_signatures:
            self.logger.debug(f"Removing merged group {sig[:8]}...")
            ad_groups.pop(sig, None)
            
        # Log final grouping results
        self.logger.info(f"Ad grouping complete: {len(ads_data)} ads grouped into {len(ad_groups)} sets")
        for sig, ads in ad_groups.items():
            self.logger.info(f"Final group {sig[:8]}: {len(ads)} ads")
            if len(ads) > 1:
                ad_ids = [ad.get('ad_archive_id', ad.get('id', 'unknown')) for ad in ads[:5]]
                if len(ads) > 5:
                    ad_ids.append("...")
                self.logger.info(f"   Ads in group: {', '.join(ad_ids)}")
            
        return ad_groups
    
    def _are_ad_groups_similar(self, group1: List[Dict], group2: List[Dict]) -> bool:
        """
        Check if two ad groups are similar by comparing their representative ads
        
        Args:
            group1: First group of ads
            group2: Second group of ads
            
        Returns:
            True if the groups are similar enough to be merged
        """
        # Use the first ad from each group as representative
        ad1 = group1[0]
        ad2 = group2[0]
        
        # Log the representative ads being compared
        ad1_id = ad1.get("ad_archive_id", ad1.get("id", "unknown"))
        ad2_id = ad2.get("ad_archive_id", ad2.get("id", "unknown"))
        self.logger.info(f"Comparing representative ads: {ad1_id} vs {ad2_id}")
        
        # Extract basic ad details for logging
        ad1_type = ad1.get("media_type", "unknown")
        ad2_type = ad2.get("media_type", "unknown")
        self.logger.info(f"Ad types: {ad1_type} vs {ad2_type}")
        
        # Check if both ads have text
        ad1_text = ""
        ad2_text = ""
        if ad1.get("creatives") and ad1["creatives"] and "body" in ad1["creatives"][0]:
            ad1_text = ad1["creatives"][0]["body"]
        if ad2.get("creatives") and ad2["creatives"] and "body" in ad2["creatives"][0]:
            ad2_text = ad2["creatives"][0]["body"]
            
        if ad1_text and ad2_text:
            text_match = "identical" if ad1_text == ad2_text else "different"
            self.logger.info(f"Ad text is {text_match}")
            self.logger.debug(f"Ad1 text: {ad1_text[:100]}...")
            self.logger.debug(f"Ad2 text: {ad2_text[:100]}...")
            
        # Check media URLs
        ad1_urls = self._extract_media_urls(ad1)
        ad2_urls = self._extract_media_urls(ad2)
        
        if ad1_urls and ad2_urls:
            self.logger.info(f"Ad1 media URLs: {', '.join([url[:40]+'...' for url in ad1_urls[:2]])}")
            self.logger.info(f"Ad2 media URLs: {', '.join([url[:40]+'...' for url in ad2_urls[:2]])}")
            
            # Check for exact URL matches
            common_urls = set(ad1_urls).intersection(set(ad2_urls))
            if common_urls:
                self.logger.info(f"Found {len(common_urls)} identical media URLs between the ads")
            
        # Use creative comparison service to check similarity
        result = self.creative_comparison_service.should_group_ads(ad1, ad2)
        self.logger.info(f"Creative comparison service decision: ads should{'' if result else ' not'} be grouped")
        return result
        
    def _extract_media_urls(self, ad: Dict) -> List[str]:
        """Extract all media URLs from an ad for logging purposes"""
        urls = []
        
        # Check direct media URL
        if ad.get("media_url"):
            urls.append(ad["media_url"])
            
        # Check image URLs
        if ad.get("main_image_urls"):
            urls.extend(ad["main_image_urls"])
            
        # Check video URLs
        if ad.get("main_video_urls"):
            urls.extend(ad["main_video_urls"])
            
        # Check creative media
        if ad.get("creatives"):
            for creative in ad["creatives"]:
                if creative.get("media"):
                    for media in creative["media"]:
                        if media.get("url"):
                            urls.append(media["url"])
                            
        return urls
    
    def _create_new_ad_set(self, content_signature: str) -> Optional[AdSet]:
        """
        Creates a new AdSet with the given content_signature.
        
        Args:
            content_signature: The content signature for the new AdSet.
            
        Returns:
            The newly created AdSet object or None if creation failed.
        """
        try:
            new_ad_set = AdSet(
                content_signature=content_signature,
                variant_count=0,  # Initial count is 0, will be incremented when ad is added
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            self.db.add(new_ad_set)
            self.db.flush()  # Get the ID without committing
            self.logger.info(f"Created new AdSet {new_ad_set.id} for content signature: {content_signature}")
            return new_ad_set
        except Exception as e:
            self.logger.error(f"Error creating new AdSet for content signature {content_signature}: {e}")
            return None

    def find_or_create_ad_set_for_ad(self, ad_data: Dict) -> Optional[AdSet]:
        """
        Finds or creates an AdSet for a new ad using a highly performant,
        index-driven candidate selection process based on perceptual hashes.
        """
        try:
            ad_id = ad_data.get("ad_archive_id", "unknown")
            self.logger.info(f"Finding AdSet for ad {ad_id} using visual index pre-filtering.")

            # 1. Calculate perceptual hash for the new ad
            new_hash = self._calculate_perceptual_hash_for_ad(ad_data)
            if not new_hash:
                self.logger.warning(f"Could not generate perceptual hash for ad {ad_id}. Creating new AdSet via fallback.")
                return self._create_new_ad_set(str(ad_id))

            # 2. Quick exact match: check if hash already exists
            exact_match = self.db.query(AdSet).filter(AdSet.content_signature == new_hash).first()
            if exact_match:
                self.logger.info(f"Exact content_signature match found â€“ using existing AdSet {exact_match.id}.")
                return exact_match

            # 3. Use pg_trgm similarity to fetch top candidate AdSets
            candidate_query = text(
                """
                SELECT id FROM ad_sets
                WHERE similarity(content_signature, :hash) > 0.8
                ORDER BY similarity(content_signature, :hash) DESC
                LIMIT 20
                """
            )
            try:
                result = self.db.execute(candidate_query, {"hash": new_hash})
                candidate_ids = [row[0] for row in result]
            except Exception as e:
                self.logger.error(f"pg_trgm similarity query failed: {e}. Falling back to new AdSet.")
                return self._create_new_ad_set(new_hash)

            self.logger.info(f"{len(candidate_ids)} candidate AdSets retrieved for visual hash {new_hash}.")

            # 4. Compare with candidate AdSets' representative ads
            if candidate_ids:
                candidates = (
                    self.db.query(AdSet)
                    .options(joinedload(AdSet.best_ad))
                    .filter(AdSet.id.in_(candidate_ids))
                    .all()
                )
                for ad_set in candidates:
                    if not ad_set.best_ad:
                        continue
                    rep_ad_data = ad_set.best_ad.to_enhanced_format()
                    if self.creative_comparison_service.should_group_ads(ad_data, rep_ad_data):
                        self.logger.info(f"Grouping ad {ad_id} with existing AdSet {ad_set.id} (hash match).")
                        return ad_set

            # 5. No suitable candidate â€“ create a new AdSet
            self.logger.info(f"No AdSet matched. Creating new AdSet for hash {new_hash}.")
            return self._create_new_ad_set(new_hash)
        except Exception as e:
            self.logger.error(f"Error in find_or_create_ad_set_for_ad: {e}")
            return None
    
    def _create_or_update_enhanced_ad(
        self,
        ad_data: Dict,
        competitor_id: int,
        campaign_name: str = None,
        platforms: List[str] = None,
        meta_data: Dict = None,
    ) -> Tuple[Optional[Ad], bool]:
        """
        Create or update an ad with enhanced extraction data.
        
        Args:
            ad_data: Ad data to create or update
            competitor_id: ID of the competitor
            campaign_name: Optional campaign name
            platforms: Optional list of platforms where the ad was found
            meta_data: Optional metadata for the ad
            
        Returns:
            Tuple of (Ad object, is_new flag)
        """
        try:
            ad_id = ad_data.get("ad_archive_id", None)
            if not ad_id:
                self.logger.error("Ad data missing ad_archive_id, cannot process")
                return None, False
                
            # Check if the ad already exists
            existing_ad = self.db.query(Ad).filter(Ad.ad_archive_id == ad_id).first()
            is_new = existing_ad is None
            
            # Enhanced data extraction
            enhanced_data = self._extract_enhanced_ad_data(ad_data)

            # Prepare meta dictionary, ensuring it's a plain dict we can mutate safely
            base_meta: Dict[str, Any] = cast(Dict[str, Any], ad_data.get("meta") or {})

            # Inject campaign name and platforms if provided
            if campaign_name:
                base_meta["campaign_name"] = campaign_name
            if platforms:
                base_meta["platforms"] = platforms
            
            if is_new:
                # Create a new ad
                
                # First, find or create the appropriate AdSet using direct comparison
                ad_set = self.find_or_create_ad_set_for_ad(ad_data)
                if not ad_set:
                    self.logger.error(f"Failed to find or create AdSet for ad {ad_id}")
                    return None, False
                
                # Create the new ad
                new_ad = Ad(
                    ad_archive_id=ad_id,
                    competitor_id=competitor_id,
                    ad_set_id=ad_set.id,
                    date_found=datetime.utcnow(),
                    meta=base_meta,
                    targeting=ad_data.get("targeting", {}),
                    lead_form=ad_data.get("lead_form", {}),
                    creatives=ad_data.get("creatives", []),
                    duration_days=enhanced_data.get("duration_days", 0)
                )
                
                # --- Maintain AdSet date range ---
                ad_found_date = new_ad.date_found.astimezone(timezone.utc)
                if ad_set.first_seen_date is None or ad_found_date < ad_set.first_seen_date:
                    ad_set.first_seen_date = ad_found_date
                if ad_set.last_seen_date is None or ad_found_date > ad_set.last_seen_date:
                    ad_set.last_seen_date = ad_found_date
                # ---------------------------------
                
                self.db.add(new_ad)
                self.db.flush()
                
                # Update the ad set's metadata
                if not ad_set.best_ad_id:
                    ad_set.best_ad_id = new_ad.id
                    
                ad_set.variant_count = ad_set.variant_count + 1
                ad_set.updated_at = datetime.utcnow()
                
                self.logger.info(f"Updated AdSet metadata: id={ad_set.id}, variants={ad_set.variant_count}")
                
                return new_ad, True
                
            else:
                # Update existing ad
                existing_ad.updated_at = datetime.utcnow()
                
                # Update duration_days if available in enhanced data
                if "duration_days" in enhanced_data:
                    existing_ad.duration_days = enhanced_data.get("duration_days")
                
                # Update meta data if provided
                if meta_data and existing_ad.meta:
                    existing_ad.meta.update(meta_data)
                
                # Safely update meta information (campaign name / platforms)
                try:
                    current_meta: Dict[str, Any] = cast(Dict[str, Any], existing_ad.meta or {})
                    if campaign_name:
                        current_meta["campaign_name"] = campaign_name
                    if platforms:
                        existing_platforms = set(current_meta.get("platforms", []))
                        existing_platforms.update(platforms)
                        current_meta["platforms"] = list(existing_platforms)
                    existing_ad.meta = current_meta
                except Exception as meta_err:
                    self.logger.error(f"Failed to update ad meta for {ad_id}: {meta_err}")
                
                # Update creatives if provided
                if ad_data.get("creatives"):
                    existing_ad.creatives = ad_data.get("creatives")
                
                self.db.flush()
                return existing_ad, False
                
        except Exception as e:
            self.logger.error(f"Error creating/updating enhanced ad: {e}")
            self.db.rollback()
            return None, False
    
    def process_raw_responses(self, raw_responses: List[Dict]) -> Tuple[Dict, Dict]:
        """
        Process raw JSON responses from Facebook Ads Library and save to database
        
        Args:
            raw_responses: List of raw JSON responses from Facebook Ads Library
            
        Returns:
            Tuple of (enhanced_data, stats)
        """
        self.logger.info(f"Processing {len(raw_responses)} raw responses with enhanced extraction")
        
        # Transform raw data to enhanced format
        enhanced_data = self.transform_raw_data_to_enhanced_format(raw_responses)
        
        # Save enhanced data to database
        stats = self.save_enhanced_ads_to_database(enhanced_data)
        
        return enhanced_data, stats 

    def transform_raw_data_to_enhanced_format(self, raw_responses: List[Dict]) -> Dict[str, List[Dict]]:
        """
        Transform raw JSON responses into enhanced format grouped by competitor
        
        Args:
            raw_responses: List of raw JSON responses from Facebook Ads Library
            
        Returns:
            Dictionary mapping competitor names to lists of enhanced ad data
        """
        enhanced_data = {}
        
        self.logger.info(f"Starting transform with {len(raw_responses)} responses, types: {[type(r).__name__ for r in raw_responses[:3]]}")
        
        for i, response in enumerate(raw_responses):
            try:
                self.logger.info(f"Processing response {i}: type={type(response).__name__}, preview={str(response)[:100]}")
                
                # Handle case where response might be a JSON string
                if isinstance(response, str):
                    self.logger.info(f"Response {i} is string, attempting JSON parse")
                    try:
                        response = json.loads(response)
                        self.logger.info(f"Successfully parsed JSON, new type: {type(response).__name__}")
                    except json.JSONDecodeError as e:
                        self.logger.error(f"Failed to parse JSON response {i}: {e}")
                        continue
                
                # Ensure response is a dictionary
                if not isinstance(response, dict):
                    self.logger.warning(f"Response {i} is not a dictionary after parsing: {type(response)}")
                    continue
                
                # Extract ads from response
                self.logger.info(f"Extracting data from response {i}")
                
                # Handle GraphQL response structure
                ads_data = []
                if "data" in response:
                    # Check for GraphQL structure: data.ad_library_main.search_results_connection.edges
                    if "ad_library_main" in response["data"]:
                        search_results = response["data"]["ad_library_main"].get("search_results_connection", {})
                        edges = search_results.get("edges", [])
                        self.logger.info(f"Found {len(edges)} edges in GraphQL response")
                        
                        for edge in edges:
                            if "node" in edge:
                                node_data = edge["node"]
                                # Check for collated_results or direct ad data
                                if "collated_results" in node_data:
                                    collated = node_data["collated_results"]
                                    if isinstance(collated, list):
                                        ads_data.extend(collated)
                                    else:
                                        ads_data.append(collated)
                                else:
                                    ads_data.append(node_data)
                    else:
                        # Fallback to direct data array
                        ads_data = response["data"] if isinstance(response["data"], list) else [response["data"]]
                else:
                    # Fallback for non-GraphQL responses
                    ads_data = response.get("data", [])
                
                if not ads_data:
                    self.logger.warning(f"No ads found in response {i}")
                    continue
                
                self.logger.info(f"Found {len(ads_data)} ads in response {i}")
                
                # Process each ad
                for j, ad_data in enumerate(ads_data):
                    self.logger.info(f"Processing ad {j} from response {i}")
                    self.logger.info(f"Ad {j} type: {type(ad_data).__name__}, content preview: {str(ad_data)[:200]}")
                    
                    # Ensure ad_data is a dictionary
                    if not isinstance(ad_data, dict):
                        self.logger.warning(f"Ad {j} is not a dictionary after parsing: {type(ad_data)}")
                        continue
                    
                    # Build clean ad object
                    clean_ad = self.build_clean_ad_object(ad_data)
                    if not clean_ad:
                        self.logger.warning(f"Failed to build clean ad object for ad {j}")
                        continue
                    
                    # Extract competitor info
                    page_name = clean_ad.get("meta", {}).get("page_name", "Unknown")
                    self.logger.info(f"Ad {j} belongs to competitor: {page_name}")
                    
                    # Group by competitor
                    if page_name not in enhanced_data:
                        enhanced_data[page_name] = []
                    
                    enhanced_data[page_name].append(clean_ad)
                    
            except Exception as e:
                self.logger.error(f"Error processing raw response {i}: {e}", exc_info=True)
                continue
                
        self.logger.info(f"Transformed {len(raw_responses)} responses into {len(enhanced_data)} competitor groups")
        return enhanced_data

    def save_enhanced_ads_to_database(self, enhanced_data: Dict[str, List[Dict]]) -> Dict[str, int]:
        """
        Save enhanced ad data to database
        
        Args:
            enhanced_data: Dictionary mapping competitor names to ad data lists
            
        Returns:
            Dictionary with processing statistics
        """
        stats = {
            "total_ads_processed": 0,
            "new_ads_created": 0,
            "existing_ads_updated": 0,
            "errors": 0,
            "competitors_processed": 0
        }
        
        try:
            for competitor_name, ads_list in enhanced_data.items():
                try:
                    stats["competitors_processed"] += 1
                    
                    # Find or create competitor
                    competitor = self.db.query(Competitor).filter(
                        Competitor.name == competitor_name
                    ).first()
                    
                    if not competitor:
                        # Extract page_id from first ad if available
                        page_id = None
                        if ads_list and ads_list[0].get("meta", {}).get("page_id"):
                            page_id = ads_list[0]["meta"]["page_id"]
                        
                        competitor = Competitor(
                            name=competitor_name,
                            page_id=page_id,
                            created_at=datetime.utcnow()
                        )
                        self.db.add(competitor)
                        self.db.flush()
                        self.logger.info(f"Created new competitor: {competitor_name}")
                    
                    # Process each ad for this competitor
                    for ad_data in ads_list:
                        try:
                            stats["total_ads_processed"] += 1
                            
                            # Create or update the ad
                            ad_obj, is_new = self._create_or_update_enhanced_ad(
                                ad_data, 
                                competitor.id
                            )
                            
                            if ad_obj:
                                if is_new:
                                    stats["new_ads_created"] += 1
                                else:
                                    stats["existing_ads_updated"] += 1
                            else:
                                stats["errors"] += 1
                                
                        except Exception as e:
                            self.logger.error(f"Error processing ad {ad_data.get('ad_archive_id', 'unknown')}: {e}")
                            stats["errors"] += 1
                            continue
                            
                except Exception as e:
                    self.logger.error(f"Error processing competitor {competitor_name}: {e}")
                    stats["errors"] += 1
                    continue
            
            # Commit all changes
            self.db.commit()
            self.logger.info(f"Database save completed: {stats}")
            
        except Exception as e:
            self.logger.error(f"Error saving to database: {e}")
            self.db.rollback()
            stats["errors"] += 1
            
        return stats

    def _extract_enhanced_ad_data(self, ad_data: Dict) -> Dict:
        """
        Extract enhanced ad data from ad_data into a format suitable for storage.
        
        Args:
            ad_data: Raw ad data to extract enhanced data from
            
        Returns:
            Dictionary with enhanced ad data
        """
        try:
            enhanced_data = {}
            
            # Extract basic metadata
            meta = ad_data.get("meta", {})
            enhanced_data["advertiser_name"] = meta.get("page_name", "")
            enhanced_data["advertiser_id"] = meta.get("page_id", "")
            enhanced_data["start_date"] = meta.get("start_date", "")
            enhanced_data["end_date"] = meta.get("end_date", None)
            enhanced_data["is_active"] = meta.get("is_active", False)
            
            # Extract primary texts
            primary_text = ""
            title = ""
            link_description = ""
            cta = ""
            
            # Extract creatives data
            creatives = ad_data.get("creatives", [])
            if creatives and len(creatives) > 0:
                # Use first creative as primary
                primary_creative = creatives[0]
                primary_text = primary_creative.get("body", "")
                title = primary_creative.get("title", "")
                link_description = primary_creative.get("description", "")
                cta = primary_creative.get("call_to_action", "")
            
            enhanced_data["primary_text"] = primary_text
            enhanced_data["title"] = title
            enhanced_data["link_description"] = link_description
            enhanced_data["cta"] = cta
            
            # Extract media URLs
            enhanced_data["image_urls"] = ad_data.get("main_image_urls", [])
            enhanced_data["video_urls"] = ad_data.get("main_video_urls", [])
            enhanced_data["primary_media_url"] = ad_data.get("media_url", "")
            
            # Extract other useful data
            enhanced_data["targeting"] = ad_data.get("targeting", {})
            enhanced_data["lead_form"] = ad_data.get("lead_form", {})
            enhanced_data["media_type"] = ad_data.get("media_type", "unknown")
            
            # Calculate duration in days if dates are available
            if enhanced_data["start_date"]:
                enhanced_data["duration_days"] = self.calculate_duration_days(
                    enhanced_data["start_date"], 
                    enhanced_data["end_date"],
                    enhanced_data["is_active"]
                )
            
            return enhanced_data
            
        except Exception as e:
            self.logger.error(f"Error extracting enhanced ad data: {e}")
            return {}




================================================
File: app/services/facebook_ads_scraper.py
================================================
import requests
import json
import csv
import time
import uuid
from urllib.parse import urlencode
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import logging
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
import re
import urllib.parse

from app.models import Ad, Competitor
from app.database import get_db
from app.services.enhanced_ad_extraction import EnhancedAdExtractionService

# Configure logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
# Set logging level for specific modules
logging.getLogger('app.services.creative_comparison_service').setLevel(logging.INFO)
logging.getLogger('app.services.enhanced_ad_extraction').setLevel(logging.INFO)


class FacebookAdsScraperConfig:
    """
    Configuration class for Facebook Ads scraper.
    
    To keep this script working, you may need to update the session-related
    values below. You can get these by inspecting the network requests in your
    browser's developer tools when browsing the Facebook Ad Library.
    
    1. Open the Ad Library in a new browser tab.
    2. Open the Developer Tools (F12) and go to the "Network" tab.
    3. Perform a search for an advertiser.
    4. Find a 'graphql' request, right-click it, and copy the cookie string
       and other relevant values like 'lsd' and 'jazoest'.
    """
    
    def __init__(
        self,
        # --- Search parameters ---
        active_status: str = 'active',
        ad_type: str = 'ALL',
        countries: List[str] = None,
        search_type: str = 'page',
        media_type: str = 'all',
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        query_string: str = '',
        page_ids: List[str] = None,
        view_all_page_id: str = '1591077094491398',
        cursor: Optional[str] = None,
        first: int = 30,
        is_targeted_country: bool = False,
        
        # --- Session parameters ---
        session_id: str = '2937f803-041c-481d-b565-81ae712d5209',
        collation_token: str = str(uuid.uuid4()),  # Random UUID for collation token
        lsd_token: str = 'AVq28ysAAt0',
        jazoest: str = '2900',
        cookie: str = 'datr=n14YaOmLyHCUO9eDBSPbd0bo; sb=n14YaN0pXid7MQ54q1q9-XHz; ps_l=1; ps_n=1; dpr=2.5; fr=1vnX3O3Y99uUSRWaj.AWchuA7IeOh24kEy8WvwbrIwUUScHnrL8m0ga2ev4Py-aNisJdM.BoaRgt..AAA.0.0.BoaRgt.AWcCsNYL2f427xwHLtWS3CN2SlU; wd=891x831',

        # --- Dynamic payload components (may need updating) ---
        search_payload_dyn: str = '7xeUmwlECdwn8K2Wmh0no6u5U4e1Fx-ewSAwHwNw9G2S2q0_EtxG4o0B-qbwgE1EEb87C1xwEwgo9oO0n24oaEd86a3a1YwBgao6C0Mo6i588Etw8WfK1LwPxe2GewbCXwJwmEtwse5o4q0HU1IEGdw46wbLwrU6C2-0VE6O1Fw59G2O1TwmU3ywo8',
        search_payload_csr: str = 'htOh24lsOWjORb9uQAheC8KVpaGuHGF8GBx2UKp2qzVUiCBxm6GwTBwBwFBx216G15whrx6482TKEuzU8E6aUdU2qwgo8E7jwsE1BU2axy0RUkxC8w4dwTw10K0aswOU02yHyE07_h00iVE04IK06ofwbG00ymQ032q03TN1i0bQw35E0Gq09pw6Yg0OG',
        search_payload_hsdp: str = 'ggPhf5icj4pbiO0KgG1awwwCpoaoKGCQb81lw8mq4nQ1kwyw2xo4t08uE33hWvN5eE-293Q4827wJwc-q0mm1qwQw8e0abDg1dE3pw6kw86UB08217w1la085w0JTw0Rzw0tk8',
        search_payload_hblp: str = '03280oFw36o0OK0k6U1Ko0nqw2hEnw0Jvw2BE4e0se06ro0Bq02y20eCg0OW0WE15E1xUjw70wbm0XE5q7E0juwRw',

        # --- General settings ---
        max_pages: Optional[int] = 1,
        delay_between_requests: int = 1,
        save_json: bool = False
    ):
        # Search parameters
        self.active_status = active_status
        self.ad_type = ad_type
        self.countries = countries or ['AE']
        self.search_type = search_type
        self.media_type = media_type
        self.start_date = start_date
        self.end_date = end_date
        self.query_string = query_string
        self.page_ids = page_ids or []
        self.view_all_page_id = view_all_page_id
        self.cursor = cursor
        self.first = first
        self.is_targeted_country = is_targeted_country
        
        # Session parameters
        self.session_id = session_id
        self.collation_token = collation_token or str(uuid.uuid4())
        self.lsd_token = lsd_token
        self.jazoest = jazoest
        self.cookie = cookie
        
        # Dynamic payload components
        self.search_payload_dyn = search_payload_dyn
        self.search_payload_csr = search_payload_csr
        self.search_payload_hsdp = search_payload_hsdp
        self.search_payload_hblp = search_payload_hblp

        # General settings
        self.max_pages = max_pages
        self.delay_between_requests = delay_between_requests
        self.save_json = save_json
        
        # Hardcoded headers (only user-agent and other static values)
        self.headers = {
            'authority': 'www.facebook.com',
            'accept': '*/*',
            'accept-language': 'en-US,en;q=0.9',
            'content-type': 'application/x-www-form-urlencoded',
            'origin': 'https://www.facebook.com',
            'priority': 'u=1, i',
            'referer': 'https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=US&sort_data[direction]=desc&sort_data[mode]=relevancy_monthly_grouped&search_type=page&media_type=all',
            'sec-ch-ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Brave";v="138"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'sec-gpc': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'x-asbd-id': '359341',
            'x-fb-friendly-name': 'AdLibrarySearchPaginationQuery'
        }


class FacebookAdsScraperService:
    """Service for scraping Facebook Ads Library data"""
    
    def __init__(self, db: Session):
        self.db = db
        self.enhanced_extractor = EnhancedAdExtractionService(db)

    def build_variables(self, config: FacebookAdsScraperConfig) -> str:
        """Build the variables JSON object from config"""
        variables = {
            "activeStatus": config.active_status,
            "adType": config.ad_type,
            "bylines": [],
            "collationToken": config.collation_token,
            "contentLanguages": [],
            "countries": config.countries,
            "cursor": config.cursor,
            "excludedIDs": None,
            "first": config.first,
            "isTargetedCountry": config.is_targeted_country,
            "location": None,
            "mediaType": config.media_type,
            "multiCountryFilterMode": None,
            "pageIDs": config.page_ids,
            "potentialReachInput": None,
            "publisherPlatforms": [],
            "queryString": config.query_string,
            "regions": None,
            "searchType": config.search_type,
            "sessionID": config.session_id,
            "sortData": None,
            "source": None,
            "startDate": config.start_date,
            "v": "608791",
            "viewAllPageID": config.view_all_page_id
        }
        return json.dumps(variables, separators=(',', ':'))

    def build_dynamic_payload(self, config: FacebookAdsScraperConfig) -> str:
        """Build payload with dynamic variables while keeping original structure"""
        variables_json = self.build_variables(config)
        
        payload = (
            f"av=0&__aaid=0&__user=0&__a=1&__req=a&__hs=20275.HYP%3Acomet_plat_default_pkg.2.1...0&dpr=3&"
            f"__ccg=GOOD&__rev=1024475506&__s=oeivzd%3Aa1ohcs%3A7a3kxu&__hsi=7524059447383386224&"
            f"__dyn={config.search_payload_dyn}&"
            f"__csr={config.search_payload_csr}&"
            f"__hsdp={config.search_payload_hsdp}&"
            f"__hblp={config.search_payload_hblp}&"
            f"__comet_req=94&lsd={config.lsd_token}&jazoest={config.jazoest}&__spin_r=1024475506&__spin_b=trunk&__spin_t=1751831604&"
            f"__jssesw=1&fb_api_caller_class=RelayModern&fb_api_req_friendly_name=AdLibrarySearchPaginationQuery&"
            f"variables={urllib.parse.quote(variables_json)}&server_timestamps=true&doc_id=24394279933540792"
        )
        return payload

    def build_referer_url(self, config: FacebookAdsScraperConfig) -> str:
        """Build the referer URL based on config"""
        params = {
            'active_status': config.active_status,
            'ad_type': 'all',
            'country': ','.join(config.countries),
            'is_targeted_country': str(config.is_targeted_country).lower(),
            'media_type': config.media_type,
            'search_type': config.search_type,
            'view_all_page_id': config.view_all_page_id
        }
        
        if config.query_string:
            params['q'] = config.query_string
        
        query_string = urlencode(params)
        return f"https://www.facebook.com/ads/library/?{query_string}"

    def fetch_ads_page(self, config: FacebookAdsScraperConfig) -> Optional[Dict]:
        """
        Fetch a single page of ads data using the Facebook Ads Library API
        
        Args:
            config: Scraper configuration
            
        Returns:
            Dictionary containing the response data
        """
        url = "https://www.facebook.com/api/graphql/"
        
        payload = self.build_dynamic_payload(config)
        
        headers = config.headers.copy()
        headers['referer'] = self.build_referer_url(config)
        headers['Cookie'] = config.cookie
        headers['x-fb-lsd'] = config.lsd_token
        
        try:
            response = requests.post(url, headers=headers, data=payload)
            response.raise_for_status()
            
            data = response.json()
            return data
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error: {e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            logger.error(f"Response text that failed to parse: {response.text[:1000]}")
            return None

    def scrape_ads(self, config: FacebookAdsScraperConfig) -> Tuple[List[Dict], List[Dict], Dict, Dict]:
        """
        Scrape Facebook Ads and process them with enhanced extraction
        
        Args:
            config: Scraper configuration
        
        Returns:
            Tuple of (all_ads_data, all_json_responses, enhanced_data, stats)
        """
        logger.info(f"Starting data collection for {config.view_all_page_id} using enhanced extraction")
        
        try:
            all_json_responses = []
            page_count = 0
            
            stats = {
                "total_processed": 0,
                "created": 0,
                "updated": 0,
                "errors": 0,
                'competitors_created': 0,
                'competitors_updated': 0,
                'campaigns_processed': 0
            }
            
            while True:
                page_count += 1
                if config.max_pages and page_count > config.max_pages:
                    logger.info(f"Reached max pages limit of {config.max_pages}.")
                    break

                logger.info(f"Scraping page {page_count}")
                
                response_data = self.fetch_ads_page(config)

                if not response_data:
                    logger.warning(f"No response data on page {page_count}. Stopping scrape.")
                    break
                
                all_json_responses.append(response_data)
                
                try:
                    search_results = response_data['data']['ad_library_main']['search_results_connection']
                    edges = search_results.get('edges', [])
                    page_info = search_results.get('page_info', {})

                    if not edges:
                        logger.info("No ads found on this page. Stopping scrape.")
                        break
                    
                    logger.info(f"Page {page_count}: Found {len(edges)} ad groups. Processing with enhanced extraction.")

                    enhanced_data, _ = self.enhanced_extractor.process_raw_responses([response_data])
                    
                    extraction_stats = self.enhanced_extractor.save_enhanced_ads_to_database(enhanced_data)
                    
                    for key, value in extraction_stats.items():
                        if key in stats:
                            stats[key] += value
                    
                    has_next_page = page_info.get('has_next_page', False)
                    end_cursor = page_info.get('end_cursor')

                    if not has_next_page:
                        logger.info("No more pages available. Finished.")
                        break
                    
                    config.cursor = end_cursor
                    
                    if config.delay_between_requests > 0:
                        logger.info(f"Waiting {config.delay_between_requests} seconds...")
                        time.sleep(config.delay_between_requests)

                except (KeyError, TypeError) as e:
                    logger.error(f"Error parsing response data on page {page_count}: {e}")
                    logger.error(f"Response data that caused error: {response_data}")
                    break
            
            logger.info(f"Scraping complete. Final stats: {stats}")
            return [], all_json_responses, enhanced_data, stats
        
        except Exception as e:
            logger.error(f"Error in ads scraping: {str(e)}")
            raise 


================================================
File: app/services/ingestion_service.py
================================================
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from datetime import datetime
import logging
from typing import Optional, Tuple, List, Dict, Any

from app.models import Competitor, Ad
from app.models.dto import AdCreate, AdIngestionResponse

logger = logging.getLogger(__name__)


class DataIngestionService:
    """
    Centralized service for handling ad data ingestion.
    
    This service acts as the single entry point for all scraped ad data,
    ensuring consistent processing and triggering of downstream tasks.
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    async def ingest_ad(self, ad_data: AdCreate) -> AdIngestionResponse:
        """
        Main method for ingesting ad data.
        
        This method:
        1. Validates the input data
        2. Finds or creates the competitor
        3. Creates or updates the ad record
        4. Triggers the AI analysis task
        
        Args:
            ad_data: Validated ad data from external scraper
            
        Returns:
            AdIngestionResponse with operation results
        """
        try:
            logger.info(f"Starting ad ingestion for ad_archive_id: {ad_data.ad_archive_id}")
            
            # Step 1: Find or create competitor
            competitor = await self._find_or_create_competitor(ad_data.competitor)
            logger.info(f"Using competitor: {competitor.name} (ID: {competitor.id})")
            
            # Step 2: Create or update ad record
            ad, is_new = await self._create_or_update_ad(ad_data, competitor.id)
            logger.info(f"{'Created new' if is_new else 'Updated existing'} ad: {ad.id}")
            
            # Step 3: Trigger AI analysis task
            analysis_task_id = await self._trigger_analysis_task(ad.id)
            logger.info(f"Triggered AI analysis task: {analysis_task_id}")
            
            # Step 4: Commit transaction
            self.db.commit()
            
            return AdIngestionResponse(
                success=True,
                ad_id=ad.id,
                competitor_id=competitor.id,
                analysis_task_id=analysis_task_id,
                message=f"Ad {ad_data.ad_archive_id} {'ingested' if is_new else 'updated'} successfully and analysis task dispatched"
            )
            
        except IntegrityError as e:
            self.db.rollback()
            logger.error(f"Database integrity error during ad ingestion: {e}")
            return AdIngestionResponse(
                success=False,
                ad_id=None,
                competitor_id=None,
                analysis_task_id=None,
                message=f"Database integrity error: Duplicate ad_archive_id or constraint violation"
            )
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Unexpected error during ad ingestion: {e}")
            return AdIngestionResponse(
                success=False,
                ad_id=None,
                competitor_id=None,
                analysis_task_id=None,
                message=f"Ingestion failed: {str(e)}"
            )
    
    async def _find_or_create_competitor(self, competitor_data) -> Competitor:
        """
        Find existing competitor or create new one.
        
        Args:
            competitor_data: CompetitorCreateDTO data
            
        Returns:
            Competitor instance
        """
        # Try to find existing competitor by page_id
        existing_competitor = self.db.query(Competitor).filter_by(
            page_id=competitor_data.page_id
        ).first()
        
        if existing_competitor:
            # Update competitor info if name has changed
            if existing_competitor.name != competitor_data.name:
                logger.info(f"Updating competitor name from '{existing_competitor.name}' to '{competitor_data.name}'")
                existing_competitor.name = competitor_data.name
                existing_competitor.updated_at = datetime.utcnow()
            
            # Reactivate if it was deactivated
            if not existing_competitor.is_active and competitor_data.is_active:
                logger.info(f"Reactivating competitor: {existing_competitor.name}")
                existing_competitor.is_active = True
                existing_competitor.updated_at = datetime.utcnow()
            
            return existing_competitor
        
        # Create new competitor
        logger.info(f"Creating new competitor: {competitor_data.name}")
        new_competitor = Competitor(
            name=competitor_data.name,
            page_id=competitor_data.page_id,
            is_active=competitor_data.is_active
        )
        
        self.db.add(new_competitor)
        self.db.flush()  # Get the ID without committing
        
        return new_competitor
    
    async def _create_or_update_ad(self, ad_data: AdCreate, competitor_id: int) -> Tuple[Ad, bool]:
        """
        Create new ad or update existing one.
        
        Args:
            ad_data: AdCreateDTO data
            competitor_id: ID of the associated competitor
            
        Returns:
            Tuple of (Ad instance, is_new_record)
        """
        # Check if ad already exists
        existing_ad = self.db.query(Ad).filter_by(
            ad_archive_id=ad_data.ad_archive_id
        ).first()
        
        # Extract enhanced data from raw data
        enhanced_data = self._extract_enhanced_data(ad_data)
        
        # Prepare ad data for database
        ad_dict = {
            "competitor_id": competitor_id,
            "ad_archive_id": ad_data.ad_archive_id,
            "ad_copy": ad_data.ad_copy,  # Keep original ad_copy as fallback
            "media_type": ad_data.media_type,
            "media_url": ad_data.media_url,
            "landing_page_url": ad_data.landing_page_url,
            "date_found": ad_data.date_found,
            "page_name": ad_data.page_name,
            "publisher_platform": ad_data.publisher_platform,
            "impressions_text": ad_data.impressions_text,
            "end_date": ad_data.end_date,
            "cta_text": ad_data.cta_text,
            "cta_type": ad_data.cta_type,
            "raw_data": ad_data.raw_data,
            # Enhanced data fields
            "targeted_countries": enhanced_data.get("targeted_countries"),
            "form_details": enhanced_data.get("form_details"),
            "running_countries": enhanced_data.get("targeted_countries"),
            "extra_texts": enhanced_data.get("extra_texts", ad_data.extra_texts),
            "main_title": enhanced_data.get("main_title"),
            "main_body_text": enhanced_data.get("main_body_text"),
            "main_caption": enhanced_data.get("main_caption"),
            "card_count": enhanced_data.get("card_count"),
            "card_bodies": enhanced_data.get("card_bodies"),
            "card_titles": enhanced_data.get("card_titles"),
            "card_cta_texts": enhanced_data.get("card_cta_texts"),
            "card_urls": enhanced_data.get("card_urls"),
            "main_image_urls": enhanced_data.get("main_image_urls"),
            "main_video_urls": enhanced_data.get("main_video_urls")
        }
        
        if existing_ad:
            # Update existing ad
            logger.info(f"Updating existing ad: {ad_data.ad_archive_id}")
            
            for key, value in ad_dict.items():
                if key != "competitor_id":  # Don't change competitor association
                    setattr(existing_ad, key, value)
            
            existing_ad.updated_at = datetime.utcnow()
            return existing_ad, False
        
        # Create new ad
        logger.info(f"Creating new ad: {ad_data.ad_archive_id}")
        new_ad = Ad(**ad_dict)
        
        self.db.add(new_ad)
        self.db.flush()  # Get the ID without committing
        
        return new_ad, True
    
    def _extract_enhanced_data(self, ad_data: AdCreate) -> Dict[str, Any]:
        """
        Extract enhanced data from raw ad data.
        
        Args:
            ad_data: AdCreateDTO with raw_data
            
        Returns:
            Dict with extracted enhanced data
        """
        enhanced = {}
        
        if not ad_data.raw_data:
            return enhanced
        
        try:
            # Extract data from snapshot if available
            snapshot = ad_data.raw_data.get('snapshot', {})
            
            # Extract main content fields
            enhanced['main_title'] = snapshot.get('title', '')
            enhanced['main_body_text'] = snapshot.get('body', {}).get('text', '')
            enhanced['main_caption'] = snapshot.get('caption', '')
            
            # Extract form details from extra_texts
            extra_texts = snapshot.get('extra_texts', [])
            enhanced['form_details'] = self._extract_form_details(extra_texts)
            
            # Also store the original extra_texts for backward compatibility
            enhanced['extra_texts'] = extra_texts
            
            # Extract targeted countries
            enhanced['targeted_countries'] = self._extract_targeted_countries(ad_data.raw_data)
            
            # Extract card data
            cards = snapshot.get('cards', [])
            if cards:
                enhanced['card_count'] = len(cards)
                enhanced['card_bodies'] = [card.get('body', '') for card in cards]
                enhanced['card_titles'] = [card.get('title', '') for card in cards]
                enhanced['card_cta_texts'] = [card.get('cta_text', '') for card in cards]
                enhanced['card_urls'] = [card.get('link_url', '') for card in cards]
            else:
                # If no separate cards but has main body, treat it as a single card
                main_body = snapshot.get('body', {}).get('text', '')
                main_title = snapshot.get('title', '')
                if main_body:
                    enhanced['card_count'] = 1
                    enhanced['card_bodies'] = [main_body]
                    enhanced['card_titles'] = [main_title] if main_title else ['']
                    enhanced['card_cta_texts'] = [snapshot.get('cta_text', '')]
                    enhanced['card_urls'] = [snapshot.get('link_url', '')]
                
            # Extract media URLs from cards or main content
            image_urls = []
            video_urls = []
            
            if cards:
                # Extract from actual cards
                for card in cards:
                    if card.get('video_hd_url'):
                        video_urls.append(card['video_hd_url'])
                    elif card.get('video_sd_url'):
                        video_urls.append(card['video_sd_url'])
                    if card.get('video_preview_image_url'):
                        image_urls.append(card['video_preview_image_url'])
                    if card.get('resized_image_url'):
                        image_urls.append(card['resized_image_url'])
            else:
                # Extract from main snapshot level
                videos = snapshot.get('videos', [])
                images = snapshot.get('images', [])
                
                for video in videos:
                    if isinstance(video, dict):
                        if video.get('video_hd_url'):
                            video_urls.append(video['video_hd_url'])
                        elif video.get('video_sd_url'):
                            video_urls.append(video['video_sd_url'])
                        if video.get('video_preview_image_url'):
                            image_urls.append(video['video_preview_image_url'])
                
                for image in images:
                    if isinstance(image, dict) and image.get('resized_image_url'):
                        image_urls.append(image['resized_image_url'])
            
            enhanced['main_video_urls'] = video_urls
            enhanced['main_image_urls'] = image_urls
            
            logger.info(f"Enhanced data extracted for ad {ad_data.ad_archive_id}: "
                       f"Countries: {enhanced.get('targeted_countries', [])}, "
                       f"Form fields: {len(enhanced.get('form_details', []))}, "
                       f"Cards: {enhanced.get('card_count', 0)}")
            
        except Exception as e:
            logger.error(f"Error extracting enhanced data: {e}")
        
        return enhanced
    
    def _extract_enhanced_data_from_raw(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract enhanced data from raw ad data (for reprocessing existing ads).
        
        Args:
            raw_data: Raw Facebook API data
            
        Returns:
            Dict with extracted enhanced data
        """
        enhanced = {}
        
        if not raw_data:
            return enhanced
        
        try:
            # Extract data from snapshot if available
            snapshot = raw_data.get('snapshot', {})
            
            # Extract main content fields
            enhanced['main_title'] = snapshot.get('title', '')
            enhanced['main_body_text'] = snapshot.get('body', {}).get('text', '')
            enhanced['main_caption'] = snapshot.get('caption', '')
            
            # Extract form details from extra_texts
            extra_texts = snapshot.get('extra_texts', [])
            logger.info(f"Reprocessing: Extracting form details from {len(extra_texts)} extra_texts")
            enhanced['form_details'] = self._extract_form_details(extra_texts)
            logger.info(f"Reprocessing: Extracted {len(enhanced.get('form_details', []))} form details")
            
            # Extract targeted countries
            enhanced['running_countries'] = self._extract_targeted_countries(raw_data)
            
            # Extract card data
            cards = snapshot.get('cards', [])
            if cards:
                enhanced['card_count'] = len(cards)
                enhanced['card_bodies'] = [card.get('body', '') for card in cards]
                enhanced['card_titles'] = [card.get('title', '') for card in cards]
                enhanced['card_cta_texts'] = [card.get('cta_text', '') for card in cards]
                enhanced['card_urls'] = [card.get('link_url', '') for card in cards]
            else:
                # If no separate cards but has main body, treat it as a single card
                main_body = snapshot.get('body', {}).get('text', '')
                main_title = snapshot.get('title', '')
                if main_body:
                    enhanced['card_count'] = 1
                    enhanced['card_bodies'] = [main_body]
                    enhanced['card_titles'] = [main_title] if main_title else ['']
                    enhanced['card_cta_texts'] = [snapshot.get('cta_text', '')]
                    enhanced['card_urls'] = [snapshot.get('link_url', '')]
            
            logger.info(f"Enhanced data extracted for reprocessing: "
                       f"Countries: {enhanced.get('running_countries', [])}, "
                       f"Form fields: {len(enhanced.get('form_details', []))}, "
                       f"Cards: {enhanced.get('card_count', 0)}")
            
        except Exception as e:
            logger.error(f"Error extracting enhanced data for reprocessing: {e}")
        
        return enhanced
    

    
    def _extract_form_details(self, extra_texts: List[Any]) -> List[str]:
        """
        Extract form details from extra_texts - treat all extra_texts as form details.
        """
        form_details = []
        
        for extra in extra_texts:
            if isinstance(extra, dict):
                text = extra.get('text', '')
            elif isinstance(extra, str):
                text = extra
            else:
                continue
                
            if text.strip():  # Only add non-empty texts
                form_details.append(text)
        
        return form_details
    
    def _extract_targeted_countries(self, raw_data: Dict[str, Any]) -> List[str]:
        """
        Extract targeted countries from various fields in raw data.
        """
        countries = []
        
        # Check targeted_or_reached_countries
        targeted_countries = raw_data.get('targeted_or_reached_countries', [])
        if targeted_countries:
            countries.extend(targeted_countries)
        
        # Check political_countries
        political_countries = raw_data.get('political_countries', [])
        if political_countries:
            countries.extend(political_countries)
        
        # Check snapshot for country info
        snapshot = raw_data.get('snapshot', {})
        country_iso = snapshot.get('country_iso_code')
        if country_iso:
            countries.append(country_iso)
        
        # Remove duplicates and empty values
        countries = list(set([c for c in countries if c]))
        
        # If no countries found, try to infer from other data
        if not countries:
            # Check if this looks like a specific region based on content
            content_text = ""
            if snapshot.get('body', {}).get('text'):
                content_text += snapshot['body']['text']
            for card in snapshot.get('cards', []):
                if card.get('body'):
                    content_text += " " + card['body']
            
            # Simple heuristics for common markets
            content_lower = content_text.lower()
            if any(keyword in content_lower for keyword in ['dubai', 'uae', 'emirates']):
                countries.append('AE')
            elif any(keyword in content_lower for keyword in ['riyadh', 'saudi', 'ksa']):
                countries.append('SA')
            elif any(keyword in content_lower for keyword in ['doha', 'qatar']):
                countries.append('QA')
        
        return countries
    
    async def _trigger_analysis_task(self, ad_id: int) -> str:
        """
        Trigger AI analysis task for the given ad.
        
        Args:
            ad_id: ID of the ad to analyze
            
        Returns:
            Task ID of the dispatched analysis task
        """
        try:
            # Use app's celery instance to avoid naming conflicts
            from app.celery import celery
            
            # Dispatch the AI analysis task
            task_result = celery.send_task('app.tasks.ai_analysis_tasks.ai_analysis_task', args=[ad_id])
            logger.info(f"AI analysis task dispatched: {task_result.id} for ad: {ad_id}")
            return task_result.id
        
        except Exception as e:
            logger.error(f"Failed to dispatch AI analysis task for ad {ad_id}: {e}")
            # Don't fail the entire ingestion if task dispatch fails
            # The ad is still saved, analysis can be triggered manually later
            return "failed-to-dispatch"
    
    async def batch_ingest_ads(self, ads_data: list[AdCreate]) -> dict:
        """
        Batch ingest multiple ads at once.
        
        Args:
            ads_data: List of AdCreateDTO objects
            
        Returns:
            Dict with batch processing results
        """
        logger.info(f"Starting batch ingestion of {len(ads_data)} ads")
        
        results = []
        successful_count = 0
        failed_count = 0
        
        for ad_data in ads_data:
            try:
                result = await self.ingest_ad(ad_data)
                results.append({
                    "ad_archive_id": ad_data.ad_archive_id,
                    "success": result.success,
                    "ad_id": result.ad_id,
                    "message": result.message
                })
                
                if result.success:
                    successful_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"Failed to ingest ad {ad_data.ad_archive_id}: {e}")
                results.append({
                    "ad_archive_id": ad_data.ad_archive_id,
                    "success": False,
                    "error": str(e)
                })
                failed_count += 1
        
        logger.info(f"Batch ingestion completed: {successful_count} successful, {failed_count} failed")
        
        return {
            "total_ads": len(ads_data),
            "successful": successful_count,
            "failed": failed_count,
            "results": results,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def get_ingestion_stats(self) -> dict:
        """
        Get statistics about ingested data.
        
        Returns:
            Dict with ingestion statistics
        """
        try:
            total_ads = self.db.query(Ad).count()
            total_competitors = self.db.query(Competitor).count()
            active_competitors = self.db.query(Competitor).filter_by(is_active=True).count()
            
            # Get recent ingestion activity (last 24 hours)
            from datetime import timedelta
            yesterday = datetime.utcnow() - timedelta(days=1)
            recent_ads = self.db.query(Ad).filter(Ad.created_at >= yesterday).count()
            
            return {
                "total_ads": total_ads,
                "total_competitors": total_competitors,
                "active_competitors": active_competitors,
                "recent_ads_24h": recent_ads,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to get ingestion stats: {e}")
            return {
                "error": "Failed to retrieve statistics",
                "timestamp": datetime.utcnow().isoformat()
            } 



================================================
File: app/tasks/__init__.py
================================================
from .basic_tasks import add_together, test_task
from .ai_analysis_tasks import ai_analysis_task, batch_ai_analysis_task
from .facebook_ads_scraper_task import scrape_facebook_ads_task, scrape_competitor_ads_task, get_facebook_ads_task_status

__all__ = [
    "add_together", 
    "test_task", 
    "ai_analysis_task", 
    "batch_ai_analysis_task",
    "scrape_facebook_ads_task",
    "scrape_competitor_ads_task", 
    "get_facebook_ads_task_status"
] 


================================================
File: app/tasks/ai_analysis_tasks.py
================================================
from celery import shared_task
from datetime import datetime
import logging
from typing import Dict, Any
from app.services.ai_service import get_ai_service
from celery.app import defaults

logger = logging.getLogger(__name__)

@shared_task(bind=True)
def ai_analysis_task(self, ad_id: int) -> Dict[str, Any]:
    """
    Placeholder AI analysis task that will be triggered after ad ingestion.
    This task will analyze the ad content and generate insights.
    
    Args:
        ad_id: The ID of the ad to analyze
        
    Returns:
        Dict with analysis results
    """
    task_id = self.request.id
    logger.info(f"Starting AI analysis for ad {ad_id} (task: {task_id})")
    
    try:
        # Import here to avoid circular imports
        from app.database import SessionLocal
        from app.models import Ad, AdAnalysis
        
        db = SessionLocal()
        
        # Get the ad record
        ad = db.query(Ad).filter(Ad.id == ad_id).first()
        if not ad:
            raise ValueError(f"Ad with ID {ad_id} not found")
        
        logger.info(f"Analyzing ad: {ad.ad_archive_id} from competitor: {ad.competitor.name}")
        
        # Get AI service instance
        ai_service = get_ai_service()
        
        if not ai_service:
            logger.warning("AI service not configured, using fallback analysis")
            analysis_data = {
                "summary": f"AI analysis not available - fallback analysis for ad {ad.ad_archive_id}",
                "hook_score": 5.0,
                "overall_score": 5.0,
                "target_audience": "General audience",
                "content_themes": [],
                "analysis_version": "v1.0-fallback",
                "confidence_score": 0.0,
                "ai_prompts": {"note": "AI service not configured"},
                "raw_ai_response": {"note": "AI service not configured"},
                "performance_predictions": {},
                "competitor_insights": {},
                "ad_format_analysis": {},
                "effectiveness_analysis": {}
            }
        else:
            # Prepare ad data for AI analysis
            ad_data = {
                "ad_archive_id": ad.ad_archive_id,
                "ad_copy": ad.ad_copy,
                "main_title": ad.main_title,
                "main_body_text": ad.main_body_text,
                "main_caption": ad.main_caption,
                "cta_text": ad.cta_text,
                "media_type": ad.media_type,
                "page_name": ad.page_name,
                "targeted_countries": ad.targeted_countries,
                "card_titles": ad.card_titles,
                "card_bodies": ad.card_bodies
            }
            
            # Call AI service for analysis
            try:
                analysis_data = ai_service.analyze_ad_content(ad_data)
                logger.info(f"AI analysis completed for ad {ad.ad_archive_id}")
            except Exception as ai_error:
                logger.error(f"AI analysis failed for ad {ad.ad_archive_id}: {str(ai_error)}")
                # Use fallback analysis on AI failure
                analysis_data = {
                    "summary": f"AI analysis failed - fallback analysis for ad {ad.ad_archive_id}",
                    "hook_score": 5.0,
                    "overall_score": 5.0,
                    "target_audience": "General audience",
                    "content_themes": [],
                    "analysis_version": "v1.0-fallback",
                    "confidence_score": 0.0,
                    "ai_prompts": {"error": str(ai_error)},
                    "raw_ai_response": {"error": str(ai_error)},
                    "performance_predictions": {},
                    "competitor_insights": {},
                    "ad_format_analysis": {},
                    "effectiveness_analysis": {}
                }
        
        # Check if analysis already exists
        existing_analysis = db.query(AdAnalysis).filter(AdAnalysis.ad_id == ad_id).first()
        
        if existing_analysis:
            # Update existing analysis
            for key, value in analysis_data.items():
                setattr(existing_analysis, key, value)
            existing_analysis.updated_at = datetime.utcnow()
            analysis = existing_analysis
            logger.info(f"Updated existing analysis for ad {ad_id}")
        else:
            # Create new analysis
            analysis = AdAnalysis(
                ad_id=ad_id,
                **analysis_data
            )
            db.add(analysis)
            logger.info(f"Created new analysis for ad {ad_id}")
        
        db.commit()
        
        result = {
            "task_id": task_id,
            "ad_id": ad_id,
            "analysis_id": analysis.id,
            "status": "completed",
            "summary": analysis_data["summary"],
            "overall_score": analysis_data["overall_score"],
            "hook_score": analysis_data["hook_score"],
            "timestamp": datetime.utcnow().isoformat()
        }
        
        logger.info(f"AI analysis completed for ad {ad_id}")
        return result
        
    except Exception as exc:
        logger.error(f"AI analysis failed for ad {ad_id}: {str(exc)}")
        # Update task state with error info
        self.update_state(
            state='FAILURE',
            meta={
                'error': str(exc),
                'ad_id': ad_id,
                'timestamp': datetime.utcnow().isoformat()
            }
        )
        raise exc
    finally:
        if 'db' in locals():
            db.close()

@shared_task
def batch_ai_analysis_task(ad_id_list: list) -> Dict[str, Any]:
    """
    Batch AI analysis task for processing multiple ads at once.
    
    Args:
        ad_id_list: List of ad IDs to analyze
        
    Returns:
        Dict with batch analysis results
    """
    logger.info(f"Starting batch AI analysis for {len(ad_id_list)} ads")
    
    results = []
    failed_ads = []
    
    # Import Celery app to get access to the task registry
    from app.celery import celery
    
    for ad_id in ad_id_list:
        try:
            # Dispatch individual analysis task using the Celery app
            task_sig = celery.signature('app.tasks.ai_analysis_tasks.ai_analysis_task')
            task = task_sig.delay(ad_id)
            results.append({
                "ad_id": ad_id,
                "task_id": task.id,
                "status": "dispatched"
            })
        except Exception as e:
            logger.error(f"Failed to dispatch analysis for ad {ad_id}: {e}")
            failed_ads.append({
                "ad_id": ad_id,
                "error": str(e)
            })
    
    return {
        "total_ads": len(ad_id_list),
        "successful_dispatches": len(results),
        "failed_dispatches": len(failed_ads),
        "results": results,
        "failures": failed_ads,
        "timestamp": datetime.utcnow().isoformat()
    } 


================================================
File: app/tasks/basic_tasks.py
================================================
from celery import shared_task
from datetime import datetime
import time
import logging

logger = logging.getLogger(__name__)

@shared_task
def add_together(x: int, y: int) -> dict:
    """
    Simple test task that adds two numbers together.
    This is the test task requested in the prompt.
    """
    logger.info(f"Adding {x} + {y}")
    result = x + y
    
    return {
        "task_name": "add_together",
        "inputs": {"x": x, "y": y},
        "result": result,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }

@shared_task(bind=True)
def test_task(self, message: str = "Hello from Celery!") -> dict:
    """
    Another simple test task to verify Celery is working properly.
    """
    task_id = self.request.id
    logger.info(f"Test task {task_id} started with message: {message}")
    
    # Simulate some work
    time.sleep(2)
    
    result = {
        "task_id": task_id,
        "message": message,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }
    
    logger.info(f"Test task {task_id} completed")
    return result

@shared_task(bind=True)
def long_running_task(self, duration: int = 10) -> dict:
    """
    A task that simulates long-running work.
    Useful for testing task monitoring and cancellation.
    """
    task_id = self.request.id
    logger.info(f"Long running task {task_id} started for {duration} seconds")
    
    for i in range(duration):
        time.sleep(1)
        # Update task state
        self.update_state(
            state='PROGRESS',
            meta={
                'current': i + 1,
                'total': duration,
                'status': f'Processing... {i + 1}/{duration}'
            }
        )
    
    result = {
        "task_id": task_id,
        "duration": duration,
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }
    
    logger.info(f"Long running task {task_id} completed")
    return result 


================================================
File: app/tasks/facebook_ads_scraper_task.py
================================================
from celery import current_task
from sqlalchemy.orm import Session
from datetime import datetime
import logging
from typing import Dict, List, Optional
import json

from app.celery_worker import celery_app
from app.database import get_db
from app.services.facebook_ads_scraper import FacebookAdsScraperService, FacebookAdsScraperConfig

logger = logging.getLogger(__name__)


@celery_app.task(bind=True, name="facebook_ads_scraper.scrape_ads")
def scrape_facebook_ads_task(
    self,
    config: Dict = None,
    view_all_page_id: str = None,
    countries: List[str] = None,
    max_pages: int = 10,
    delay_between_requests: int = 1
):
    """
    Celery task for scraping Facebook Ads Library data
    
    Args:
        config: Optional dictionary with scraper configuration
        view_all_page_id: Page ID to scrape ads from
        countries: List of country codes to search in
        max_pages: Maximum number of pages to scrape
        delay_between_requests: Delay between requests in seconds
    """
    task_id = current_task.request.id
    logger.info(f"Starting Facebook Ads scraping task: {task_id}")

    # Get database session
    db = next(get_db())

    try:
        # Create scraper service
        scraper = FacebookAdsScraperService(db)
        # Create configuration
        scraper_config = FacebookAdsScraperConfig(
            view_all_page_id=view_all_page_id or (config.get('view_all_page_id') if config else None),
            countries=countries or (config.get('countries') if config else None),
            max_pages=max_pages,
            delay_between_requests=delay_between_requests
        )
        # Update config with any additional parameters
        if config:
            for key, value in config.items():
                if hasattr(scraper_config, key) and key not in ['view_all_page_id', 'countries', 'max_pages', 'delay_between_requests']:
                    setattr(scraper_config, key, value)
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper.scrape_ads(scraper_config)
        # Save to database if requested
        if scraper_config.save_json and all_json_responses:
            # Save raw JSON responses to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_ads_{scraper_config.view_all_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")
        
        # Return stats
        return {
            "task_id": task_id,
            "stats": stats,
            "config": {
                "view_all_page_id": scraper_config.view_all_page_id,
                "countries": scraper_config.countries,
                "max_pages": scraper_config.max_pages
            },
            "enhanced_data_summary": {
                "advertiser_info": enhanced_data.get("advertiser_info", {}),
                "campaigns_count": len(enhanced_data.get("campaigns", [])),
                "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
            }
        }
    except Exception as e:
        logger.error(f"Error in Facebook Ads scraping task: {str(e)}")
        raise
    finally:
        db.close()


@celery_app.task(bind=True, name="facebook_ads_scraper.scrape_competitor_ads")
def scrape_competitor_ads_task(
    self,
    competitor_page_id: str,
    countries: List[str] = None,
    max_pages: int = 5,
    delay_between_requests: int = 1,
    active_status: str = "ALL",
    ad_type: str = "ALL",
    media_type: str = "ALL",
    save_json: bool = False
):
    """
    Celery task for scraping ads from a specific competitor page
    
    Args:
        competitor_page_id: Facebook page ID of the competitor
        countries: List of country codes to search in
        max_pages: Maximum number of pages to scrape
        delay_between_requests: Delay between requests in seconds
        active_status: Filter by ad status (ALL, ACTIVE, INACTIVE)
        ad_type: Filter by ad type (ALL, POLITICAL_AND_ISSUE_ADS, etc.)
        media_type: Filter by media type (ALL, VIDEO, IMAGE, etc.)
        save_json: Whether to save raw JSON responses to file
    """
    task_id = self.request.id
    logger.info(f"Starting competitor ads scraping task: {task_id} for page ID: {competitor_page_id}")
    
    # Get database session
    db = next(get_db())
    
    try:
        # Check if competitor exists in database
        from app.models.competitor import Competitor
        competitor = db.query(Competitor).filter_by(page_id=competitor_page_id).first()
        if not competitor:
            logger.warning(f"Competitor with page_id {competitor_page_id} not found, skipping scraping")
            return {
                'success': False,
                'warning': f"Competitor with page_id {competitor_page_id} not found, skipping scraping",
                'competitor_page_id': competitor_page_id,
                'task_id': task_id,
                'completion_time': datetime.utcnow().isoformat()
            }
        
        # Create scraper service
        scraper_service = FacebookAdsScraperService(db)
        
        # Create scraper configuration for specific competitor
        scraper_config = FacebookAdsScraperConfig(
            view_all_page_id=competitor_page_id,
            countries=countries or ['AE'],
            max_pages=max_pages,
            delay_between_requests=delay_between_requests,
            search_type='page',  # Ensure we're searching by page
            active_status=active_status,
            ad_type=ad_type,
            media_type=media_type,
            save_json=save_json
            )
            
        # Scrape ads
        all_ads_data, all_json_responses, enhanced_data, stats = scraper_service.scrape_ads(scraper_config)
            
        # Save raw JSON responses to file if requested
        if save_json and all_json_responses:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_ads_competitor_{competitor_page_id}_{timestamp}.json"
            with open(filename, "w") as f:
                json.dump(all_json_responses, f)
            logger.info(f"Saved raw JSON responses to {filename}")

        # Prepare final results
        results = {
            'success': True,
            'competitor_page_id': competitor_page_id,
            'total_ads_scraped': stats.get('total_processed', 0),
            'database_stats': stats,
            'completion_time': datetime.utcnow().isoformat(),
            'task_id': task_id,
            'enhanced_data_summary': {
                "advertiser_info": enhanced_data.get("advertiser_info", {}),
                "campaigns_count": len(enhanced_data.get("campaigns", [])),
                "total_ads": sum(len(c.get("ads", [])) for c in enhanced_data.get("campaigns", []))
            }
        }
        logger.info(f"Competitor ads scraping task completed successfully. Results: {results}")
        return results
    except Exception as e:
        logger.error(f"Error in competitor ads scraping task: {str(e)}")
        db.rollback()
        return {
            'success': False,
            'error': str(e),
            'competitor_page_id': competitor_page_id,
            'task_id': task_id,
            'completion_time': datetime.utcnow().isoformat()
        }
    finally:
        db.commit()
        db.close()


@celery_app.task(bind=True, name="facebook_ads_scraper.get_task_status")
def get_facebook_ads_task_status(self, task_id: str):
    """
    Get the status of a Facebook Ads scraping task
    
    Args:
        task_id: ID of the task to check
    """
    try:
        # Get task result
        task_result = celery_app.AsyncResult(task_id)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'status': 'Task is waiting to be processed'
            }
        elif task_result.state == 'PROGRESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'progress': task_result.info
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'result': task_result.result
            }
        else:
            # Task failed
            response = {
                'task_id': task_id,
                'state': task_result.state,
                'error': str(task_result.info)
            }
        
        return response
        
    except Exception as e:
        logger.error(f"Error getting task status: {str(e)}")
        return {
            'task_id': task_id,
            'state': 'ERROR',
            'error': f'Failed to get task status: {str(e)}'
        } 


